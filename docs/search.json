[
  {
    "objectID": "slides/06_hte.html#treatment-effect-heterogeneity-motivation",
    "href": "slides/06_hte.html#treatment-effect-heterogeneity-motivation",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Treatment Effect Heterogeneity: Motivation",
    "text": "Treatment Effect Heterogeneity: Motivation\n\nMore comprehensive evaluation:\n\nwho wins or loses and by how much?\n\nThis is useful along at least two dimensions:\n\nInforms action:\n\nMore efficient allocation of public and private resources via targeting in the future:\n\nPersonalized policies, ads, medicine, …\n\n\nUnderstanding:\n\nHeterogeneous effects can be suggestive for underlying mechanisms"
  },
  {
    "objectID": "slides/06_hte.html#treatment-effect-heterogeneity-definition",
    "href": "slides/06_hte.html#treatment-effect-heterogeneity-definition",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Treatment Effect Heterogeneity: Definition",
    "text": "Treatment Effect Heterogeneity: Definition\n\nExpected treatment effect in the target subpopulation with characteristics \\(\\mathbf{X_i}\\) given by Conditional Average Treatment Effect (CATE):\n\n\\(\\tau(\\mathbf{x}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}]\\)\n\n\n\n\n\\(\\mathbf{X_i} = \\mathbf{H_i} \\cup \\mathbf{C_i}\\)\n\n\\(\\mathbf{H_i}\\): motivated by the research question to understand specific effect heterogeneity in a pre-defined the target subpopulation.\n\\(\\mathbf{C_i}\\): confounders that are required for identification.\n\n\n\n\n\nRandomized experiments - no confounders:\n\nCATE defined with respect to considered heterogeneity variables: \\(\\mathbf{X_i} = \\mathbf{H_i}\\)\n\n\n\n\n\nMeasured Confounding - distinguish two types of CATEs:\n\nGroup ATE (GATE) for groups G defined by H: \\(\\tau(\\mathbf{g}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{G_i = g}]\\)\nIndividualized ATE (IATE = CATE): \\(\\tau(\\mathbf{x}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}]\\)\n\nmost flexible/ personalized/ individualized effect prediction\n\nEstimation step is affected by whether we are interested in GATEs or IATEs."
  },
  {
    "objectID": "slides/06_hte.html#treatment-effect-heterogeneity-identification",
    "href": "slides/06_hte.html#treatment-effect-heterogeneity-identification",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Treatment Effect Heterogeneity: Identification",
    "text": "Treatment Effect Heterogeneity: Identification\n\nNo need to establish new identification results:\n\nAll target parameters can be thought of as special cases of conditioning ITE on some function \\(f(\\mathbf{X_i = x})\\)\nAnd by the Law of Iterated Expectations (LIE):\n\n\n\\[\n\\begin{align*}\n\\mathbb{E}[Y_i(1) - Y_i(0) | f(\\mathbf{X_i}) = f(\\mathbf{x})] &= \\mathbb{E}[\\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}, f(\\mathbf{X_i = x}) ] | f(\\mathbf{X_i = x})] \\\\\n&= \\mathbb{E}[\\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x} ] | f(\\mathbf{X_i = x})]\n\\end{align*}\n\\]\n\n\nAs \\(\\mathbf{X_i} = \\mathbf{H_i} \\cup \\mathbf{C_i}\\) is assumed to contain all confounders, the inner expectation \\(\\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x} ]\\) is identified in randomized experiments or under measured confounding\n=&gt; All aggregations with respect to a function \\(f(\\mathbf{X_i})\\) are also identified."
  },
  {
    "objectID": "slides/06_hte.html#group-ates-examples",
    "href": "slides/06_hte.html#group-ates-examples",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Group ATEs: Examples",
    "text": "Group ATEs: Examples\n\nGroup ATE (GATE): \\(\\tau(\\mathbf{g}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{G_i = g}]\\)\nExamples for subgroups of interest:\n\nMutually exclusive subgroups, e.g.: \\(G = \\{\\text{female}, \\text{male}\\}\\), \\(G = \\{ \\text{age} &lt; 50, \\text{age} \\geq 50 \\}\\), \\(G = \\{ \\text{age} &lt; 50 \\space \\& \\space \\text{female}, \\text{age} \\geq 50 \\space \\& \\space \\text{female}, \\text{age} \\geq 50 \\space \\& \\space \\text{male}, \\dots  \\}\\), …\nSingle or low-dimensional continuous variable, e.g.: G = age, G = income, …\nOther functions or small subsets of \\(\\mathbf{X_i}\\)\n\nGroups should be pre-determined and not be the result of data snooping"
  },
  {
    "objectID": "slides/06_hte.html#group-ates-estimation",
    "href": "slides/06_hte.html#group-ates-estimation",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Group ATEs: Estimation",
    "text": "Group ATEs: Estimation\n\nThree strategies:\n\nStratify the data and rerun the analysis for each subgroup.\n\nDownside: Requires a lot of data and computation, can lead to high variance estimates for small subgroups.\n\nSpecify an interaction term in an OLS regression model:\n\n\\(Y_i = \\beta_{0} + \\tau T_i + \\beta_{G_i} G_{i} + \\beta_{T_iG_i} T_{i} G_{i} + \\mathbf{\\beta_{X_i}X_{i}}+ \\epsilon_i\\)\nDownside: Requires a correct model specification, can be sensitive to misspecification.\n\nDouble Machine Learning with AIPW model to estimate the GATEs directly."
  },
  {
    "objectID": "slides/06_hte.html#group-ates-double-machine-learning",
    "href": "slides/06_hte.html#group-ates-double-machine-learning",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Group ATEs: Double Machine Learning",
    "text": "Group ATEs: Double Machine Learning\n\nPrevious lecture: ATE (AIPW) can be estimated as mean of a pseudo-outcome:\n\n\\(\\hat{\\tau}_{\\text{ATE}}^{\\text{AIPW}} = \\frac{1}{N}\\sum_{i=1}^n \\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}\\)\n\nPseudo-outcome is given by:\n\n\\(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} = \\hat{\\mu}(1, \\mathbf{X_i}) - \\hat{\\mu}(0, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\hat{\\mu}(1, \\mathbf{X_i}))}{\\hat{e}_1(\\mathbf{X_i})} - \\frac{(1-T_i)(Y_i - \\hat{\\mu}(0, \\mathbf{X_i})}{\\hat{e}_0(\\mathbf{X_i}))}\\)\n\n\n\n\nEquivalent to a linear regression model with pseudo-outcome and constant:\n\n\\(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} = \\alpha + \\epsilon_i\\) with \\(\\hat{\\alpha} = \\hat{\\tau}_{\\text{ATE}}^{\\text{AIPW}}\\)\n\nCan be extended with heterogeneity variable(s) \\(G_i\\):\n\n\\(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} = \\alpha + \\beta G_i + \\epsilon_i\\)\n=&gt; Modelling the level of the effect, not the level of the outcome."
  },
  {
    "objectID": "slides/06_hte.html#group-ates-advantages-of-dml",
    "href": "slides/06_hte.html#group-ates-advantages-of-dml",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Group ATEs: Advantages of DML",
    "text": "Group ATEs: Advantages of DML\n\nNeyman-orthogonality of \\(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}\\) allows to apply standard statistical inference (Semenova and Chernozhukov, 2021).\nComputationally less expensive than subgroup analyses\n\nOnly one additional OLS, no new nuisance parameters).\n\nMore flexible than specifying interaction terms in a linear model, as we flexibly adjust for confounding by ML methods.\nAs \\(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}\\) is an unbiased signal, i.e. \\(\\mathbb{E}[\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} | G_i = g] = \\tau(g)\\), to regress the pseudo-outcome \\(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}\\) on low-dimensional \\(G_i\\) we can either use\n\nOLS or series regression (Semenova and Chernozhukov, 2021).\nKernel regression (Fan et al., 2022; Zimmert & Lechner, 2019)."
  },
  {
    "objectID": "slides/06_hte.html#group-ates-proof-of-dml",
    "href": "slides/06_hte.html#group-ates-proof-of-dml",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Group ATEs: Proof of DML",
    "text": "Group ATEs: Proof of DML\n\nProof that \\(\\mathbb{E}[\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} \\mid G_i = g] = \\tau(g)\\):\n\n\\[\n\\begin{align*}\n\\mathbb{E}[\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} \\mid G_i = g] &= \\mathbb{E} \\left[  \\mu(1,\\mathbf{X_i}) + \\frac{T_i(Y_i - \\mu(1,\\mathbf{X_i}))}{e(\\mathbf{X_i})} - \\mu(0,\\mathbf{X_i}) - \\frac{(1-T_i)(Y_i - \\mu(0,\\mathbf{X_i}))}{1 - e(\\mathbf{X_i})} \\bigg| G_i = g \\right] \\\\\n&\\overset{LIE}{=} \\mathbb{E} \\left[ \\underbrace{\\mathbb{E} \\left[ \\mu(1, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\mu(1, \\mathbf{X_i}))}{e(\\mathbf{X_i})} \\mid \\mathbf{X_i = x} \\right]}_{\\text{CAPO-AIPW =&gt; }\\mathbb{E}[Y_i(1) \\mid \\mathbf{X_i = x}]} - \\underbrace{\\mathbb{E} \\left[ \\mu(0, \\mathbf{X_i}) + \\frac{(1-T_i)(Y_i - \\mu(0, \\mathbf{X_i}))}{1 - e(\\mathbf{X_i})} \\mid \\mathbf{X_i = x} \\right]}_{CAPO-AIPW =&gt; \\mathbb{E}[Y(0) \\mid \\mathbf{X_i = x}]} \\bigg| G_i = g \\right] \\\\\n&= \\mathbb{E}\\left[\\mathbb{E}[Y_i(1) \\mid \\mathbf{X_i = x}] - \\mathbb{E}[Y_i(0) \\mid \\mathbf{X_i = x}] \\bigg| G_i = g\\right] \\\\\n&\\overset{LIE}{=} \\mathbb{E}[Y_i(1) - Y_i(0) \\mid G_i = g] \\\\\n&= \\tau(g)\n\\end{align*}\n\\]\n\nLaw of Iterated Expectations uses that \\(G_i\\) is a function of \\(X_i\\)."
  },
  {
    "objectID": "slides/06_hte.html#group-ates-example-based-on-dml",
    "href": "slides/06_hte.html#group-ates-example-based-on-dml",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Group ATEs: Example based on DML",
    "text": "Group ATEs: Example based on DML\n\nAssess the effect of 401(k) program participation on net financial assets of 9,915 households in the US in 1991.\nFirst step (not shown): Estimate \\(\\hat{\\tau}_{\\text{ATE}}^{\\text{AIPW}}\\) using DoubleML.\n\n\n\n\n# Get the indvidual ATEs (pseudo-outcomes)\ndata$ate_i &lt;- dml_irm_forest[[\"psi_b\"]] # get numerator of score function, which is equal to pseudo outcome\nmean_ate = mean(data$ate_i) # mean of pseudo outcomes = ATE\n\nlibrary(estimatr) # for linear robust post estimation\nsummary(lm_robust(ate_i ~ hown, data = data))\n\n\n\nEstimates and significance testing of the effect of target variables\n     Estimate. Std. Error t value Pr(&gt;|t|)    \ne401      8206       1106   7.421 1.16e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:\nlm_robust(formula = ate_i ~ hown, data = data)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n(Intercept)     3477        711   4.890 1.025e-06     2083     4870 9913\nhown            7445       1835   4.058 4.990e-05     3849    11041 9913\n\nMultiple R-squared:  0.00106 ,  Adjusted R-squared:  0.0009588 \nF-statistic: 16.47 on 1 and 9913 DF,  p-value: 4.99e-05\n\n\n\n\nlibrary(np) # for kernel post estimation\nage = data$age            \nate_i = data$ate_i                \nnp_model = npreg(ate_i ~ age)  # kernel regression\nplot(np_model)  # plot the kernel regression"
  },
  {
    "objectID": "slides/06_hte.html#predicting-individualized-ates",
    "href": "slides/06_hte.html#predicting-individualized-ates",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Predicting Individualized ATEs",
    "text": "Predicting Individualized ATEs\n\nGroup-level heterogeneity variables were hand-picked.\nNow predict individualized treatment effects based on all covariates \\(\\mathbf{X_i}\\):\n\nIndividualized ATE (IATE = CATE): \\(\\tau(\\mathbf{x}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}]\\)\nConditional expectation with unobserved outcome (counterfactuals)\n\nGiven the assumptions of observed confounding, we can write the CATE as:\n\n\\(\\tau(\\mathbf{x}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}] = \\mathbb{E}[Y_i | T_i, \\mathbf{X_i = x}] - \\mathbb{E}[Y_i | T_i, \\mathbf{X_i = x}]\\)\nwhich can be approximated with ML."
  },
  {
    "objectID": "slides/06_hte.html#s-learner-and-t-learner",
    "href": "slides/06_hte.html#s-learner-and-t-learner",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "S-Learner and T-Learner",
    "text": "S-Learner and T-Learner\n\nS-learner:\n\n\nUse ML estimator of your choice to fit outcome model using \\(\\mathbf{X_i}\\) AND \\(T_i\\) in the full sample: \\(\\hat{\\mu}(T_i; \\mathbf{X_i})\\).\n\n\nEstimate CATE as \\(\\hat{\\tau}(\\mathbf{x}) = \\hat{\\mu}(1; \\mathbf{X_i}) - \\hat{\\mu}(0; \\mathbf{X_i})\\).\n\n\n\n\n\nT-learner:\n\n\nUse ML estimator of your choice to fit model \\(\\hat{\\mu}(1; \\mathbf{X_i})\\) in treated subsample.\n\n\nUse ML estimator of your choice to fit model \\(\\hat{\\mu}(0; \\mathbf{X_i})\\) in control subsample.\n\n\nEstimate CATE as \\(\\hat{\\tau}(\\mathbf{x}) = \\hat{\\mu}(1; \\mathbf{X_i}) - \\hat{\\mu}(0; \\mathbf{X_i})\\)."
  },
  {
    "objectID": "slides/06_hte.html#s-learner-and-t-learner-example",
    "href": "slides/06_hte.html#s-learner-and-t-learner-example",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "S-Learner and T-Learner: Example",
    "text": "S-Learner and T-Learner: Example\n\nAssess the effect of 401(k) program participation on net financial assets of 9,915 households in the US in 1991.\nExamples without proper cross-fitting.\n\n\n\n\nlibrary(hdm) # for the data\nlibrary(grf) # generalized random forests, could also use mlr3\n\n\n# Get data\ndata(pension)\n# Outcome\nY = pension$net_tfa\n# Treatment\nT = pension$p401\n# Create main effects matrix\nX = model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension)\n\n# Implement the S-Learner\nTX = cbind(T,X)\nrf = regression_forest(TX,Y)\nT0X = cbind(rep(0,length(Y)),X)\nT1X = cbind(rep(1,length(Y)),X)\ncate_sl = predict(rf,T1X)$predictions - predict(rf,T0X)$predictions\nhist(cate_sl)\n\n\n# Implement the T-Learner\nrfmu1 = regression_forest(X[T==1,],Y[T==1])\nrfmu0 = regression_forest(X[T==0,],Y[T==0])\ncate_tl = predict(rfmu1, X)$predictions - predict(rfmu0, X)$predictions\nhist(cate_tl)"
  },
  {
    "objectID": "slides/06_hte.html#s-learner-and-t-learner-disadvantage",
    "href": "slides/06_hte.html#s-learner-and-t-learner-disadvantage",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "S-Learner and T-Learner: Disadvantage",
    "text": "S-Learner and T-Learner: Disadvantage\n\nThe prediction problems do not know of joint goal to approximate a difference:\n\n\\(\\hat{\\mu}(1; \\mathbf{X_i})\\) minimizes \\(MSE(\\hat{\\mu}(1; \\mathbf{x})) = \\mathbb{E}[(\\hat{\\mu}(1; \\mathbf{x}) - \\mu(1; \\mathbf{X_i}))^2]\\).\n\\(\\hat{\\mu}(0; \\mathbf{X_i})\\) minimizes \\(MSE(\\hat{\\mu}(0; \\mathbf{x})) = \\mathbb{E}[(\\hat{\\mu}(0; \\mathbf{x}) - \\mu(0; \\mathbf{X_i}))^2]\\).\nBUT they should aim to minimize:\n\n\n\\[\n\\begin{align*}\n\\text{MSE}(\\hat{\\tau}(\\mathbf{x}))) &= \\mathbb{E}[(\\hat{\\tau}(\\mathbf{x})) - \\tau(\\mathbf{x})))^2] \\\\\n&= \\mathbb{E}[(\\hat{\\mu}(1, \\mathbf{x})) - \\hat{\\mu}(0, \\mathbf{x})) - (\\mu(1, \\mathbf{x})) - \\mu(0, \\mathbf{x}))))^2] \\\\\n&= \\mathbb{E}[(\\hat{\\mu}(1, \\mathbf{x})) - \\mu(1, \\mathbf{x})))^2] + \\mathbb{E}[(\\hat{\\mu}(0, \\mathbf{x})) - \\mu(0, \\mathbf{x})))^2] \\\\\n&\\quad - 2\\mathbb{E}[(\\hat{\\mu}(1, \\mathbf{x})) - \\mu(1, \\mathbf{x})))(\\hat{\\mu}(0, \\mathbf{x})) - \\mu(0, \\mathbf{x})))] \\\\\n&= \\text{MSE}(\\hat{\\mu}(1, \\mathbf{x}))) + \\text{MSE}(\\hat{\\mu}(0, \\mathbf{x}))) - 2\\text{MCE}(\\hat{\\mu}(1, \\mathbf{x})), \\hat{\\mu}(0, \\mathbf{x})))\n\\end{align*}\n\\]\n\n\nLechner (2018) calls the additional term Mean Correlated Error (MCE): correlated errors matter less\nExample - both make same error: \\(\\hat{\\mu}(1; \\mathbf{X_i}) = \\mu(1; \\mathbf{X_i}) + 2\\) and \\(\\hat{\\mu}(0; \\mathbf{X_i}) = \\mu(0; \\mathbf{X_i}) + 2\\)\n\nBut their CATE would still be on point: \\(MSE(\\hat{\\tau}(\\mathbf{x})) = 4 + 4 - 2(2 \\cdot 2) = 0\\)\n\nExample - errors go in different direction: \\(\\hat{\\mu}(1; \\mathbf{X_i}) = \\mu(1; \\mathbf{X_i}) + 2\\) and \\(\\hat{\\mu}(0; \\mathbf{X_i}) = \\mu(0; \\mathbf{X_i}) - 2\\)\n\nBut their CATE would be off: \\(MSE(\\hat{\\tau}(\\mathbf{x})) = 4 + 4 - 2(2 \\cdot (-2)) = 16\\)"
  },
  {
    "objectID": "slides/06_hte.html#two-approaches-to-improvements",
    "href": "slides/06_hte.html#two-approaches-to-improvements",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Two Approaches to Improvements",
    "text": "Two Approaches to Improvements\n\nModify supervised ML methods to target causal effect estimation\n\nMethod specific, e.g.:\n\nCausal tree (Athey and Imbens, 2016)\nCausal forest (Athey, Tibshirani & Wager, 2019)\n\nNot covered here (does not scale very well to high-dimensional data)\n\nCombine supervised ML methods to target causal effect estimation\n\nGeneric approach - Metalearners, e.g.:\n\nX-learner (Künzel et al., 2019)\n\nnot covered here; handles sample imbalance, but not doubly robust\n\nR-learner\nDR-learner"
  },
  {
    "objectID": "slides/06_hte.html#what-are-metalearners",
    "href": "slides/06_hte.html#what-are-metalearners",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "What are Metalearners?",
    "text": "What are Metalearners?\n\nMetalearners combine multiple supervised ML steps in a pipeline that outputs predicted CATEs.\nThe common ones require the following steps:\n\nEstimate nuisance parameters using suitable ML method.\nPlug them into a clever minimization problem targeting CATE.\nSolve the minimization problem using suitable ML method.\nPredict CATE using the model learned in 3.\n\nMost popular ML methods are suitable and can be applied in steps 1, 3 and 4.\nLike for standard prediction methods, statistical inference is usually not available."
  },
  {
    "objectID": "slides/06_hte.html#r-learner-idea",
    "href": "slides/06_hte.html#r-learner-idea",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "R-learner: Idea",
    "text": "R-learner: Idea\n\nPartially linear model, but now allowing for treatment effects that vary with \\(\\mathbf{X}\\):\n\n\\(Y_i = \\tau(\\mathbf{X_i}) T_i + g(\\mathbf{X_i}) + \\epsilon_{Y_i}, \\quad \\mathbb{E}(\\epsilon_{Y_i} | T_i,\\mathbf{X_i}) = 0\\)\n=&gt; \\(\\underbrace{Y_i - \\overbrace{\\mathbb{E}[Y_i \\mid \\mathbf{X_i}]}^{\\mu(\\mathbf{X_i})}}_{\\text{outcome residual}} = \\tau(\\mathbf{X_i}) \\underbrace{( T_i - \\overbrace{\\mathbb{E}[T_i \\mid \\mathbf{X_i}]}^{e(\\mathbf{X_i})})}_{\\text{treatment residual}} + \\epsilon_{Y_i}\\)\n\nThis motivates the R-learner of Nie and Wager, 2020:\n\n\\(\\hat{\\tau}_{\\text{RL}}(\\mathbf{x}) = \\arg \\min_{\\tau} \\sum_{i=1}^n ( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tau(\\mathbf{X_i}) ( T_i - \\hat{e}(\\mathbf{X_i})))^2\\)\nwith cross-fitted high-quality nuisance parameters from first step.\nBut how to estimate it?"
  },
  {
    "objectID": "slides/06_hte.html#r-learner-with-linear-ml-methods",
    "href": "slides/06_hte.html#r-learner-with-linear-ml-methods",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "R-learner with Linear ML-Methods",
    "text": "R-learner with Linear ML-Methods\n\nCATE as linear function \\(\\tau(\\mathbf{X_i}) = \\mathbf{\\beta' X_i}\\):\n\n\\[\n\\begin{align*}\n\\hat{\\beta}_{RL} &= \\underset{\\beta}{\\operatorname{arg\\,min}} \\sum_{i=1}^N( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\mathbf{\\beta'} \\underbrace{(T_i - \\hat{e}(\\mathbf{X_i})) \\mathbf{X_i}}_{=\\mathbf{\\tilde{X}_i}})^2 \\\\\n&= \\underset{\\beta}{\\operatorname{arg\\,min}} \\sum_{i=1}^N \\left( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\mathbf{\\beta'} \\mathbf{\\tilde{X}_i} \\right)^2\n\\end{align*}\n\\]\n\n\\(\\mathbf{\\tilde{X}_i} = (T_i - \\hat{e}(\\mathbf{X_i})) \\mathbf{X_i}\\) are modified / pseudo-covariates.\n\\(\\hat{\\tau}_{\\text{RL}}(\\mathbf{x}) = \\mathbf{\\hat{\\beta}_{RL} x} \\neq \\mathbf{\\hat{\\beta}_{RL} \\tilde{x}}\\) is the estimated CATE for a specific \\(\\mathbf{x}\\).\nAll linear shrinkage estimators (Lasso and friends) can be applied, nuisance parameters can still be estimated with non-linear ML."
  },
  {
    "objectID": "slides/06_hte.html#r-learner-with-generic-ml-methods",
    "href": "slides/06_hte.html#r-learner-with-generic-ml-methods",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "R-learner with Generic ML-Methods",
    "text": "R-learner with Generic ML-Methods\n\nIf we are not willing to impose linearity of the CATE, we can rewrite the R-learner:\n\n\\[\n\\begin{align*}\n\\hat{\\tau}_{\\text{RL}}(\\mathbf{x}) &= \\arg \\min_{\\tau} \\sum_{i=1}^n ( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tau(\\mathbf{X_i}) ( T_i - \\hat{e}(\\mathbf{X_i})))^2 \\\\\n&= \\arg \\min_{\\tau} \\sum_{i=1}^n \\frac{( T_i - \\hat{e}(\\mathbf{X_i}))^2}{(T_i - \\hat{e}(\\mathbf{X_i}))^2}(Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tau(\\mathbf{X_i}) ( T_i - \\hat{e}(\\mathbf{X_i})))^2 \\\\\n&= \\arg \\min_{\\tau} \\sum_{i=1}^n (T_i - \\hat{e}(\\mathbf{X_i}))^2 \\bigg(\\frac{Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tau(\\mathbf{X_i}) ( T_i - \\hat{e}(\\mathbf{X_i}))}{ T_i - \\hat{e}(\\mathbf{X_i})}\\bigg)^2 \\\\\n&= \\arg \\min_{\\tau} \\sum_{i=1}^n (T_i - \\hat{e}(\\mathbf{X_i}))^2 \\bigg(\\frac{Y_i - \\hat{\\mu}(\\mathbf{X_i})}{ T_i - \\hat{e}(\\mathbf{X_i})} - \\tau(\\mathbf{X_i})\\bigg)^2 \\\\\n\\end{align*}\n\\]\n\nSupervised ML methods that can deal with weighted minimization (e.g. neural nets, random forest, boosting, …) with\n\nweights: \\((T_i - \\hat{e}(\\mathbf{X_i}))^2\\).\npseudo-outcome: \\(\\frac{Y_i - \\hat{\\mu}(\\mathbf{X_i})}{ T_i - \\hat{e}(\\mathbf{X_i})}\\).\nthe unmodified covariates: \\(\\mathbf{X_i}\\)."
  },
  {
    "objectID": "slides/06_hte.html#dr-learner",
    "href": "slides/06_hte.html#dr-learner",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "DR-learner",
    "text": "DR-learner\n\nRecall the pseudo-outcome of the AIPW-ATE from previous lecture and condition on \\(\\mathbf{X_i}\\) (same “trick” as for GATE estimation):\n\n\\(\\tau_{\\text{DR}}(\\mathbf{x}) = \\mathbb{E}\\bigg[ \\underbrace{\\hat{\\mu}(1, \\mathbf{X_i}) - \\hat{\\mu}(0, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\hat{\\mu}(1, \\mathbf{X_i}))}{ \\hat{e}_1(\\mathbf{X_i})} - \\frac{(1-T_i)(Y_i - \\hat{\\mu}(0, \\mathbf{X_i})}{\\hat{e}_0(\\mathbf{X_i}))}}_{\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}} \\bigg| \\mathbf{X_i= x} \\bigg]\\)\n\\(\\tau_{\\text{DR}}(\\mathbf{x}) = \\mathbb{E}\\bigg[ \\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} \\bigg| \\mathbf{X_i= x} \\bigg]\\)\n\nDR-learner by Kennedy (2020) uses \\(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}\\) in a generic ML problem:\n\n\\(\\hat{\\tau}_{RL}(\\mathbf{x}) = \\underset{\\tau}{\\operatorname{arg\\,min}} \\sum_{i=1}^N \\left( \\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} - \\tau(\\mathbf{X_i})\\right)^2\\)\nCross-fitting: in 4 subsamples (1) train a model for \\(\\hat{e(\\mathbf{X_i})}\\), (2) train a model for \\(\\hat{\\mu(\\mathbf{X_i})}\\), (3) construct \\(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}\\) and regress on \\(\\mathbf{X_i}\\), (4) predict \\(\\hat{\\tau}_{RL}(\\mathbf{x})\\). Then rotate."
  },
  {
    "objectID": "slides/06_hte.html#dr-learner-example",
    "href": "slides/06_hte.html#dr-learner-example",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "DR-learner: Example",
    "text": "DR-learner: Example\n\nAssess the effect of 401(k) program participation on net financial assets of 9,915 households in the US in 1991.\n\n\n\n\nlibrary(hdm) # for the data\nlibrary(causalDML) # generalized random forests, could also use mlr3\n\n\n# Get data\ndata(pension)\n# Outcome\nY = pension$net_tfa\n# Treatment\nT = pension$p401\n# Create main effects matrix\nX = model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension)\n\n# Implement the DR-Learner\ndr = dr_learner(Y,T,X,\n      ml_w = list(create_method(\"forest_grf\")),\n      ml_y = list(create_method(\"forest_grf\")),\n      ml_tau = list(create_method(\"forest_grf\"))\n)\n\n# DR-learner distribution of B-A\nhist(dr$cates[,1])"
  },
  {
    "objectID": "slides/06_hte.html#how-to-evaluate-estimated-cates",
    "href": "slides/06_hte.html#how-to-evaluate-estimated-cates",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "How to evaluate estimated CATEs?",
    "text": "How to evaluate estimated CATEs?\n\nDescriptive: histogram, kernel density plots, box plots, etc. …\nInference: test whether effect heterogeneity is systematic or just noise.\nExplore what drives the heterogeneous effects.\n\n\n\nChallenges with inference:\n\nUnique to causal ML: Due to missing counterfactual, we cannot benchmark predicted against effect =&gt; no classic out-of-sample testing.\nShared with supervised ML: statistical inference for predicted CATE is not available or at least challenging.\n\nApproach to inference:\n\nRather than (consistent) estimation of & inference on the individual CATEs directly, derive summary statistics of their (noisy) distribution.\nTest joint hypothesis that there is effect heterogeneity & the applied estimation method is able to detect it at least partially.\n\n\n\n\n\nWe discuss the three methods proposed by Chernozhukov et al. (2017-2023):\n\nBest Linear Predictor (BLP).\nHigh-vs.-low Sorted Group Average Treatment Effect (GATES).\nClassification Analysis (CLAN) to explore what drives the heterogeneous effects."
  },
  {
    "objectID": "slides/06_hte.html#best-linear-predictor-blp---definition",
    "href": "slides/06_hte.html#best-linear-predictor-blp---definition",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Best Linear Predictor (BLP) - Definition",
    "text": "Best Linear Predictor (BLP) - Definition\n\nBLP is defined as the solution of the hypothetical regression of the true CATE on the demeaned predicted CATE:\n\n\n\n\nDefinition “Best Linear Predictor (BLP)”\n\n\nThe best linear predictor of \\(\\tau(\\mathbf{X_i})\\) by \\(\\hat{\\tau}(\\mathbf{X_i})\\) is the solution to:\n\\((\\beta_1, \\beta_2) = \\underset{\\tilde{\\beta_1}, \\tilde{\\beta_2}}{\\operatorname{arg\\,min}} \\space \\mathbb{E} \\left[ \\left( \\tau(\\mathbf{X_i}) - \\tilde{\\beta_1} - \\tilde{\\beta_2} \\left( \\hat{\\tau}(\\mathbf{X_i}) - \\mathbb{E}[\\hat{\\tau}(\\mathbf{X_i})] \\right) \\right)^2 \\right]\\)\n\nwhich, if exists, is defined as\n\n\\(\\mathbb{E}[\\tau(\\mathbf{X_i}) | \\hat{\\tau}(\\mathbf{X_i}) ] := \\beta_1 + \\beta_2\\underbrace{(\\hat{\\tau}(\\mathbf{X_i}) - \\mathbb{E}[\\hat{\\tau}(\\mathbf{X_i})])}_{\\text{demeaned prediction}}\\)\n\nwhere\n\n\\(\\beta_1 = \\mathbb{E}[\\tau(\\mathbf{X_i})] = \\text{ATE} \\text{ (because of the demeaning)}\\)\n\\(\\beta_2 = \\frac{\\text{Cov}[\\tau(\\mathbf{X_i}), \\hat{\\tau}(\\mathbf{X_i})]}{\\text{Var}[\\hat{\\tau}(\\mathbf{X_i})]}\\)"
  },
  {
    "objectID": "slides/06_hte.html#blp---interpretation",
    "href": "slides/06_hte.html#blp---interpretation",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "BLP - Interpretation",
    "text": "BLP - Interpretation\n\n\\(\\beta_2 = \\frac{\\text{Cov}[\\tau(\\mathbf{X_i}), \\hat{\\tau}(\\mathbf{X_i})]}{\\text{Var}[\\hat{\\tau}(\\mathbf{X_i})]} = 1\\) if \\(\\hat{\\tau}(\\mathbf{X_i}) = \\tau(\\mathbf{X_i})\\) (what we would like to see)\n\\(\\beta_2 = 0\\) if \\(\\text{Cov}[\\tau(\\mathbf{X_i}), \\hat{\\tau}(\\mathbf{X_i})] = 0\\), which can have two reasons:\n\n\n\\(\\tau(\\mathbf{X_i})\\) is constant (no heterogeneity to detect).\n\n\n\\(\\tau(\\mathbf{X_i})\\) is not constant but the estimator is not capable of finding it (bad estimator and/or not enough observations).\n\n\nTherefore, testing \\(H_0: \\beta_2 = 0\\) is a joint test of\n\n\nexistence of heterogeneity and\n\n\nthe estimators capability to find it."
  },
  {
    "objectID": "slides/06_hte.html#blp---identification-strategy-a",
    "href": "slides/06_hte.html#blp---identification-strategy-a",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "BLP - Identification Strategy A",
    "text": "BLP - Identification Strategy A\nStrategy A: Weighted residual BLP\n\n\\((\\beta_1, \\beta_2) = \\underset{\\tilde{\\beta_1}, \\tilde{\\beta_2}}{\\operatorname{arg\\,min}} \\space \\mathbb{E} \\left[ \\omega(\\mathbf{X_i}) \\left( Y_i - \\tilde{\\beta_1}(T_i - e(X_i)) - \\tilde{\\beta_2} (T_i - e(X_i)) \\left( \\hat{\\tau}(\\mathbf{X_i}) - \\mathbb{E}[\\hat{\\tau}(\\mathbf{X_i})] \\right) \\color{#005e73}{- \\alpha \\mathbf{X^{C}_{i}}} \\right)^2 \\right]\\)\nwhere:\n\n\\(\\omega(\\mathbf{X_i}) = \\frac{1}{e(\\mathbf{X_i})(1-e(\\mathbf{X_i}))}\\)\n\\(\\color{#005e73}{\\mathbf{X^{C}_{i}}}\\) is not required for identification, but contains optional functions of \\(X_i\\) to reduce estimation noise, e.g. \\([1,\\hat\\mu(0,\\mathbf{X_i}), e(\\mathbf{X_i}), e(\\mathbf{X_i})\\hat{\\tau}(\\mathbf{X_i})]\\)\n\nSee Appendix A in Chernozhukov et al. (2017-2023) for a detailed derivation."
  },
  {
    "objectID": "slides/06_hte.html#blp---identification-strategy-b",
    "href": "slides/06_hte.html#blp---identification-strategy-b",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "BLP - Identification Strategy B",
    "text": "BLP - Identification Strategy B\nStrategy B: Horvitz-Thompson BLP\n\n\\((\\beta_1, \\beta_2) = \\underset{\\tilde{\\beta_1}, \\tilde{\\beta_2}}{\\operatorname{arg\\,min}} \\space \\mathbb{E} \\left[  \\left( H_iY_i - \\tilde{\\beta_1} - \\tilde{\\beta_2} \\left( \\hat{\\tau}(\\mathbf{X_i}) - \\mathbb{E}[\\hat{\\tau}(\\mathbf{X_i})] \\right) \\color{#005e73}{- \\alpha H_i \\mathbf{X^{C}_{i}}} \\right)^2 \\right]\\)\nwhere:\n\n\\(H_i = \\frac{T_i-e(\\mathbf{X_i})}{e(\\mathbf{X_i})(1-e(\\mathbf{X_i}))}\\) are the Horvitz-Thompson (IPW) weights.\n\\(H_iY_i\\) serves as a pseudo-outcome.\n\\(\\color{#005e73}{\\mathbf{X^{C}_{i}}}\\) is not required for identification, but contains optional functions of \\(X_i\\) to reduce estimation noise, e.g. \\([1,\\hat\\mu(0,\\mathbf{X_i}), e(\\mathbf{X_i}), e(\\mathbf{X_i})\\hat{\\tau}(\\mathbf{X_i})]\\)\n\nSee Appendix A in Chernozhukov et al. (2017-2023) for a detailed derivation."
  },
  {
    "objectID": "slides/06_hte.html#sorted-group-average-treatment-effect-gates",
    "href": "slides/06_hte.html#sorted-group-average-treatment-effect-gates",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Sorted Group Average Treatment Effect (GATES)",
    "text": "Sorted Group Average Treatment Effect (GATES)\n\n\n\nIdea:\n\nslice the distribution of \\(\\hat{\\tau}(\\mathbf{X_i})\\) into \\(K\\) parts and compare the average treatment effect of individuals within each slice.\nif \\(\\hat{\\tau}(\\mathbf{X_i})\\) is a good approximation of \\(\\tau(\\mathbf{X_i})\\), then we expect to observe the following monotonicity: \\(\\gamma_1 \\leq \\gamma_2 \\leq \\ldots \\leq \\gamma_K\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition “Sorted Group Average Treatment Effect (GATES)”\n\n\n\\(\\gamma_k := \\mathbb{E}[ \\tau(\\mathbf{X_i}) | G_k]\\), \\(k = 1, \\ldots, K\\)\n\nwhere \\(G_k= \\{\\hat{\\tau}(\\mathbf{X_i}) \\in I_k \\}\\) with \\(I_k = [l_{k-1},l_k)\\) and \\(-\\infty = l_0 &lt; l_1 &lt; \\dots &lt; l_K = \\infty\\)."
  },
  {
    "objectID": "slides/06_hte.html#gates---identification",
    "href": "slides/06_hte.html#gates---identification",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "GATES - Identification",
    "text": "GATES - Identification\nStrategy A: Weighted residual GATES\n\n\\((\\gamma_1, \\dots, \\gamma_K) = \\underset{\\tilde{\\gamma}_1, \\dots, \\tilde{\\gamma}_K}{\\operatorname{arg\\,min}} \\space \\mathbb{E} \\left[ \\omega(\\mathbf{X_i}) \\left( Y_i - \\sum_k\\tilde{\\gamma_k}(T_i - e(X_i))\\mathbb{1}[G_k]  \\color{#005e73}{- \\alpha \\mathbf{X^{C}_{i}}} \\right)^2 \\right]\\)\n\nwhere \\(\\omega(\\mathbf{X_i}) = \\frac{1}{e(\\mathbf{X_i})(1-e(\\mathbf{X_i}))}\\).\n\n\nStrategy B: Horvitz-Thompson GATES\n\n\\((\\gamma_1, \\dots, \\gamma_K) = \\underset{\\tilde{\\gamma}_1, \\dots, \\tilde{\\gamma}_K}{\\operatorname{arg\\,min}} \\space \\mathbb{E} \\left[  \\left( H_iY_i -\\sum_k \\tilde{\\gamma_k}\\mathbb{1}[G_k] \\color{#005e73}{- \\alpha H_i \\mathbf{X^{C}_{i}}} \\right)^2 \\right]\\)\n\nwhere \\(H_iY_i\\) serves as a pseudo-outcome and \\(H_i = \\frac{T_i-e(\\mathbf{X_i})}{e(\\mathbf{X_i})(1-e(\\mathbf{X_i}))}\\) being the Horvitz-Thompson (IPW) weights.\n\n\\(\\color{#005e73}{\\mathbf{X^{C}_{i}}}\\) is not required for identification, but contains optional functions of \\(X_i\\) to reduce estimation noise, e.g. \\([1,\\hat\\mu(0,\\mathbf{X_i}), e(\\mathbf{X_i}), e(\\mathbf{X_i})\\hat{\\tau}(\\mathbf{X_i})]\\)\nSee Appendix A in Chernozhukov et al. (2017-2023) for a detailed derivation."
  },
  {
    "objectID": "slides/06_hte.html#classification-analysis-clan",
    "href": "slides/06_hte.html#classification-analysis-clan",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Classification Analysis (CLAN)",
    "text": "Classification Analysis (CLAN)\nClassification Analysis (CLAN) can be implemented by simple mean comparisons of covariates in extreme GATES groups:\n\n\n\nDefinition “Classification Analysis (CLAN)”\n\n\nClassification Analysis (CLAN) compares the covariate values of the least affected group G1 with the most affected group GK defined for the GATES:\n\n\\(\\delta_K - \\delta_1\\)\n\nwhere\n\n\\(\\delta_k = \\mathbb{E}[X_i | G_k] = \\frac{1}{n_k} \\sum_{i=1}^{n} X_i \\mathbb{1}[G_k]\\)."
  },
  {
    "objectID": "slides/06_hte.html#blp-gates-clan---implementation",
    "href": "slides/06_hte.html#blp-gates-clan---implementation",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "BLP, GATES & CLAN - Implementation",
    "text": "BLP, GATES & CLAN - Implementation\n\nR package GenericML by Welz, Alfons, Demirer, and Chernozhukov (2022). \nAlgorithm:\n\nIN: \\(\\text{Data} = (Y_i, \\mathbf{X_i}, T_i )^{N}_{i=1}\\), significance level \\(\\alpha\\), a suite of ML methods, number of splits \\(S\\).\nOUT: \\(p-\\text{values}\\) and \\((1-2\\alpha)\\) confidence intervals of point estimates of each target parameter in GATES, BLP, and CLAN. \n\n\nCompute propensity scores \\(\\hat{e}(\\mathbf{X_i})\\).\nDo S splits of \\(\\{1, . . . ,N\\}\\) into disjoint sets \\(A\\) and \\(M\\) of same size.\nfor each ML method and each split \\(s = 1, . . . , S\\), do\n\nTune and train each ML method to learn \\(\\hat{\\mu}(0, \\mathbf{X_i})\\) and \\(\\hat{\\tau}(\\mathbf{X_i})\\) on A.\nOn M, use \\(\\hat{\\mu}(0, \\mathbf{X_i})\\) and \\(\\hat{\\tau}(\\mathbf{X_i})\\) to estimate the BLP, GATES, CLAN target parameters.\nCompute some performance measures for the ML methods.\n\nChoose the best ML method based on the medians of the performance measures.\nCalculate the medians of the confidence bounds, p-values, and point estimates of each target parameter.\nAdjust the confidence bounds and p-values."
  },
  {
    "objectID": "slides/06_hte.html#more-references",
    "href": "slides/06_hte.html#more-references",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "More References",
    "text": "More References\n\nCATE Prediction Methods:\n\nBART (Hahn, Murray & Carvalho, 2020).\nCausal Boosting/MARS, … (Powers, Qian, Jung, Schuler, Shah, Hastie & Tibshirani, 2019).\nDragonnet (Shi, Blei & Veitch, 20191).\nModified Causal Forest (Lechner & Mareckova, 2022).\nOrthogonal Random Forest (Oprescu, Syrgkanis & Wu, 2019).\nTARNet (Shalit, Johansson & Sontag 2019).\nX-learner (Künzel, Sekhon, Bickel & Yu, 2019).\n\n\n\n\nHET Evaluation:\n\nRank-Weighted Average Treatment Effect (RATE) (Yadlowsky et al., 2021).\nCalibration Error for Heterogeneous Treatment Effects (Xu & Yadlowsky, 2022).\nMore on GATES in experiments (Imai & Li, 2022-2024)."
  },
  {
    "objectID": "slides/06_hte.html#optimal-policy-learning---goal",
    "href": "slides/06_hte.html#optimal-policy-learning---goal",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Optimal Policy Learning - Goal",
    "text": "Optimal Policy Learning - Goal\n\nFrom evaluation (What works for whom?) towards data-driven (personalized) treatment recommendations:\n\nHow to optimally treat whom?  \n\nNotation:\n\nBinary treatment indicator: \\(T_i \\in \\{0, 1\\}\\)\nPotential outcome (PO) under treatment \\(t\\): \\(Y_i(t)\\)\nExogenous covariate(s): \\(\\mathbf{X_i}\\)\nConditional Average PO: \\(\\mu_t(\\mathbf{x}) := \\mathbb{E}[Y(t) \\mid \\mathbf{X_i = x}]\\)\nConditional Average Treatment Effect (CATE): \\(\\tau(\\mathbf{x}) := \\mu_1(\\mathbf{x}) - \\mu_0(\\mathbf{x})\\)\n\nAdditional notation:\n\nPolicy rule for \\(x\\) (conditional treatment choice): \\(\\pi(\\mathbf{X_i}) \\in \\{0,1\\}\\).\nPO under policy \\(\\pi(\\mathbf{X_i})\\): \\(Y_i(\\pi(\\mathbf{X_i}))\\).\nValue function (average PO under policy \\(\\pi(\\mathbf{X_i})\\)): \\(Q(\\pi) := \\mathbb{E}[Y_i(\\pi(\\mathbf{X_i}))]\\).  \n\nGoal: Find the optimal policy \\(\\pi^*\\) that maximizes the value function \\(Q(\\pi)\\)."
  },
  {
    "objectID": "slides/06_hte.html#optimal-policy-alternatives",
    "href": "slides/06_hte.html#optimal-policy-alternatives",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Optimal Policy Alternatives",
    "text": "Optimal Policy Alternatives\n\n\nAssign individuals to treatment with higher PO under treatment than without?\n\n\n\\(\\pi^* = \\mathbb{1}[Y_i(1) &gt; Y_i(0)] = \\mathbb{1}[Y_i(1) - Y_i(0) &gt; 0] = \\mathbb{1}[\\tau_i &gt; 0]\\)\nFundamental problem of causal inference: counterfactuals unknown.\n\n\n\n\n\nAssign individuals to treatment with higher CATE than without?\n\n\n\\(\\pi^* = \\mathbb{1}[Y_i(1) &gt; Y_i(0) | \\mathbf{X_i = x}] = \\mathbb{1}[\\tau(\\mathbf{X_i = x}) &gt; 0]\\)\nProblem: minimizing \\(\\text{MSE}_{\\text{CATE}} = \\mathbb{E}[(\\hat{\\tau}(\\mathbf{x}) - \\tau(\\mathbf{x})^2]\\) does not necessarily improve downstream policy rule learning (Qian & Murphy, 2011).\nSimilar to the case where MSE minimization in treated and control groups separately is not the best strategy to minimize CATE MSE.\n\n\n\n\n\n\nInstead: \\(\\pi^* = \\underset{\\pi}{\\operatorname{arg\\,min}} \\space \\mathbb{E}[Y_i(\\pi(\\mathbf{X_i}))] = \\underset{\\pi}{\\operatorname{arg\\,min}} \\space Q(\\pi(\\mathbf{X_i})))\\)"
  },
  {
    "objectID": "slides/06_hte.html#optimal-policy-objective-function",
    "href": "slides/06_hte.html#optimal-policy-objective-function",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Optimal Policy Objective Function",
    "text": "Optimal Policy Objective Function\n\nObjective function can have many different forms but one has proven very useful in the context of ML policy learning:\n\nComparing the value function against a benchmark policy that assigns treatments via fair coin flip:\n\n50-50 chance of being treated: \\(\\pi^{\\text{coin}} \\sim \\text{Bernoulli}(0,5)\\).\n\n\n\n\\[\n\\begin{align*}\n\\pi^* &= \\arg \\max_{\\pi} Q(\\pi) = \\arg \\max_{\\pi} \\mathbb{E}[Y(\\pi)] = \\arg \\max_{\\pi} \\mathbb{E}[Y(\\pi) \\color{#005e73}{- 0.5 \\mathbb{E}[Y(1)] + 0.5 \\mathbb{E}[Y(0)]}] \\\\\n&= \\arg \\max_{\\pi} \\mathbb{E}[\\pi Y(1) + (1 - \\pi) Y(0)] - 0.5 \\mathbb{E}[Y(1)] - 0.5 \\mathbb{E}[Y(0)] \\\\\n&= \\arg \\max_{\\pi} \\mathbb{E}[(\\pi - 0.5) Y(1)] + \\mathbb{E}[(0.5 - \\pi) Y(0)] = \\arg \\max_{\\pi} \\mathbb{E}[(\\pi - 0.5) (Y(1) - Y(0))] \\\\\n&= \\arg \\max_{\\pi}  \\color{#005e73}{2} \\mathbb{E}[(\\pi - 0.5) (Y(1) - Y(0))] \\\\\n&= \\arg \\max_{\\pi} \\mathbb{E}[(2\\pi - 1)(Y(1) - Y(0))] \\\\\n&\\overset{LIE}{=} \\arg \\max_{\\pi} \\mathbb{E}[(2\\pi - 1) \\tau(\\mathbf{X_i})] \\\\\n&= \\arg \\max_{\\pi}  \\underbrace{\\mathbb{E}[|\\tau(\\mathbf{X_i})| \\text{sign}(\\tau(\\mathbf{X_i})) (2\\pi(\\mathbf{X_i}) - 1)]}_{A(\\pi)} \\\\\n\\end{align*}\n\\]\n\nwhere \\((2\\pi(\\mathbf{X_i}) - 1) \\in \\{-1,1\\}\\) is one if policy assigns treatment and minus one if not."
  },
  {
    "objectID": "slides/06_hte.html#optimal-policy-objective-function---intuition",
    "href": "slides/06_hte.html#optimal-policy-objective-function---intuition",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Optimal Policy Objective Function - Intuition",
    "text": "Optimal Policy Objective Function - Intuition\n\n\\(A(\\pi) := \\mathbb{E}[|\\tau(\\mathbf{X_i})| \\text{sign}(\\tau(\\mathbf{X_i})) (2\\pi(\\mathbf{X_i}) - 1)]\\) measures the advantage of a policy compared to random allocation:\n\nIf \\(\\text{sign}(\\tau(\\mathbf{X_i})) (2\\pi(\\mathbf{X_i}) - 1) = 1\\), i.e. if the policy picks the better treatment for \\(\\mathbf{X_i}\\), we earn the absolute value of the CATE.\nIf \\(\\text{sign}(\\tau(\\mathbf{X_i})) (2\\pi(\\mathbf{X_i}) - 1) = -1\\), i.e. if the policy picks the worse treatment for \\(\\mathbf{X_i}\\), we lose the absolute value of the CATE.\n\nWe need to get it right for those with biggest CATEs, those with CATEs close to zero are negligible.\nThis shows the difference to CATE MSE minimization, where we need to find good approximations everywhere."
  },
  {
    "objectID": "slides/06_hte.html#optimal-policy-identification-estimation",
    "href": "slides/06_hte.html#optimal-policy-identification-estimation",
    "title": "(6) Heterogeneous Treatment Effects",
    "section": "Optimal Policy Identification & Estimation",
    "text": "Optimal Policy Identification & Estimation\n\nPotential outcomes or CATE functions unknown, need to be identified before optimization.\nAthey and Wager (2021) recommend the pseudo-outcome (again) because of all the nice properties:\n\n\\(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} = \\hat{\\mu}(1, \\mathbf{X_i}) - \\hat{\\mu}(0, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\hat{\\mu}(1, \\mathbf{X_i}))}{\\hat{e}_1(\\mathbf{X_i})} - \\frac{(1-T_i)(Y_i - \\hat{\\mu}(0, \\mathbf{X_i})}{\\hat{e}_0(\\mathbf{X_i}))}\\)\n\n\n\n\nBinary weighted classification problem: classify the sign of the CATE while favoring correct classifications with larger absolute CATEs.\n\n\\(\\hat{\\pi} = \\underset{\\pi \\in \\Pi}{\\arg \\max} \\left\\{ \\frac{1}{N} \\sum_{i=1}^N \\overbrace{|\\hat{Y}_{i,\\text{ATE}}|}^{\\text{weight}} \\underbrace{\\operatorname{sign}(\\hat{Y}_{i,\\text{ATE}}}_{\\text{to be classified}} \\overbrace{(2\\pi(X_i) - 1)}^{\\text{function to be learned}} \\right\\}\\)\n\nPossible methods: e.g. decision trees/forests, logistic lasso, SVM, etc."
  }
]