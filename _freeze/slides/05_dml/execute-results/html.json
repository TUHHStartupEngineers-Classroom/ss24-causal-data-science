{
  "hash": "9456d0554b9d5ed35d07241f05c86006",
  "result": {
    "engine": "knitr",
    "markdown": "---\n# TITLE & AUTHOR\ntitle: \"(5) Double Machine Learning\"\nsubtitle: \"Causal Data Science for Business Analytics\"\nauthor: \"Christoph Ihl\"\ninstitute: \"Hamburg University of Technology\"\ndate: today\ndate-format: \"dddd, D. MMMM YYYY\"\n# FORMAT OPTIONS\nformat: \n  revealjs:\n    width: 1600\n    height: 900\n    footer: \"Causal Data Science: (5) Double Machine Learning\"\n    slide-number: true\n---\n\n\n\n\n# Preliminaries {data-stack-name=\"Preliminaries\"}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n<style type=\"text/css\">\ndiv.callout-note{border-left-color:#00C1D4 !important}div.callout-note.callout-style-default .callout-title{background-color:#005e73;color:white}.callout.callout-style-default{border-left:solid #005e73 .3rem;border-right:solid 1px #005e73;border-top:solid 1px #005e73;border-bottom:solid 1px #005e73}\n</style>\n:::\n\n\n\n\n\n## Observed Confounding {.smaller}\n\n- We need these assumptions:\n\n::: {.callout-note icon=false}\n\n## Assumption `A1`: \"Conditional Exchangeability / Unconfoundedness / Ignorability / Independence\".\n\n$Y_i(t) \\perp\\!\\!\\!\\perp T_i \\mid \\mathbf{X_i = x}, \\forall t \\in \\{0,1,\\dots\\}, \\text{ and } \\mathbf{x} \\in \\mathbb{X}$.\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## Assumption `A2`: \"Positivity / Overlap / Common Support\".\n\n$0 < P(T_i = t|\\mathbf{X_i = x}), \\forall t \\in \\{0,1,\\dots\\}, \\text{ and } \\mathbf{x} \\in \\mathbb{X}$.\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## Assumption `A3`: \"Stable Unit Treatment Value Assumption (SUTVA).\"\n\n$Y_i = Y(T_i)$\n\n:::\n\n- ... to achieve identification of the ATE:\n\n::: {.callout-note icon=false}\n\n## Theorem: \"Identification of the ATE\":\n\n$\\tau_{\\text{ATE}} = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] = \\mathbb{E_X}[\\mathbb{E}[Y_i|T_i=1,\\mathbf{X_i = x}] - \\mathbb{E}[Y_i|T_i=0, \\mathbf{X_i = x}]]$\n\n:::\n\n\n## Types of Parameters\n\nCausal Machine Learning methods force us to distinguish between two types of parameters:\n\n- 1. `Target parameter` is motivated by the research question and defined under modelling assumption,\n  - e.g. effect of a treatment on some outcome.\n- 2. `Nuisance parameters` are inputs that are required to obtain the target parameter, but are not relevant for our research question.\n  - e.g. propensity scores. \n\n- Focus on the target parameter and do not get tempted to interpret every single coefficient from a regression.\n\n\n## Frisch-Waugh-Lovell (FWL) Theorem\n\n- We can estimate $\\beta_T$ in a standard linear regression $Y_i = \\beta_0 + \\beta_T T_i + \\mathbf{\\beta_{X}' X_i} + \\epsilon_i$ in a `three-stage procedure`:\n\n1. Run a regression of the form $Y_i = \\beta_{Y0} + \\mathbf{\\beta_{Y \\sim X}' X_i} + \\epsilon_{Y_i \\sim X_i}$ and extract the estimated residuals $\\hat\\epsilon_{Y_i \\sim X_i}$.\n\n2. Run a regression of the form $T_i = \\beta_{T0} + \\mathbf{\\beta_{T \\sim X}' X_i} + \\epsilon_{T_i \\sim X_i}$ and extract the estimated residuals $\\hat\\epsilon_{T_i \\sim X_i}$.\n\n3. Run a residual-on-residual regression of the form $\\hat\\epsilon_{Y_i \\sim X_i} = \\beta_T \\hat\\epsilon_{T_i \\sim X_i} + \\epsilon_i$ (no constant).\n\nThe resulting estimate $\\hat\\beta_T$ is `numerically identical` to the estimate we would get if we just run the full OLS model.\n\n\n\n\n## Target Parameters\n\n- `Average potential outcome (APO)`: $\\mu_t := \\mathbb{E}[Y_i(t)]$.\n  - What is the expected `outcome if everybody receives treatment t`?\n- `Average Treatment Effect (ATE)`: $\\tau_{\\text{ATE}} := \\mathbb{E}[Y_i(1)] -  \\mathbb{E}[Y_i(0)] = \\mu_1 - \\mu_0$.\n  - What is the expected treatment `effect in the population`?\n\n::: {.fragment}\n\n- Note that the target parameters are just `different aggregations of the Conditional Average Potential Outcome (CAPO)`: $\\mathbb{E}[Y_i(t) \\mid  \\mathbf{X_i = x} ]$\n  - $\\mu_t := \\mathbb{E}[Y_i(t)] \\overset{LIE}{=} \\mathbb{E}[\\mathbb{E}[Y_i(t) \\mid \\mathbf{X_i = x}]]$\n  - $\\tau_{\\text{ATE}} := \\mathbb{E}[Y_i(1)] -  \\mathbb{E}[Y_i(0)] \\overset{LIE}{=} \\mathbb{E}[\\mathbb{E}[Y_i(1) \\mid  \\mathbf{X_i = x}]] - \\mathbb{E}[\\mathbb{E}[Y_i(0) \\mid  \\mathbf{X_i = x} ]]$.\n- It suffices to show that the CAPO is identified.\n\n\n\n:::\n\n## General Approach\n\n- Causal Machine Learning methods are mostly about `rewriting stuff` such that we are allowed to leverage `supervised ML to estimate the nuisance parameters`.\n- Importantly, the `target parameters of interest remain the same` although the rewritten form can look quite different to the original/familiar model.\n- The methods usually boil down to running `multiple supervised ML regressions` and combining their predictions into a `final OLS regression`.\n- Supervised ML holds the promise of `data-driven model selection` and `complex non-linear relationships`, and thus, `getting (one of) the nuisance parameters right`.\n- The crucial point is that the `statistical inference in this final OLS regression is valid if we follow a particular recipe`.\n  - Recipe of how to split the estimation of causal effects into prediction tasks.\n\n\n\n\n\n\n\n\n\n\n\n# Doubly Robust Methods {data-stack-name=\"Doubly Robust Methods\"}\n\n## Doubly Robust Methods: Idea {.smaller}\n\n- Given the three assumptions hold, we have seen two ways to identify the ATE: $\\tau_{\\text{ATE}} = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] = \\mu_1 - \\mu_0$\n\n::: {.fragment}\n\n\n- `Conditional outcome regression:` \n  - $\\tau_{\\text{ATE}} = \\mathbb{E_X}[\\mathbb{E}[Y_i | T_i = 1, \\mathbf{X_i = x}] - \\mathbb{E}[Y_i | T_i = 0, \\mathbf{X_i = x}]]$\n  - Simplified notation with `nuisance parameter` $\\mu(t, \\mathbf{x}) = \\mathbb{E}[Y_i | T_i = t, \\mathbf{X_i = x}]$ as conditional average potential outcome:\n  - $\\tau_{\\text{ATE}} = \\mathbb{E_X}[\\mu(t = 1, \\mathbf{x}) - \\mu(t=0,\\mathbf{x})]$\n\n::: \n\n::: {.fragment}\n- `Inverse probability weighting:` \n  - $\\tau_{\\text{ATE}} = \\mathbb{E}\\left[\\frac{T_i Y_i}{P(T_i = 1 \\mid \\mathbf{X_i = x})}\\right] - \\mathbb{E}\\left[\\frac{(1 - T_i) Y_i}{1 - P(T_i = 1 \\mid \\mathbf{X_i})}\\right]$ \n  - $\\tau_{\\text{ATE}} = \\mathbb{E}\\left[\\frac{T_i Y_i}{PS(\\mathbf{X_i})}\\right] - \\mathbb{E}\\left[\\frac{(1 - T_i) Y_i}{1 - PS(\\mathbf{X_i})}\\right]$\n  - Simplified notation with `nuisance parameter` $e_t(\\mathbf{x}) = P(T_i = t \\mid \\mathbf{X_i = x})$ as propensity score:\n  - $\\tau_{\\text{ATE}} = \\mathbb{E}\\left[\\frac{\\mathbb{1}(T_i = 1) Y_i}{e_{t=1}(\\mathbf{x})}\\right] - \\mathbb{E}\\left[\\frac{\\mathbb{1}(T_i = 0) Y_i}{e_{t=0}(\\mathbf{x})}\\right]$\n\n::: \n\n::: {.fragment}\n- Idea of `doubly robust` methods: \n  - Combine both approaches, such that the ATE estimator is consistent, `even if only one of the two models is correctly specified`. \n::: \n\n\n## Doubly Robust Estimator: Definition {.smaller}\n\n- `Doubly robust` or `Augmented Inverse Propensity Score Weighting (AIPW)` estimator:\n- Conditional average potential outcome (CAPO) given by:\n\n$$\n\\begin{align*}\n\\mu_t^{\\text{AIPW}}(\\mathbf{x}) := \\mathbb{E}[Y_i(t) | \\mathbf{X_i = x}] &\\overset{(A1,A3)}{=} \\mathbb{E}[Y_i | T_i = 0, \\mathbf{X_i = x}] := \\mu(t, \\mathbf{x}) \\\\\n&\\text{(conditional outcome regression)} \\\\\n&\\overset{(2)}{=} \\mathbb{E}\\left[\\frac{\\mathbb{1}(T_i = t)Y_i}{e_t(x)} \\bigg| \\mathbf{X_i = x}\\right] \\\\\n&\\text{(inverse probability weighting)} \\\\\n&\\overset{(3)}{=} \\mathbb{E}\\bigg[\\mu(t, \\mathbf{x}) + \\frac{\\mathbb{1}(T_i = t)(Y_i - \\mu(t, \\mathbf{x}))}{e_t(\\mathbf{x})} \\bigg| \\mathbf{X_i = x}\\bigg] \\\\\n&\\text{(augmenting outcome regression with IPW weights)} \\\\\n\\end{align*}\n$$\n\n::: {.fragment}\n  - Average potential outcome (APO) given by:\n\n    $$\\mu_t^{\\text{AIPW}} = \\mathbb{E_x}\\bigg[\\mathbb{E}\\bigg[\\mu(t, \\mathbf{x}) + \\frac{\\mathbb{1}(T_i = t)(Y_i - \\mu(t, \\mathbf{x}))}{e_t(\\mathbf{x})} \\bigg| \\mathbf{X_i = x}\\bigg] \\bigg] = \\mathbb{E}\\bigg[\\mu(t, \\mathbf{x}) + \\frac{\\mathbb{1}(T_i = t)(Y_i - \\mu(t, \\mathbf{x}))}{e_t(\\mathbf{x})} \\bigg] \\\\$$\n:::\n\n\n## Doubly Robust Estimator: Proof {.smaller}\n\n\n- Proof for Equation (2):\n\n\n$$\n\\begin{align*}\n\\mu_t(\\mathbf{x}) := \\mathbb{E}[Y_i(t) | \\mathbf{X_i = x}] &= \\mathbb{E}[Y_i | T_i = 0, \\mathbf{X_i = x}]\\\\\n&= \\mathbb{E}[\\underbrace{\\mathbb{1}(T_i = t)}_{=1} Y_i \\mid T_i = t, \\mathbf{X_i = x}]\\\\\n&= \\mathbb{E}\\left[ \\mathbb{1}(T_i = t) \\frac{e_t(\\mathbf{x})}{e_t(\\mathbf{x})} \\mid {T_i = t, \\mathbf{X_i = x}}\\right] + (1 - e_t(\\mathbf{x})) \\mathbb{E}[\\underbrace{\\mathbb{1}(T_i = t)}_{=0} Y_i \\mid T_i \\neq t, \\mathbf{X_i = x}]/e_t(\\mathbf{x})\\\\\n&= \\frac{\\overbrace{e_t(\\mathbf{x}) \\mathbb{E}[\\mathbb{1}(T_i = t) Y_i \\mid T_i = t, \\mathbf{X_i = x}] + (1 - e_t(\\mathbf{x})) \\mathbb{E}[\\mathbb{1}(T_i = t) Y_i \\mid T_i \\neq t, \\mathbf{X_i = x}]}^{\\overset{LIE}{=}\\mathbb{E}[\\mathbb{1}(T_i = t) Y_i | \\mathbf{X_i = x}]}}{e_t(\\mathbf{x})} \\\\\n&= \\frac{\\mathbb{E}\\left[\\mathbb{1}(T_i = t)Y_i \\mid \\mathbf{X_i = x} \\right]}{e_t(\\mathbf{x})} = \\mathbb{E}\\left[\\frac{\\mathbb{1}(T_i = t)Y_i}{e_t(\\mathbf{x})} \\bigg| \\mathbf{X_i = x}\\right]\n\\end{align*}\n$$\n\n## Doubly Robust Estimator: Proof {.smaller}\n\n\n- Proof for Equation (3):\n\n\n$$\n\\begin{align*}\n\\mu_t(\\mathbf{x}) := \\mathbb{E}[Y_i(t) | \\mathbf{X_i = x}] &= \\mathbb{E}\\left[\\mu(t, \\mathbf{x}) + \\frac{\\mathbb{1}(T_i = t)(Y_i - \\mu(t, \\mathbf{x}))}{e_t(\\mathbf{x})} \\bigg| \\mathbf{X_i = x}\\right] \\\\\n&= \\mathbb{E}\\left[Y_i(t) - Y_i(t) + \\mu(t, \\mathbf{x}) + \\frac{\\mathbb{1}(T_i = t)(Y_i - \\mu(t, \\mathbf{x}))}{e_t(\\mathbf{x})} \\bigg| \\mathbf{X_i = x}\\right] \\\\\n&= \\mathbb{E}\\left[Y_i(t) - Y_i(t) + \\mu(t, \\mathbf{x}) + \\frac{\\mathbb{1}(T_i = t)(Y_i(t) - \\mu(t, \\mathbf{x}))}{e_t(\\mathbf{x})} \\bigg| \\mathbf{X_i = x}\\right] \\\\\n&= \\mathbb{E}[Y_i(t) \\mid \\mathbf{X_i = x}] + \\mathbb{E} \\left[(Y_i(t) - \\mu(t, \\mathbf{x})) \\bigg(\\frac{\\mathbb{1}(T_i = t) - e_t(\\mathbf{x})}{e_t(\\mathbf{x})}\\bigg) \\bigg| \\mathbf{X_i = x} \\right] \\\\\n&\\overset{(4)}{=} \\mu_t(\\mathbf{x}) + \\underbrace{\\mathbb{E} \\left[ (Y_i(t) - \\mu(t, \\mathbf{x})) \\bigg(\\frac{\\mathbb{1}(T_i = t) - e_t(\\mathbf{x})}{e_t(\\mathbf{x})}\\bigg) \\bigg| \\mathbf{X_i = x} \\right]}_{\\text{needs to be 0 for the conditional APO to be identified}}\n\\end{align*}\n$$\n\n\n## Doubly Robust Estimator: Proof {.smaller}\n\n\n- Proof for Equation (4):\n\n- Let $\\tilde{\\mu}(t,\\mathbf{x})$ and $\\tilde{e}_{t}(\\mathbf{x})$ be some candidate functions for the conditional outcome regression and the propensity score, respectively. \n\n$$\n\\begin{align*}\n\\mathbb{E} \\left[ (Y_i(t) - \\tilde{\\mu}(t, \\mathbf{x})) \\bigg(\\frac{\\mathbb{1}(T_i = t) - \\tilde{e}_t(\\mathbf{x})}{\\tilde{e}_t(\\mathbf{x})}\\bigg) \\bigg| \\mathbf{X_i = x} \\right] &= \\mathbb{E} [ (Y_i(t) - \\tilde{\\mu}(t, \\mathbf{x})) \\mid \\mathbf{X_i = x}] \\mathbb{E} \\left[ \\bigg(\\frac{\\mathbb{1}(T_i = t) - \\tilde{e}_t(\\mathbf{x})}{\\tilde{e}_t(\\mathbf{x})}\\bigg) \\bigg| \\mathbf{X_i = x} \\right] \\\\\n&\\text{(ignorability allows to separate expectations)} \\\\\n&= (\\mathbb{E} [ Y_i(t) \\mid \\mathbf{X_i = x}] - \\tilde{\\mu}(t, \\mathbf{x}))   \\bigg(\\frac{\\mathbb{E} [\\mathbb{1}(T_i = t \\mid \\mathbf{X_i = x} ] - \\tilde{e}_t(\\mathbf{x})}{\\tilde{e}_t(\\mathbf{x})}\\bigg)  \\\\\n&= (\\mu_t(\\mathbf{x}) - \\tilde{\\mu}(t, \\mathbf{x})) \\frac{(e_t(\\mathbf{x}) - \\tilde{e}_t(\\mathbf{x}))}{\\tilde{e}_t(\\mathbf{x})} \\\\\n\\end{align*}\n$$\n\n::: {.fragment}\n\n- the last expression becomes 0 if either $\\tilde{\\mu}(t, \\mathbf{x}) = \\mu_t(\\mathbf{x})$ or $\\tilde{e}_t(\\mathbf{x}) = e_t(\\mathbf{x})$.\n\n:::\n\n\n## Doubly Robust Estimator: Theorem\n\n- Augmentation leads to the following theoretical properties:\n\n::: {.callout-note icon=false}\n\n## Theorem 4.3: \"Doubly Robust Estimator\".\n\nGiven $Y_i(t) \\perp\\!\\!\\!\\perp T_i \\mid \\mathbf{X_i = x}$ (conditional unconfoundedness) and given $0 < P(T_i = t|\\mathbf{X_i = x}), \\forall t$ (positivity), then:\n\n(1) If either $\\tilde{e}_{t}(\\mathbf{x}) = e_t(\\mathbf{x})$ or $\\tilde{\\mu}(t = 1,\\mathbf{x}) = \\mu_1(\\mathbf{x})$, then $\\mu_{1} = \\mathbb{E}[Y_i(1)]$\n\n(1) If either $\\tilde{e}_{t}(\\mathbf{x}) = e_t(\\mathbf{x})$ or $\\tilde{\\mu}(t = 0,\\mathbf{x}) = \\mu_0(\\mathbf{x})$, then $\\mu_{0} = \\mathbb{E}[Y_i(0)]$\n\n(3) If either $\\tilde{e}_{t}(\\mathbf{x}) = e_t(\\mathbf{x})$ or $\\tilde{\\mu}(t = 1,\\mathbf{x}) = \\mu_1(\\mathbf{x}), \\tilde{\\mu}(t = 0,\\mathbf{x}) = \\mu_0(\\mathbf{x})$, then $\\mu_{1} - \\mu_{0} = \\tau_{\\text{ATE}}$\n\n:::\n\n- with $\\tilde{\\mu}(t,\\mathbf{x})$ and $\\tilde{e}_{t}(\\mathbf{x})$ being some candidate functions for the conditional outcome regression and the propensity score, respectively. \n\n\n## Doubly Robust Estimator: Theorem {.smaller}\n\n\n- `Proof` showing that $\\mu_{t} = \\mathbb{E}[Y_i(t)]$:\n\n$$\n\\begin{align*}\n\\tilde{\\mu}_{t} - \\mathbb{E}[Y_i(t)] &= \\mathbb{E}\\bigg[\\tilde{\\mu}(t, \\mathbf{x}) + \\frac{\\mathbb{1}(T_i = t)(Y_i - \\tilde{\\mu}(t, \\mathbf{x}))}{\\tilde{e}_t(\\mathbf{x})} \\bigg] -  \\mathbb{E}[Y_i(t)] &\\text{(by defintion)} \\\\\n&= \\mathbb{E}\\bigg[\\frac{\\mathbb{1}(T_i = t)(Y_i - \\tilde{\\mu}(t, \\mathbf{x}))}{\\tilde{e}_t(\\mathbf{x})} - (Y_i(t) -  \\tilde{\\mu}(t, \\mathbf{x}))\\bigg] &\\text{(linearity of expectations)} \\\\\n&= \\mathbb{E}\\left[\\frac{\\mathbb{1}(T_i = t) - \\tilde{e}_t(\\mathbf{x})}{\\tilde{e}_t(\\mathbf{x})}(Y_i(t) - \\tilde{\\mu}(t, \\mathbf{x}))\\right] &\\text{(combining terms)} \\\\\n&= \\mathbb{E}\\left(\\mathbb{E}\\left[\\frac{\\mathbb{1}(T_i = t) - \\tilde{e}_t(\\mathbf{x})}{\\tilde{e}_t(\\mathbf{x})}(Y_i(t) - \\tilde{\\mu}(t, \\mathbf{x}) \\bigg| \\mathbf{X_i}\\right]\\right) &\\text{(law of iterated expectations)} \\\\\n&= \\mathbb{E}\\left(\\mathbb{E}\\left[\\frac{\\mathbb{1}(T_i = t) - \\tilde{e}_t(\\mathbf{x})}{\\tilde{e}_t(\\mathbf{x})} \\bigg| \\mathbf{X_i}\\right] \\cdot \\mathbb{E}\\left[Y_i(t) - \\tilde{\\mu}(t, \\mathbf{x})  \\bigg| \\mathbf{X_i}\\right]\\right) &\\text{(ignorability allows to separate expectations)} \\\\\n&= \\mathbb{E}\\left( \\frac{(e_t(\\mathbf{x}) - \\tilde{e}_t(\\mathbf{x}))}{\\tilde{e}_t(\\mathbf{x})}(\\mu_t(\\mathbf{x}) - \\tilde{\\mu}(t, \\mathbf{x}))\\right) \\\\\n\\end{align*}\n$$\n\n\n\n## Doubly Robust Estimator: Sample Version\n\n- `Step 1`: obtain the fitted values of the propensity scores: \n  - $\\hat{e}_t(\\mathbf{X_i})$\n- `Step 2`: obtain the fitted values of the outcome regressions: \n  - $\\hat{\\mu}(t, \\mathbf{X_i})$.\n- `Step 3`: construct the doubly robust estimator: \n  - $\\hat{\\tau}_{\\text{ATE}} = \\hat{\\mu}_{1} - \\hat{\\mu}_{0}$ with\n\n  - $\\hat{\\mu}_{1} = \\frac{1}{n} \\sum_{i=1}^n \\left[\\hat{\\mu}(t= 1, \\mathbf{X_i}) + \\frac{\\mathbb{1}(T_i = 1)(Y_i - \\hat{\\mu}(t = 1, \\mathbf{ X_i})}{\\hat{e}_1(\\mathbf{X_i})}\\right]$\n\n  - $\\hat{\\mu}_{0} = \\frac{1}{n} \\sum_{i=1}^n \\left[\\hat{\\mu}(t= 0, \\mathbf{X_i}) + \\frac{\\mathbb{1}(T_i = 0)(Y_i - \\hat{\\mu}(t = 0, \\mathbf{ X_i})}{\\hat{e}_0(\\mathbf{X_i})}\\right]$\n\n\n## Doubly Robust Estimator: Example {.smaller}\n\n- Assess the impact of participating in the U.S. National Supported Work (NSW) training program targeted to 445 individuals with social and economic problems on their real earnings. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(Matching)                       # load Matching package\ndata(lalonde)                           # load lalonde data\nattach(lalonde)                         # store all variables in own objects\nlibrary(drgee)                          # load drgee package\nT = treat                              # define treatment (training)\nY = re78                                # define outcome \nX = cbind(age,educ,nodegr,married,black,hisp,re74,re75,u74,u75) # covariates\ndr = drgee(oformula = formula(Y ~ X), eformula = formula(T ~ X), elink=\"logit\") # DR reg\nsummary(dr)                             # show results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  drgee(oformula = formula(Y ~ X), eformula = formula(T ~ X), elink = \"logit\")\n\nOutcome:  Y \n\nExposure:  T \n\nCovariates:  Xage,Xeduc,Xnodegr,Xmarried,Xblack,Xhisp,Xre74,Xre75,Xu74,Xu75 \n\nMain model:  Y ~ T \n\nOutcome nuisance model:  Y ~ Xage + Xeduc + Xnodegr + Xmarried + Xblack + Xhisp + Xre74 + Xre75 + Xu74 + Xu75 \n\nOutcome link function:  identity \n\nExposure nuisance model:  T ~ Xage + Xeduc + Xnodegr + Xmarried + Xblack + Xhisp + Xre74 + Xre75 + Xu74 + Xu75 \n\nExposure link function:  logit \n\n  Estimate Std. Error z value Pr(>|z|)  \nT   1674.1      672.4    2.49   0.0128 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Note: The estimated parameters quantify the conditional\nexposure-outcome association, given the covariates\nincluded in the nuisance models)\n\n 445  complete observations used\n```\n\n\n:::\n:::\n\n\n\n# Double Machine Learning with Partially Linear Regression {data-stack-name=\"DML-PLR\"}\n\n## Partially Linear Regression Model\n\n- Observed $Y_i$ and $T_i$ are a partially linear function of confounding variables $X_i$:\n  $\\begin{align}\\begin{aligned}Y_i = \\tau T_i + g(\\mathbf{X_i}) + \\epsilon_{Y_i}, & &\\mathbb{E}(\\epsilon_{Y_i} | T_i,\\mathbf{X_i}) = 0 \\\\T_i = m(\\mathbf{X_i}) + \\epsilon_{T_i}, & &\\mathbb{E}(\\epsilon_{T_i} | \\mathbf{X_i}) = 0\\end{aligned}\\end{align}$\n\n::: {.fragment}\n\n- Conditional average potential outcome: \n  - $\\mathbb{E}[Y_i(t) | \\mathbf{X_i}] \\overset{(A1, A3)}{=} \\mathbb{E}[Y_i | T_i = t, \\mathbf{X_i}] = \\tau t + g(\\mathbf{X_i})$\n\n:::\n\n\n::: {.fragment}\n\n- Target parameters:\n  - $\\tau_{\\text{CATE}} = \\mathbb{E}[Y_i | T_i = 1, \\mathbf{X_i}] - \\mathbb{E}[Y_i | T_i = 0, \\mathbf{X_i}]$\n  - $\\tau_{\\text{CATE}} = (\\tau 1 + g(\\mathbf{X_i})) - (\\tau 0 + g(\\mathbf{X_i})) = \\tau$\n  - $\\tau_{\\text{ATE}} = \\mathbb{E_X}[\\beta_T] = \\tau$\n  - => homogeneous treatment effects\n\n:::\n\n## Identification under Partial Linearity {.smaller}\n\n- Following [Robinson (1988)](https://journals.sagepub.com/doi/10.1177/00491241221099552), we can write the partially linear regression model as a generalization of the Frisch-Waugh-Lovell theorem:\n  - $\\underbrace{Y_i - \\overbrace{\\mathbb{E}[Y_i \\mid \\mathbf{X_i}]}^{\\mu(\\mathbf{X_i})}}_{\\text{outcome residual}} = \\tau \\underbrace{( T_i - \\overbrace{\\mathbb{E}[T_i \\mid \\mathbf{X_i}]}^{e(\\mathbf{X_i})})}_{\\text{treatment residual}} + \\epsilon_{Y_i}$\n  \n::: {.fragment}\n\n- $\\tau_{\\text{ATE}}$ is identified by a residual-on-residual regression without constant:\n  - Population estimand: \n    - $\\tau_{\\text{ATE}} = \\arg \\min_{\\tilde{\\tau}} \\mathbb{E}[( \\underbrace{ Y_i - \\mu(\\mathbf{X_i})}_{\\text{pseudo outcome}} - \\tilde{\\tau} \\underbrace{( T_i - e(\\mathbf{X_i}))}_{\\text{single regressor}})^2] = \\frac{\\text{Cov}[(Y_i - \\mu(\\mathbf{X_i})) (T_i - e(\\mathbf{X_i}))]}{\\text{Var}[T_i - e(\\mathbf{X_i})]}$\n  - Sample estimator: \n    - $\\hat{\\tau}_{\\text{ATE}} = \\arg \\min_{\\tilde{\\tau}} \\frac{1}{N}\\sum_{i=1}^n ( \\underbrace{Y_i - \\mu(\\mathbf{X_i})}_{\\text{pseudo outcome}} - \\tilde{\\tau} \\underbrace{( T_i - e(\\mathbf{X_i}))}_{\\text{single regressor}})^2 = \\frac{\\sum_{i=1}^n (Y_i - \\mu(\\mathbf{X_i})) (T_i - e(\\mathbf{X_i}))}{\\sum_{i=1}^n (T_i - e(\\mathbf{X_i}))^2}$\n\n:::\n\n::: {.fragment}\n-  However, regression not feasible because nuisance parameters unknown: ML toolbox might be useful.\n::: \n\n\n## Double Machine Learning under Partial Linearity \n\n\n\n- [Chernozhukov et al. (2018)](https://doi.org/10.1111/ectj.12097) propose a three step procedure:\n\n  1. Form prediction model for the treatment: $\\hat{e}(\\mathbf{X_i})$\n  \n  2. Form prediction model for the outcome: $\\hat{\\mu}(\\mathbf{X_i})$\n  \n  3. Run feasible residual-on-residual regression: \n    - $\\hat{\\tau}_{\\text{ATE}} = \\arg \\min_{\\tilde{\\tau}} \\frac{1}{N}\\sum_{i=1}^n ( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tilde{\\tau} ( T_i - \\hat{e}(\\mathbf{X_i})))^2 = \\frac{\\sum_{i=1}^n (Y_i - \\hat{\\mu}(\\mathbf{X_i})) (T_i - \\hat{e}(\\mathbf{X_i}))}{\\sum_{i=1}^n (T_i - \\hat{e}(\\mathbf{X_i}))^2}$\n\n\n\n::: {.fragment}\n\n- Predictions of nuisance parameters $\\hat{e}(\\mathbf{X_i})$ and  $\\hat{\\mu}(\\mathbf{X_i})$ have to fulfill `two conditions`:\n  1. `High-quality`: consistency and convergence rates faster than $N^{\\frac{1}{4}}$.\n  2. `Out-of-sample`: individual predictions formed without the observation itself.\n  \n  => `standard (robust) OLS inference` is valid (see [Chernozhukov et al. (2018)](https://doi.org/10.1111/ectj.12097)).\n\n:::\n\n## High Quality Predictions in DML\n\n- `Consistency`: ML methods converge to the true nuisance parameters as $N \\to \\infty$.\n- `Convergence rate`:\n  - Parametric models like OLS converge at the rate $N^{\\frac{1}{2}}$: \n    - RMSE ($\\mathbb{E}[\\sqrt{(\\hat{\\mu}(\\mathbf{X_i}) - \\mu(\\mathbf{X_i}))^2}]$) expected to halve if sample increases by factor four\n  - ML methods usually do not converge as quickly because they can not leverage the structural information of a parametric model.\n  - For Double ML to work, it suffices that the RMSE more than halves if we increase sample size by factor 16 (convergence rate: $N^{\\frac{1}{4}}$).\n  - Achievable with popular ML methods like [(Post-) LASSO](https://projecteuclid.org/journals/bernoulli/volume-19/issue-2/Least-squares-after-model-selection-in-high-dimensional-sparse-models/10.3150/11-BEJ410.full), [Random Forests](https://arxiv.org/abs/2007.03210), or [Neural Networks](https://www.econometricsociety.org/publications/econometrica/2021/01/01/deep-neural-networks-estimation-and-inference) .\n\n\n\n\n## Out-of-Sample Predictions in DML {.smaller}\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n- `K-fold Cross-Fitting`:\n  - Split the sample into $K$ folds.\n  - For each fold $k$, train a prediction model for the nuisance parameters on the remaining $K-1$ folds.\n  - Predict the nuisance parameters for fold $k$ using the model trained on the remaining $K-1$ folds.\n  - Repeat for all $K$ folds to obtain predictions for each individual observation in each fold.\n  - Use combined predictions for the residual-on-residual regression.\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n![](_images/5/cross_validation.jpg){fig-align=\"center\" height=70% width=70%}\n:::\n\n:::\n\n\n::: {.fragment}\n\n=> Predictions are formed without the observation itself, still no waste of information.\n\n=> Nuisance parameters induce `no bias by overfitting` ([Chernozhukov et al. (2018)](https://doi.org/10.1111/ectj.12097)).\n\n::: \n\n\n## Neyman-orthogonal Score Functions {.smaller}\n\n- Predicted nuisance parameters have to be used in a `Neyman-orthogonal score function` (score) $\\psi$! \n- $\\psi$ has to satisfy `moment condition` $\\mathbb{E}[\\psi(Y_i,T_i,\\hat{\\tau}, \\hat{\\mu}(\\mathbf{X_i}), \\hat{e}(\\mathbf{X_i}))] = 0$ to identify the target parameter $\\tau_{\\text{ATE}}$.\n- In PLR, $\\psi$ is the solution to the minimization problem of the residual-on-residual regression - derivative w.r.t. $\\hat{\\tau}$:\n\n$$\\begin{align*}\n&\\frac{1}{N} \\sum_{i=1}^N \\underbrace{(Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\hat{\\tau}(T_i - \\hat{e}(\\mathbf{X_i}))) (T_i - \\hat{e}(X_i))}_{\\psi(Y_i, T_i, \\hat{\\tau}, \\hat{\\mu}(\\mathbf{X_i}), \\hat{e}(\\mathbf{X_i}))} = 0 \\\\\n\\Rightarrow \\hat{\\tau}_{\\text{ATE}} &= \\frac{\\sum_{i=1}^n (Y_i - \\hat{\\mu}(\\mathbf{X_i})) (T_i - \\hat{e}(\\mathbf{X_i}))}{\\sum_{i=1}^n (T_i - \\hat{e}(\\mathbf{X_i}))^2}\n\\end{align*}$$\n\n\n\n::: {.fragment}\n\n- `Neyman-orthogonality` of score $\\psi(Y_i,T_i,\\hat{\\tau}, \\hat{\\mu}(\\mathbf{X_i}), \\hat{e}(\\mathbf{X_i}))$:\n  - (Gateaux) derivative of the score function with respect to the nuisance parameters is zero in expectation at the true value of the nuisance parameters:\n    - $\\partial_r \\mathbb{E}[\\psi(Y_i, T_i, \\hat{\\tau}, \\mu(\\mathbf{X_i}) + r(\\mu(\\mathbf{X_i}) - \\hat{\\mu}(\\mathbf{X_i})), e(\\mathbf{X_i}) + r(e(\\mathbf{X_i}) - \\hat{e}(\\mathbf{X_i})))]\\mid_{r=0} = 0$\n  - Ensures that the $\\hat{\\tau}$ is robust against biases in the prediction of nuisance parameters (e.g. by regularization).\n  - Note (without proof): residual-on-residual regression fulfills this requirement!\n\n:::\n\n\n\n## Overcoming Regularization & Overfitting Bias {.smaller}\n\n- Compare a non-orthogonal score function, e.g. $\\hat{\\tau}_{\\text{ATE}} = \\frac{\\sum_{i=1}^n T_i (Y_i - \\hat{\\mu}(\\mathbf{X_i}))}{\\sum_{i=1}^n T_i^2}$ to orthogonal one: $\\hat{\\tau}_{\\text{ATE}} = \\frac{\\sum_{i=1}^n (Y_i - \\hat{\\mu}(\\mathbf{X_i})) (T_i - \\hat{e}(\\mathbf{X_i}))}{\\sum_{i=1}^n (T_i - \\hat{e}(\\mathbf{X_i}))^2}$.\n\n\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n- Compare with and without K-fold Cross-Fitting.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05_dml_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# simulating the data\nlibrary(DoubleML)\nset.seed(1234)\nn_rep = 1000 # number samples\nn_obs = 500 # number of observations\nn_vars = 20 # number of covariates\nalpha = 0.5 # true treatment effect\n\n\ndata = list()\nfor (i_rep in seq_len(n_rep)) {\n# command to simulate Y_i and T_i based on true non-linear nuisance functions m(x) and e(x)\n  data[[i_rep]] = make_plr_CCDDHNR2018(alpha=alpha, n_obs=n_obs, dim_x=n_vars,\n                                       return_type=\"data.frame\")\n}\n\n# define custom (non-othogonal) score function\nnon_orth_score = function(y, d, l_hat, m_hat, g_hat, smpls) {\n  u_hat = y - g_hat\n  psi_a = -1*d*d\n  psi_b = d*u_hat\n  psis = list(psi_a = psi_a, psi_b = psi_b)\n  return(psis)\n}\n\nlibrary(mlr3)\nlibrary(mlr3learners)\n\n# define the ml prediction models from the mlr3 package:\nml_l = lrn(\"regr.ranger\", num.trees = 132, max.depth = 5, mtry = 12, min.node.size = 1) # learner for mu = E[Y|X]\nml_m = lrn(\"regr.ranger\", num.trees = 378, max.depth = 3, mtry = 20, min.node.size = 6) # learner for e = E[T|X]\n\n\n# run the simulation \nfor (i_rep in seq_len(n_rep)) {\n  df = data[[i_rep]]\n  obj_dml_data = double_ml_data_from_data_frame(df, y_col = \"y\", d_cols = \"d\")\n  # key function friom the DoubleML package\n  obj_dml_plr_nonorth = DoubleMLPLR$new(obj_dml_data, # suppy data\n                                        ml_l, ml_m, ml_g,# supply the machine learning methods\n                                        n_folds=2, # no cross fitting at first\n                                        score=non_orth_score, # supply custom score function\n                                        # score='partialling out' # built-in orthogonal score function PLM\n                                        apply_cross_fitting=TRUE) # no cross fitting at first\n  # extract and store estimates for each sample\n  obj_dml_plr_nonorth$fit()\n  this_theta = obj_dml_plr_nonorth$coef\n  this_se = obj_dml_plr_nonorth$se\n  print(abs(theta_nonorth[i_rep] - this_theta))\n  print(abs(se_nonorth[i_rep] - this_se))\n  theta_nonorth[i_rep] = this_theta\n  se_nonorth[i_rep] = this_se\n}\n```\n:::\n\n\n:::\n\n\n\n:::\n\n\n# Double Machine Learning with Augmented Inverse Probability Weighting {data-stack-name=\"DML-AIPW\"}\n\n## Interactive Regression Model\n\n- More general model that relaxes the homogeneous treatment assumption (binary $T_i$ is not additively separable anymore):\n  $\\begin{align}\\begin{aligned}Y_i = g(T_i, \\mathbf{X_i}) + \\epsilon_{Y_i}, & &\\mathbb{E}(\\epsilon_{Y_i} | T_i,\\mathbf{X_i}) = 0 \\\\T_i = m(\\mathbf{X_i}) + \\epsilon_{T_i}, & &\\mathbb{E}(\\epsilon_{T_i} | \\mathbf{X_i}) = 0\\end{aligned}\\end{align}$\n\n::: {.fragment}\n\n- We can use the identification results from the Doubly Robust / AIPW estimator: \n  - Average potential outcome (APO):\n    - $\\mu_t^{\\text{AIPW}} = \\mathbb{E}[Y_i(t)] = \\mathbb{E}\\bigg[\\mu(t, \\mathbf{X_i}) + \\frac{\\mathbb{1}(T_i = t)(Y_i - \\mu(t, \\mathbf{X_i}))}{e_t(\\mathbf{X_i})} \\bigg] \\\\$\n  - Average treatment effect (ATE):\n    - $\\tau_{\\text{ATE}}^{\\text{AIPW}} = \\mathbb{E}[Y_i(1) - Y_i(0)] = \\mathbb{E}\\bigg[\\mu(1, \\mathbf{X_i}) - \\mu(0, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\mu(1, \\mathbf{X_i}))}{e_1(\\mathbf{X_i})} - \\frac{(1-T_i)(Y_i - \\mu(0, \\mathbf{X_i})}{e_0(\\mathbf{X_i}))} \\bigg]$\n  \n:::\n\n\n## Double Machine Learning under AIPW {.smaller}\n\n\n\n- [Chernozhukov et al. (2018)](https://doi.org/10.1111/ectj.12097) propose a three step procedure:\n\n  1. Form prediction model for the treatment: $\\hat{e}(\\mathbf{X_i})$\n  \n  2. Form prediction model for the outcome: $\\hat{\\mu}(T_i, \\mathbf{X_i})$\n  \n  3. a) Estimate the APO: \n    - $\\mu_t^{\\text{AIPW}} = \\frac{1}{N}\\sum_{i=1}^n \\bigg(\\hat{\\mu}(t, \\mathbf{X_i}) + \\frac{\\mathbb{1}(T_i = t)(Y_i - \\hat{\\mu}(t, \\mathbf{X_i}))}{\\hat{e}_t(\\mathbf{X_i})} \\bigg) \\\\$\n    \n  3. b) Estimate the ATE:\n    - $\\tau_{\\text{ATE}}^{\\text{AIPW}} = \\frac{1}{N}\\sum_{i=1}^n \\bigg(\\hat{\\mu}(1, \\mathbf{X_i}) - \\hat{\\mu}(0, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\hat{\\mu}(1, \\mathbf{X_i}))}{\\hat{e}_1(\\mathbf{X_i})} - \\frac{(1-T_i)(Y_i - \\hat{\\mu}(0, \\mathbf{X_i})}{\\hat{e}_0(\\mathbf{X_i}))} \\bigg)$\n\n\n\n::: {.fragment}\n\n- To obtain a consistent, asymptotically normal and semi-parametrically efficient estimator that allows `standard (robust) inference`, we need the `same three key ingredients`:\n  1. High-quality machine learning methods\n  2. K-fold cross-validation\n  3. `Neyman-orthogonal score function`: let's proof this!\n\n\n:::\n\n\n\n## DML-AIPW Score Function (1) {.smaller}\n\n- As the score defining the ATE is just the difference between APOs, it inherits ist Neyman orthogonality. Hence let's focus on the AIPW score of the APO, which is:\n\n$$\\begin{align*}\n&\\mathbb{E}\\bigg[ \\underbrace{\\mu(t, \\mathbf{X_i}) + \\frac{\\mathbb{1}(T_i = t)(Y_i - \\mu(t, \\mathbf{X_i}))}{e_t(\\mathbf{X_i})} - \\mu_t^{\\text{AIPW}}}_{\\psi(Y_i, T_i, \\mu(t, \\mathbf{X_i}), e(\\mathbf{X_i}))}\\bigg] = 0 \\\\\n\\Rightarrow \\mu_t^{\\text{AIPW}} = & \\mathbb{E}\\bigg[\\mu(t, \\mathbf{X_i}) + \\frac{\\mathbb{1}(T_i = t)(Y_i - \\mu(t, \\mathbf{X_i}))}{e_t(\\mathbf{X_i})} \\bigg]\n\\end{align*}$$\n\n- `Neyman-orthogonality of a score` $\\psi$ means that the Gateaux derivative with respect to the nuisance parameters is zero in expectation at the true nuisance parameters (NP). This means:\n\n$$\\partial_r \\mathbb{E}\\left[\\psi(Y_i, T_i, \\mu + r(\\tilde{\\mu}-\\mu), e + r(\\tilde{e} - e)) \\mid \\mathbf{X_i = x}\\right] |_{r=0} = 0$$\n\n- where we suppress the dependencies of NPs and denote by, e.g., $\\tilde{\\mu}$ a value of the outcome nuisance that is different to the true value $\\mu$. We can show this equation holds with the following four steps.\n\n\n\n## DML-AIPW Score Function (2) {.smaller}\n\n- (1) Add perturbations to the true nuisance parameters in the score:\n\n$$\n\\begin{align*}\n\\psi&(Y_i, T_i, \\mu + r(\\tilde{\\mu} - \\mu), e + r(\\tilde{e} - e)) \\\\ &= (\\mu + r(\\tilde{\\mu} - \\mu)) + \\frac{ \\mathbb{1}(T_i = t) Y_i}{e + r(\\tilde{e} - e)} - \\frac{\\mathbb{1}(T_i = t) (\\mu + r(\\tilde{\\mu} - \\mu))}{e + r(\\tilde{e} - e)} - \\mu_t^{\\text{AIPW}}\n\\end{align*}\n$$\n\n\n- (2) Take the conditional expectation:\n\n$$\n\\begin{align*}\n&\\mathbb{E}\\left[ \\psi(Y_i, T_i, \\mu + r(\\tilde{\\mu} - \\mu), e + r(\\tilde{e} - e)) \\mid \\mathbf{X_i = x} \\right] \\\\ &= \\mathbb{E}\\left[ (\\mu + r(\\tilde{\\mu} - \\mu)) + \\frac{ \\mathbb{1}(T_i = t) Y_i}{e + r(\\tilde{e} - e)} - \\frac{\\mathbb{1}(T_i = t) (\\mu + r(\\tilde{\\mu} - \\mu))}{e + r(\\tilde{e} - e)} - \\mu_t^{\\text{AIPW}} \\bigg| \\mathbf{X_i = x} \\right] \\\\ &= (\\mu + r(\\tilde{\\mu} - \\mu)) + \\frac{ e\\mu}{e + r(\\tilde{e} - e)} - \\frac{e (\\mu + r(\\tilde{\\mu} - \\mu))}{e + r(\\tilde{e} - e)} - \\mu_t^{\\text{AIPW}}\n\\end{align*}\n$$\n- where we use that $\\mathbb{E}[\\mathbb{1}(T_i = t)Y_i \\mid \\mathbf{X_i = x}] \\overset{(A3)}{=} \\mathbb{E}[\\mathbb{1}(T_i = t)Y_i(t) \\mid \\mathbf{X_i = x}] \\overset{(A1)}{=} e\\mu$ and $\\mathbb{E}[\\mathbb{1}(T_i = t) \\mid \\mathbf{X_i = x}] = e$.\n\n\n## DML-AIPW Score Function (3) {.smaller}\n\n\n- (3) Take the derivative with respect to $r$:\n\n$$\n\\begin{align*}\n\\frac{\\partial}{\\partial r} &\\mathbb{E}[\\psi(Y_i, T_i, \\mu + r(\\tilde{\\mu} - \\mu), e + r(\\tilde{e} - e)) \\mid X_i = x] \\\\ \n&= (\\tilde{\\mu} - \\mu) - \\frac{e\\mu(\\tilde{e} - e)}{(e + r(\\tilde{e} - e))^2} - \\frac{e(\\tilde{\\mu} - \\mu)(e+r(\\tilde{e} - e)) - e(\\mu + r(\\tilde{\\mu} - \\mu))(\\tilde{e} - e)}{(e + r(\\tilde{e} - e))^2}\n\\end{align*}\n$$\n\n- (4) Evaluate at the true nuisance values, i.e. set $r = 0$:\n\n$$\n\\begin{align*}\n\\frac{\\partial}{\\partial r} &\\mathbb{E}[\\psi(Y_i, T_i, \\mu + r(\\tilde{\\mu} - \\mu), e + r(\\tilde{e} - e)) \\mid X_i = x] |_{r=0} \\\\ \n&= (\\tilde{\\mu} - \\mu) - \\frac{e\\mu(\\tilde{e} - e)}{(e + 0(\\tilde{e} - e))^2} - \\frac{e(\\tilde{\\mu} - \\mu)(e+r(\\tilde{e} - e)) - e(\\mu + 0(\\tilde{\\mu} - \\mu))(\\tilde{e} - e)}{(e + 0(\\tilde{e} - e))^2} \\\\\n&= (\\tilde{\\mu} - \\mu) - \\frac{e\\mu(\\tilde{e} - e)}{e^2} - \\frac{e(\\tilde{\\mu} - \\mu)e - e\\mu(\\tilde{e} - e)}{e^2} \\\\\n&= (\\tilde{\\mu} - \\mu) - \\frac{e\\mu(\\tilde{e} - e)}{e^2} - \\frac{e^2}{e^2}(\\tilde{\\mu} - \\mu) + \\frac{e\\mu(\\tilde{e} - e)}{e^2}\\\\\n&= 0\n\\end{align*}\n$$\n\n## DML-AIPW: Example {.smaller}\n\n- Assess the effect of 401(k) program participation on net financial assets of 9,915 households in the US in 1991.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(DoubleML)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(data.table)\n\n# suppress messages during fitting\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\n# load data as a data.table\ndata = fetch_401k(return_type = \"data.table\", instrument = TRUE)\n\n# Set up basic model: Specify variables for data-backend\nfeatures_base = c(\"age\", \"inc\", \"educ\", \"fsize\",\"marr\", \"twoearn\", \"db\", \"pira\", \"hown\")\n\n# Initialize DoubleMLData (data-backend of DoubleML)\ndata_dml_base = DoubleMLData$new(data,\n                                 y_col = \"net_tfa\", # outcome variable\n                                 d_cols = \"e401\", # treatment variable\n                                 x_cols = features_base) # covariates\n\n# Initialize Random Forrest Learner\nrandomForest = lrn(\"regr.ranger\")\nrandomForest_class = lrn(\"classif.ranger\")\n\n# Random Forest\nset.seed(123)\ndml_irm_forest = DoubleMLIRM$new(data_dml_base,\n                                 ml_g = randomForest,\n                                 ml_m = randomForest_class,\n                                 score = \"ATE\",\n                                 trimming_threshold = 0.01,\n                                 apply_cross_fitting = TRUE,\n                                 n_folds = 5)\n\n# Set nuisance-part specific parameters\ndml_irm_forest$set_ml_nuisance_params(\n    \"ml_g0\", \"e401\", list(max.depth = 6, mtry = 4, min.node.size = 7)) # learner for outcome 0\ndml_irm_forest$set_ml_nuisance_params(\n    \"ml_g1\", \"e401\", list(max.depth = 6, mtry = 3, min.node.size = 5)) # learner for outcome 1\ndml_irm_forest$set_ml_nuisance_params( \n    \"ml_m\", \"e401\", list(max.depth = 6, mtry = 3, min.node.size = 6)) # learner for treatment\n\ndml_irm_forest$fit()\ndml_irm_forest$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimates and significance testing of the effect of target variables\n     Estimate. Std. Error t value Pr(>|t|)    \ne401      8206       1106   7.421 1.16e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n# General Recipe {data-stack-name=\"General Recipe\"}\n\n## Definitions\n\n- Data and parameters;\n  - $W$ is a set of `observed variables`; e.g., $W = \\{Y, T, X\\}$.\n  - $\\theta$ is the `target parameter`.\n  - $\\eta$ is a set of `nuisance parameters`; e.g., $\\eta = \\{\\mu(X), e(X)\\}$.\n\n\n::: {.fragment}\n\n\n- Score functions $\\psi(W, \\tilde{\\theta}, \\tilde{\\eta})$ must satisfy two properties in Double ML:\n  - $\\mathbb{E}[\\psi(W, \\theta, \\eta)] = 0$: i.e. `moment condition` with expectation zero if evaluated at true parameters.\n  - $\\partial_r\\mathbb{E}[\\psi(W, \\theta, \\eta + r(\\hat{\\eta} - \\eta))]|_{r=0} = 0$: i.e. `Neyman-orthogonality`.\n\n:::\n\n\n## Examples \n\n- Moment condition of the residual-on-residual regression:\n  - $\\mathbb{E} \\left[ (Y - \\mu(X) - \\tau (T - e(X))) (T - e(X)) \\right] = 0$\n  - $W = (T, X, Y), \\quad \\theta = \\tau, \\quad \\eta = (\\mu(X), e(X))$\n  - with $\\mu(X) := \\mathbb{E}[Y \\mid X]$ and $e(X) := \\mathbb{E}[T \\mid X]$\n\n\n::: {.fragment}\n\n- Moment condition of the AIPW-ATE:\n  - $\\mathbb{E} \\left[ \\mu(1, X) - \\mu(0, X) + \\frac{T(Y - \\mu(1,X))}{e(X)} - \\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)} - \\tau_{ATE} \\right] = 0$\n  - $W = (T, X, Y), \\quad \\theta = \\tau_{\\text{ATE}}, \\quad \\eta = (\\mu(1, X), \\mu(0, X), e(X))$\n  - with $\\mu(t,X) := \\mathbb{E}[Y \\mid T = t, X]$ and $e(X) := \\mathbb{E}[T=1 \\mid X]$\n\n:::\n\n\n## Linear Score Functions \n\n- We will focus on linear score functions that can be represented as follows:\n  - $\\psi(W, \\tilde{\\theta}, \\tilde{\\eta}) = \\tilde{\\theta}\\psi_a(W, \\tilde{\\eta}) + \\psi_b(W, \\tilde{\\eta})$\n- such that the moment condition can be written as:\n  - $\\mathbb{E}[\\psi(W, \\theta, \\eta)] = \\theta \\mathbb{E}[\\psi_a(W, \\eta)] +  \\mathbb{E}[\\psi_b(W, \\eta)] = 0$\n- and the solution is:\n  - $\\theta = - \\frac{\\mathbb{E}[\\psi_b(W, \\eta)]}{\\mathbb{E}[\\psi_a(W, \\eta)]}$\n\n\n\n\n## Example Residual-on-residual Regression\n\n- Moment condition:\n\n\n$$\n\\begin{align*}\n\\mathbb{E} \\left[ (Y - \\mu(X) - \\tau(T - e(X)))(T - e(X)) \\right] &= 0 \\\\\n\\mathbb{E} \\left[ (Y - \\mu(X))(T - e(X)) - \\tau(T - e(X))(T - e(X)) \\right] &= 0 \\\\\n\\tau \\mathbb{E} [ \\underbrace{-1(T - e(X))^2}_{\\psi_a} ] + \\mathbb{E} [ \\underbrace{(Y - \\mu(X))(T - e(X))}_{\\psi_b} ] &= 0 \\\\\n\\Rightarrow \\tau = - \\frac{\\mathbb{E}[\\psi_b(W; \\eta)]}{\\mathbb{E}[\\psi_a(W; \\eta)]} = \\frac{\\mathbb{E}[(Y - \\mu(X))(T - e(X))]}{\\mathbb{E}[(T - e(X))^2]}\n\\end{align*}\n$$\n\n\n## Example AIPW-ATE {.smaller}\n\n- Moment condition:\n\n\n$$\n\\begin{align*}\n\\mathbb{E} \\left[ \\mu(1, X) - \\mu(0, X) + \\frac{T(Y - \\mu(1, X))}{e(X)} - \\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)} - \\tau_{\\text{ATE}} \\right] &= 0 \\\\\n\\tau_{\\text{ATE}} \\underbrace{(-1)}_{\\psi_a} + \\mathbb{E} \\bigg[ \\underbrace{\\mu(1, X) - \\mu(0, X) + \\frac{T(Y - \\mu(1, X))}{e(X)} - \\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)}}_{\\psi_b} \\bigg] &= 0 \\\\\n\\Rightarrow \\tau_{\\text{ATE}} = -\\frac{\\mathbb{E}[\\psi_b(W; \\eta)]}{\\mathbb{E}[\\psi_a(W; \\eta)]} = \\mathbb{E} \\left[ \\mu(1, X) - \\mu(0, X) + \\frac{T(Y - \\mu(1, X))}{e(X)} - \\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)} \\right]\n\\end{align*}\n$$\n\n\n## Double ML Recipe\n\n\n1. Find `Neyman-orthogonal score` for your target parameter:\n    - can be constructed ([see Chernozhukov et al. (2018), Section 2](https://doi.org/10.1111/ectj.12097))\n2. `Predict nuisance parameters` $\\hat{\\eta}$ with cross-fitted high-quality ML.\n3. `Solve empirical moment condition` to estimate the target parameter:\n    - $\\theta = - \\frac{\\mathbb{E}[\\psi_b(W, \\eta)]}{\\mathbb{E}[\\psi_a(W, \\eta)]}$\n4. `Calculate standard error`:\n    - $\\hat{\\sigma}^2 = \\frac{N^{-1} \\sum_{i} \\psi(W_i; \\hat{\\theta}, \\hat{\\eta}_i)^2}{[N^{-1} \\sum_{i} \\psi_a(W_i; \\hat{\\eta}_i)]^2} \\quad \\Rightarrow \\quad \\text{se}(\\hat{\\theta}) = \\sqrt{\\frac{\\hat{\\sigma}^2}{N}}$\n    - To calculate t-values, confidence intervals, etc.\n    - Can be motivated by the concept of `influence functions`.\n\n\n\n\n## Standard Errors in DML {.smaller}\n\n- `Influence functions`: \n  - $\\Psi(W; \\theta, \\eta) := - \\mathbb{E} \\left[ \\frac{\\partial \\psi}{\\partial \\theta} \\right]^{-1} \\psi(W; \\theta, \\eta) = - \\mathbb{E}[\\psi_a(W; \\eta)]^{-1} \\psi(W; \\theta, \\eta)$\n  - `Scaled version of the score` with important characteristics:\n    - $\\Psi(W_i; \\theta, \\eta_i)$ measures the influence of an estimator $\\theta$ to infinitesimal changes in the distribution, i.e. of each observation $W_i$\n    - $\\mathbb{E}[\\Psi(W; \\theta, \\eta)] = \\mathbb{E}[ -\\mathbb{E}[\\psi_a(W; \\eta)]^{-1} \\psi(W; \\theta, \\eta)] = -\\mathbb{E}[\\psi_a(W; \\eta)]^{-1}  \\underbrace{\\mathbb{E}[\\psi(W; \\theta, \\eta)]}_{=0} = 0$\n\n\n::: {.fragment}\n\n- Estimator distribution and influence function are closely linked:\n  - $\\sqrt{N}(\\hat{\\theta} - \\theta) = \\frac{1}{\\sqrt{N}} \\sum_{i} \\psi(W_i; \\theta, \\eta_i) + o_p(1) \\xrightarrow{d} N(0, \\underbrace{\\operatorname{Var}[\\psi(W; \\theta, \\eta)]}_{\\sigma^2})$\n- Estimator variance (suppressing arguments for brevity):\n  - $\\sigma^2 = \\text{Var}[\\psi] = \\mathbb{E}[\\psi^2] - \\underbrace{\\mathbb{E}[\\psi]^2}_{=0} = \\mathbb{E}[\\psi^2] = \\mathbb{E}[(\\mathbb{E}[\\psi_a]^{-1} \\psi)^2] = \\mathbb{E}[\\psi_a]^{-2} \\mathbb{E}[\\psi^2] = \\frac{\\mathbb{E}[\\psi^2]}{\\mathbb{E}[\\psi_a]^2}$\n\n\n:::\n\n# {.center .center-x data-state=\"hide-menubar\"}\n<style>\n.table-text {\n  text-align: center !important;\n  font-size: 2em;\n  padding-bottom: 1em !important;\n}\n.row-bottom {\n  text-align: center !important;\n  padding-top: 2em !important;\n}\n</style>\n<table width = \"100%\">\n<tbody>\n  <tr>\n    <td colspan=\"2\" class=\"table-text\">Thank you for your attention!</td>\n  </tr>\n  <tr >\n    <td class=\"row-bottom\" style=\"vertical-align: baseline; width: 50%;\">\n          <img src=\"https://raw.githubusercontent.com/christophihl/TIE_website/main/assets/images/logo.svg\" style=\"vertical-align: middle; width: 60%; height: auto;\" />\n          <img src=\"author_pic.jpg\" class=\"picture\" style=\"vertical-align: middle; horizontal-align: right; width: 30%; height: auto;\" />\n    </td>\n    <td class = \"row-bottom\" style = \"vertical-align: middle; width = 50%\">\n      <ul style = \"list-style:none;\">\n        <li><a href = \"https://www.startupengineer.io/authors/ihl/\" target = \"_blank\"><i class = \"fas fa-home\"></i> startupengineer.io/authors/ihl</a></li>\n        <li><a href = \"https://www.linkedin.com/in/christoph-ihl/\" target = \"_blank\"><i class = \"fab fa-linkedin\"></i> christoph-ihl</a></li>\n        <li><a href = \"https://github.com/christophihl\" target = \"_blank\"><i class = \"fab fa-github\"></i> christophihl</a></li>\n        <li><a href = \"https://twitter.com/Ihluminate/\" target = \"_blank\"><i class = \"fab fa-twitter\"></i> Ihluminate</a></li>\n      </ul>\n    </td>\n  </tr>\n</tbody>\n</table>\n\n",
    "supporting": [
      "05_dml_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}