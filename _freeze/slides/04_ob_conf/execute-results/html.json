{
  "hash": "74e6996f035965f3297612805261eecc",
  "result": {
    "engine": "knitr",
    "markdown": "---\n# TITLE & AUTHOR\ntitle: \"(4) Observed Confounding\"\nsubtitle: \"Causal Data Science for Business Analytics\"\nauthor: \"Christoph Ihl\"\ninstitute: \"Hamburg University of Technology\"\ndate: today\ndate-format: \"dddd, D. MMMM YYYY\"\n# FORMAT OPTIONS\nformat: \n  revealjs:\n    width: 1600\n    height: 900\n    footer: \"Causal Data Science: (4) Observed Confounding\"\n    slide-number: true\n---\n\n\n\n\n# Identification {data-stack-name=\"Identification\"}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n<style type=\"text/css\">\ndiv.callout-note{border-left-color:#00C1D4 !important}div.callout-note.callout-style-default .callout-title{background-color:#005e73;color:white}.callout.callout-style-default{border-left:solid #005e73 .3rem;border-right:solid 1px #005e73;border-top:solid 1px #005e73;border-bottom:solid 1px #005e73}\n</style>\n:::\n\n\n\n\n## What to do without randomized experiment?\n\n\n::: {.nonincremental}\n- In many business settings, randomization of treatments is not possible.\n  - We need to find ways to work with so-called `observational data`.\n- Identification strategy to **adjust for all confounders** has many names:\n  - Ignorability\n  - Unconfoundedness\n  - Selection-on-observables\n  - Conditional independence\n  - Backdoor adjustment\n  - Measured confounding\n  - **Observed Confounding**\n  \n\n:::\n\n\n## Assumption 1: Conditional Independence {.smaller}\n\n- Potential outcomes are conditionally independent of $T$ after controlling for covariates $\\mathbf{X}$. \n  - $T$ is as good as randomly assigned among subjects with the same values in $\\mathbf{X}$.\n\n::: {.callout-note icon=false}\n\n## Assumption: \"Conditional Exchangeability / Unconfoundedness / Ignorability / Independence\".\n\nFormally, $(Y(1), Y(0)) \\perp\\!\\!\\!\\perp T \\, | \\, \\mathbf{X}$.\n\n:::\n\n::: {.fragment}\n- This entails the somewhat weaker, but sufficient formulation of conditional exchangeability in terms of means: \n  - **before** treatment: $\\mathbb{E}[Y_i(0)|T_i=0, \\mathbf{X_i = x_i}] = \\mathbb{E}[Y_i(0)|T_i=1, \\mathbf{X_i = x_i}]$ \n  - **after** treatment: $\\mathbb{E}[Y_i(1)|T_i=1, \\mathbf{X_i = x_i}] = \\mathbb{E}[Y_i(1)|T_i=0, \\mathbf{X_i = x_i}]$\n  - differences in mean potential outcomes across treatment and control groups are entirely due to differences in observed covariates.\n:::\n\n\n\n\n\n## Backdoor Adjustement {.smaller}\n\n\n::: {.columns}\n\n\n::: {.column width=\"40%\"}\n\n- all confounders observed, measured and controlled for\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_ob_conf_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n\n::: {.column width=\"40%\"}\n\n- some confounders remain unobserved\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_ob_conf_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n\n:::\n\n\n- **Minimal adjustment set** of confounders to be observed: **not testable**.\n- **Sensitivity analysis** can be used to assess the robustness of the results to unobserved confounding (later session).\n\n\n## Observed Confounder Selection\n\n- Credible identification demands to define the adjustment set of variables that makes conditional independence assumption credible to hold.\n- This requires good theoretical or practical knowledge about the treatment assignment mechanism.\n- DAGs provide a principled framework to structure that knowledge and to disentangle good and bad controls:\n  - No post-treatment variables, etc ...\n  - See [Cinelly, Forley and Pearl (2022)](https://journals.sagepub.com/doi/10.1177/00491241221099552) for a nice overview.\n  \n\n## Assumption 2: Common Support\n\n- Conditioning on many covariates can also be detrimental, because we might end up conditioning on a zero probability event for some subgroups / values of X (division by zero) \n\n::: {.callout-note icon=false}\n\n## Assumption: \"Positivity / Overlap / Common Support\".\n\nFor all values of covariates $x$ present in the population of interest (i.e. $x$ such that $P(X=x) > 0$), we have \n$0 < P(T=1|X=x) < 1$.\n\n:::\n\n\n::: {.fragment}\n- There is a trade-off between positivity and conditional independence.\n- Some models might be forced to *extrapolate* to regions without sufficient support by using their parametric assumptions.\n  - Parametric instead of non-parametric identification.\n:::\n\n\n\n\n## Common Support & Extrapolation\n\n\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_ob_conf_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n\n\n\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_ob_conf_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n:::\n\n:::\n\n\n\n\n\n\n## ATE under Observed Confounding\n\n- With the assumptions of conditional unconfoundedness & positivity (together with consistency, and no interference), we can identify the ATE as:\n\n::: {.callout-note icon=false}\n\n## Theorem: \"Identification of the ATE\":\n\n$\\tau_{\\text{ATE}} = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] = \\mathbb{E_X}[\\mathbb{E}[Y_i|T_i=1, X_i] - \\mathbb{E}[Y_i|T_i=0, X_i]]$\n\n:::\n\n\n\n## ATE under Observed Confounding\n\n\n- `Proof:`\n\n$$\n\\begin{align*}\n\\tau_{\\text{ATE}} = \\mathbb{E}[\\tau_i] &= \\mathbb{E}[Y_i(1) - Y_i(0)] \\\\\n&= \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] \\\\\n&\\text{(linearity of expectation)} \\\\\n&= \\mathbb{E}_X [\\mathbb{E}[Y_i(1) \\mid X_i]] - \\mathbb{E}_X [\\mathbb{E}[Y_i(0) \\mid X_i]] \\\\\n&\\text{(law of iterated expectations)} \\\\\n&= \\mathbb{E}_X [\\mathbb{E}[Y_i(1) \\mid T_i = 1, X_i]] - \\mathbb{E}_X [\\mathbb{E}[Y_i(0) \\mid T_i = 0, X_i]] \\\\\n&\\text{(conditional ignorability and positivity)} \\\\\n&= \\mathbb{E}_X [\\mathbb{E}[Y_i \\mid T_i = 1, X_i]] - \\mathbb{E}_X [\\mathbb{E}[Y_i \\mid T_i = 0, X_i]] \\\\\n&\\text{(consistency)}\n\\end{align*}\n$$\n\n\n\n\n# Conditional Outcome Regression {data-stack-name=\"Regression\"}\n\n## Identification with Linear Regression\n\n- Most direct approach is to run a linear regression **conditional on covariates**:\n\n  - $Y_i = \\beta_0 + \\beta_T T_i + \\beta_{X_1} X_{i1} + \\dots + \\beta_{X_K} X_{iK} + \\epsilon_i$\n  - $Y_i = \\beta_0 + \\beta_T T_i + \\mathbf{\\beta_{X}' X_i} + \\epsilon_i$\n  \n  - $\\mathbb{E}[Y_i | T_i, \\mathbf{X_i}] = \\beta_0 + \\beta_T T_i + \\mathbf{\\beta_{X}' X_i}$\n\n::: {.fragment}\n\n- If this linear model is correct, then (given $T_i$ is binary):\n  - $\\tau_{\\text{CATE}} = \\mathbb{E}[Y_i | T_i = 1, \\mathbf{X_i}] - \\mathbb{E}[Y_i | T_i = 0, \\mathbf{X_i}]$\n  - $\\tau_{\\text{CATE}} = (\\beta_0 + \\beta_T \\cdot 1  + \\mathbf{\\beta_{X}' X_i}) - (\\beta_0 + \\beta_T \\cdot 0 + \\mathbf{\\beta_{X}' X_i}) = \\beta_T$\n  - $\\tau_{\\text{ATE}} = \\mathbb{E_X}[\\beta_T] = \\beta_T$\n\n:::\n\n\n\n## Frisch-Waugh-Lovell (FWL) Theorem\n\n- We can estimate $\\beta_T$ in a standard linear regression $Y_i = \\beta_0 + \\beta_T T_i + \\mathbf{\\beta_{X}' X_i} + \\epsilon_i$ in a `three-stage procedure`:\n\n1. Run a regression of the form $Y_i = \\beta_{Y0} + \\mathbf{\\beta_{Y \\sim X}' X_i} + \\epsilon_{Y_i \\sim X_i}$ and extract the estimated residuals $\\hat\\epsilon_{Y_i \\sim X_i}$.\n\n2. Run a regression of the form $T_i = \\beta_{T0} + \\mathbf{\\beta_{T \\sim X}' X_i} + \\epsilon_{T_i \\sim X_i}$ and extract the estimated residuals $\\hat\\epsilon_{T_i \\sim X_i}$.\n\n3. Run a residual-on-residual regression of the form $\\hat\\epsilon_{Y_i \\sim X_i} = \\beta_T \\hat\\epsilon_{T_i \\sim X_i} + \\epsilon_i$ (no constant).\n\nThe resulting estimate $\\hat\\beta_T$ is `numerically identical` to the estimate we would get if we just run the full OLS model.\n\n\n## Controlling for Covariates: FWL Theorem Logic\n\n\n![](_images/4/controlForX.gif)\n\n\n\n\n\n## Identification with Linear Regression\n  \n- OLS estimate of $\\hat{\\beta_T}$ is now given by:\n  - $Y_i = \\hat{\\beta_0} + \\hat{\\beta}_T T_i + \\mathbf{\\hat{\\beta}_{X}' X_i} + \\hat{\\epsilon}_i$\n  - $\\hat{\\beta_T} = \\frac{\\text{Cov}(Y_i, T_i | \\mathbf{X_i})}{\\text{Var}(T_i | \\mathbf{X_i})}$\n  - Difference to randomized experiments: $\\mathbf{X_i}$ may affect $T_i$, i.e. $\\text{Cov}(T_i, \\mathbf{X_i}) \\neq 0$\n\n::: {.fragment}\n\n- Two possible implications:\n  - (1) $se(\\hat{\\beta_T})$ may be larger than in randomized experiments.\n  - (2) if the linear model is misspecified, $\\hat{\\beta_T}$ may be biased and inconsistent.\n    - if the relation between $Y_i$ and $\\mathbf{X_i}$ is in fact non-linear, this spills over to the estimation of $\\hat{\\beta_T}$ via the\ncorrelation of $T_i$ and $\\mathbf{X_i}$.\n\n:::\n\n## Identification with Linear Regression\n\n- Possible misspecifications: Omission of ...\n  - (1) Multiplicative interactions between covariates: e.g. $X_{i1} \\cdot X_{i2}$.\n  - (2) Higher order terms: e.g. $X_{i1} \\cdot X_{i1} = X_{i1}^2$.\n  - (3) Interactions between treatment and covariates to allow for `effect heterogeneity`: e.g. $T_i \\cdot X_{i1}$.\n  \n::: {.fragment}\n\n- More flexible model:\n  - $\\mathbb{E}[Y_i | T_i, \\mathbf{X_i}] = \\beta_0 + \\beta_T T_i + \\mathbf{\\beta_{X}' X_i} + \\mathbf{\\beta_{TX}' X_i}T_i + \\beta_{X^2_1} X^2_{i1} + \\dots + \\beta_{X_1X_2} X_{i1} X_{i2} + \\dots$\n  - $\\tau_{\\text{CATE}} = \\mathbb{E}[Y_i | T_i = 1, \\mathbf{X_i}] - \\mathbb{E}[Y_i | T_i = 0, \\mathbf{X_i}]$\n  - $\\tau_{\\text{CATE}} = \\beta_T + \\mathbf{\\beta_{TX}' X_i}$\n  - $\\tau_{\\text{ATE}} = \\mathbb{E_X}[\\tau_{\\text{CATE}}] = \\mathbb{E_X}[\\beta_T + \\mathbf{\\beta_{TX}' X_i}] = \\beta_T + \\beta_{TX}'\\mathbb{E_X}[\\mathbf{X_i}] = \\beta_T + \\mathbf{\\beta_{TX}'\\overline{X}}$\n:::\n\n\n## Identification with Linear Regression\n\n- Alternatively, estimate two separate models for treated and control group:\n  - $\\mathbb{E}[Y_i | T_i = 1, \\mathbf{X_i}] = \\beta_{0,1} + \\mathbf{\\beta_{X,1}' X_i} + \\beta_{X^2_1,1} X^2_{i1} + \\dots + \\beta_{X_1X_2,1} X_{i1} X_{i2} + \\dots$\n  - $\\mathbb{E}[Y_i | T_i = 0, \\mathbf{X_i}] = \\beta_{0,0} + \\mathbf{\\beta_{X,0}' X_i} + \\beta_{X^2_1,0} X^2_{i1} + \\dots + \\beta_{X_1X_2,0} X_{i1} X_{i2} + \\dots$\n  - $\\tau_{\\text{CATE}} = \\mathbb{E}[Y_i | T_i = 1, \\mathbf{X_i}] - \\mathbb{E}[Y_i | T_i = 0, \\mathbf{X_i}]$\n- Then averaging:\n  - $\\tau_{\\text{ATE}} = \\mathbb{E_X}[\\tau_{\\text{CATE}}] = \\mathbb{E_X}[\\mathbb{E}[Y_i | T_i = 1, \\mathbf{X_i}] - \\mathbb{E}[Y_i | T_i = 0, \\mathbf{X_i}]]$\n\n\n\n\n\n## Conditional Outcome Regression: Example {.smaller}\n\n- Assess the impact of participating in the U.S. National Supported Work (NSW) training program targeted to 445 individuals with social and economic problems on their real earnings. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(Matching)                       # load Matching package for the data\nlibrary(Jmisc)                          # load Jmisc package with demean function\nlibrary(lmtest)                         # load lmtest package\nlibrary(sandwich)                       # load sandwich package\nlibrary(modelsummary)                   # load modelsummary package\ndata(lalonde)                           # load lalonde data\nattach(lalonde)                         # store all variables in own objects\nT = treat                                 # define treatment (training)\nY = re78                                  # define outcome \nX = cbind(age,educ,nodegr,married,black,hisp,re74,re75,u74,u75) # covariates\nDXdemeaned = T * demean(X)                  # interaction of D and demeaned X \nols = lm(Y ~ T + X + DXdemeaned)                # run OLS regression\nmodelsummary(ols, vcov = sandwich::vcovHC, estimate = \"est = {estimate} (se = {std.error}, t = {statistic}){stars}\", statistic = \"p = {p.value}, CI = [{conf.low}, {conf.high}]\", gof_map = c(\"r.squared\"), coef_omit = \"X\")  \n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!DOCTYPE html> \n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>tinytable_418i2gk38zyydi5ho9y5</title>\n    <style>\n.table td.tinytable_css_nngbuyqmi28smp3mvm8p, .table th.tinytable_css_nngbuyqmi28smp3mvm8p {    border-bottom: solid 0.1em #d3d8dc; }\n.table td.tinytable_css_d4gwvzfvb6qwls50nqc7, .table th.tinytable_css_d4gwvzfvb6qwls50nqc7 {    text-align: left; }\n.table td.tinytable_css_cew9o18fle1s9udnkdch, .table th.tinytable_css_cew9o18fle1s9udnkdch {    text-align: center; }\n.table td.tinytable_css_vesx3p3cdngv5knh664p, .table th.tinytable_css_vesx3p3cdngv5knh664p {    border-bottom: solid 0.05em black; }\n    </style>\n    <script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n    <script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n    <script>\n    MathJax = {\n      tex: {\n        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']]\n      },\n      svg: {\n        fontCache: 'global'\n      }\n    };\n    </script>\n  </head>\n\n  <body>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_418i2gk38zyydi5ho9y5\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\"> </th>\n                <th scope=\"col\">(1)</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td>(Intercept)</td>\n                  <td>est = 7161.163 (se = 3975.959, t = 1.801)+</td>\n                </tr>\n                <tr>\n                  <td>           </td>\n                  <td>p = 0.072, CI = [-653.935, 14976.260]     </td>\n                </tr>\n                <tr>\n                  <td>T          </td>\n                  <td>est = 1583.468 (se = 711.171, t = 2.227)* </td>\n                </tr>\n                <tr>\n                  <td>           </td>\n                  <td>p = 0.027, CI = [185.600, 2981.336]       </td>\n                </tr>\n                <tr>\n                  <td>R2         </td>\n                  <td>0.104                                     </td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n\n    <script>\n      function styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(i, j, css_id) {\n        var table = document.getElementById(\"tinytable_418i2gk38zyydi5ho9y5\");\n        table.rows[i].cells[j].classList.add(css_id);\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_418i2gk38zyydi5ho9y5');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_tinytable_w8mmo0eot9hzgtm3yxtu(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_418i2gk38zyydi5ho9y5\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(0, 0, 'tinytable_css_nngbuyqmi28smp3mvm8p') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(0, 1, 'tinytable_css_nngbuyqmi28smp3mvm8p') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(0, 0, 'tinytable_css_d4gwvzfvb6qwls50nqc7') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(1, 0, 'tinytable_css_d4gwvzfvb6qwls50nqc7') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(2, 0, 'tinytable_css_d4gwvzfvb6qwls50nqc7') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(3, 0, 'tinytable_css_d4gwvzfvb6qwls50nqc7') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(4, 0, 'tinytable_css_d4gwvzfvb6qwls50nqc7') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(5, 0, 'tinytable_css_d4gwvzfvb6qwls50nqc7') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(0, 1, 'tinytable_css_cew9o18fle1s9udnkdch') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(1, 1, 'tinytable_css_cew9o18fle1s9udnkdch') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(2, 1, 'tinytable_css_cew9o18fle1s9udnkdch') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(3, 1, 'tinytable_css_cew9o18fle1s9udnkdch') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(4, 1, 'tinytable_css_cew9o18fle1s9udnkdch') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(5, 1, 'tinytable_css_cew9o18fle1s9udnkdch') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(4, 0, 'tinytable_css_vesx3p3cdngv5knh664p') })\nwindow.addEventListener('load', function () { styleCell_tinytable_w8mmo0eot9hzgtm3yxtu(4, 1, 'tinytable_css_vesx3p3cdngv5knh664p') })\n    </script>\n\n  </body>\n\n</html>\n```\n\n:::\n:::\n\n\n\n\n\n# Matching {data-stack-name=\"Matching\"}\n\n\n\n## Matching Idea\n\n![](_images/4/matchfig.gif)\n\n\n## Matching Methods\n\n- `Idea:` find and match treated and nontreated observations with similar (or ideally identical) covariate values.\n  - create a sample of treated and nontreated groups that are comparable in terms of covariate distributions, just as it would be\nthe case in a successful experiment.\n  - With or without replacement.\n  - 1:1 or 1:M matching.\n\n- Methods:\n  - Nearest Neighbor Matching\n  - Radius (Caliper) Matching\n  - Propensity Score Matching\n  \n\n## Nearest Neighbor Matching\n\n- For each treated unit, find the one nearest nontreated unit in terms of covariate values (1:1 matching):\n  - Average treatment effect on the treated (ATT):\n  - $\\hat{\\tau}_{\\text{ATT}} = \\frac{1}{n_1} \\sum_{i:T_i=1} \\left( Y_i - \\sum_{j:T_j=0} I( \\lVert \\mathbf{X_j} - \\mathbf{X_i} \\rVert = \\min_{l:T_l=0} \\lVert \\mathbf{X_l} - \\mathbf{X_i} \\rVert) Y_j) \\right)$\n  - Average treatment effect on the untreated (ATU):\n  - $\\hat{\\tau}_{\\text{ATU}} = \\frac{1}{n_0} \\sum_{i:T_i=0} \\left( Y_i - \\sum_{j:T_j=1} I( \\lVert \\mathbf{X_j} - \\mathbf{X_i} \\rVert = \\min_{l:T_l=1} \\lVert \\mathbf{X_l} - \\mathbf{X_i} \\rVert) Y_j) \\right)$\n  - Average treatment effect (ATE):\n  - $\\hat{\\tau}_{\\text{ATE}} = \\frac{n_1}{n} \\hat{\\tau}_{\\text{ATT}} +  \\frac{n_0}{n} \\hat{\\tau}_{\\text{ATU}}$\n\n\n## Distance Measures\n\n  - Euclidean Distance:\n    - $\\lVert \\mathbf{X_j} - \\mathbf{X_i} \\rVert = \\sqrt{ \\sum_{k=1}^{K} (X_{jk} - X_{ik})^2 }$\n    - Standardized version by normalizing based on the variance of the covariates: \n    - $\\lVert \\mathbf{X_j} - \\mathbf{X_i} \\rVert = \\sqrt{ \\sum_{k=1}^{K} \\frac{(X_{jk} - X_{ik})^2}{\\hat{\\text{Var}}(X_k)} }$\n  - Mahalanobis Distance:\n    - In addition, normalizing based on covariance with remaining covariates:\n    - $\\lVert \\mathbf{X_j} - \\mathbf{X_i} \\rVert = \\sqrt{\\sum_{k=1}^{K} \\sum_{l=1}^{K} \\frac{(X_{jk} - X_{ik}) (X_{jl} - X_{il})}{\\hat{\\text{Cov}}(X_k, X_l)}}$\n\n\n## 1:M Matching\n\n- 1:M matching with fixed M:\n  - $\\hat{\\tau}_{\\text{ATT}} = \\frac{1}{n_1} \\sum_{i:T_i=1} \\left[ Y_i - \\hat{\\mu_0}(\\mathbf{X_i}) \\right]$ with $\\hat{\\mu}_{0}(\\mathbf{X_i}) = \\frac{1}{M} \\sum_{j \\in J(i:T_i=1)} Y_j$\n  - $\\hat{\\tau}_{\\text{ATU}} = \\frac{1}{n_0} \\sum_{i:T_i=0} \\left[ Y_i - \\hat{\\mu_1}(\\mathbf{X_i}) \\right]$ with $\\hat{\\mu}_{1}(\\mathbf{X_i}) = \\frac{1}{M} \\sum_{j \\in J(i:T_i=0)} Y_j$\n   - $\\hat{\\tau}_{\\text{ATE}} = \\frac{n_1}{n} \\hat{\\tau}_{\\text{ATT}} +  \\frac{n_0}{n} \\hat{\\tau}_{\\text{ATU}}$\n\n::: {.fragment}\n\n- `Regression-based correction for the bias` which comes from not finding fully comparable matches for a reference observation:\n  - $\\hat{\\mu}_{0}(\\mathbf{X_i}) = \\frac{1}{M} \\sum_{j \\in J(i:T_i=1)} [Y_j - (\\tilde{\\mu}_{0}(\\mathbf{X_j}) - \\tilde{\\mu}_{0}(\\mathbf{X_i}))]$\n  - $\\hat{\\mu}_{1}(\\mathbf{X_i}) = \\frac{1}{M} \\sum_{j \\in J(i:T_i=0)} [Y_j - (\\tilde{\\mu}_{1}(\\mathbf{X_j}) - \\tilde{\\mu}_{1}(\\mathbf{X_i}))]$\n  - where $\\tilde{\\mu}_{0}(\\mathbf{X_i})$ (respectively, $\\tilde{\\mu}_{1}(\\mathbf{X_i})$) is the predicted value of $Y$ for observation $i$ from a regression of $Y$ on $X$ among the untreated (respectively, treated) group.\n\n:::\n\n\n## Radius (Caliper) Matching\n\n- Caliper $C$ (implies a variable M):\n  - $\\hat{\\mu}_{0/1}(\\mathbf{X_i}) = \\frac{\\sum_{j : T_j = 0/1} I\\left(\\lVert \\mathbf{X_j} - \\mathbf{X_i} \\rVert \\leq C \\right) \\cdot Y_i}{\\sum_{j : T_j = 0/1} I\\left(\\lVert \\mathbf{X_j} - \\mathbf{X_i} \\rVert \\leq C \\right)}$ \n  - With a kernel function $\\kappa$:\n  - $\\hat{\\mu}_{0/1}(\\mathbf{X_i}) = \\frac{\\sum_{j : T_j = 0/1} \\kappa\\left(\\frac{\\lVert \\mathbf{X_j} - \\mathbf{X_i} \\rVert}{C}\\right) \\cdot Y_i}{\\sum_{j : T_j = 0/1} \\kappa\\left(\\frac{\\lVert \\mathbf{X_j} - \\mathbf{X_i} \\rVert}{C}\\right)}$   \n\n\n\n## Covariate Matching: Example {.smaller}\n\n- Assess the impact of participating in the U.S. National Supported Work (NSW) training program targeted to 445 individuals with social and economic problems on their real earnings. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(Matching)                       # load Matching package for the data\ndata(lalonde)                           # load lalonde data\nattach(lalonde)                         # store all variables in own objects\nT = treat                               # define treatment (training)\nY = re78                                # define outcome \nX = cbind(age,educ,nodegr,married,black,hisp,re74,re75,u74,u75) # covariates\npairmatching = Match(Y=Y, Tr=T, X=X)    # pair matching\nsummary(pairmatching)              # matching output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEstimate...  1686.1 \nAI SE......  866.4 \nT-stat.....  1.9461 \np.val......  0.051642 \n\nOriginal number of observations..............  445 \nOriginal number of treated obs...............  185 \nMatched number of observations...............  185 \nMatched number of observations  (unweighted).  267 \n```\n\n\n:::\n\n```{.r .cell-code}\npairmatching = Match(Y=Y, Tr=T, X=X, M=3, BiasAdjust = TRUE, estimand = \"ATE\", caliper = NULL)    # pair matching\nsummary(pairmatching)              # matching output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEstimate...  1396.3 \nAI SE......  712.09 \nT-stat.....  1.9609 \np.val......  0.049893 \n\nOriginal number of observations..............  445 \nOriginal number of treated obs...............  185 \nMatched number of observations...............  445 \nMatched number of observations  (unweighted).  1525 \n```\n\n\n:::\n:::\n\n\n\n\n## Propensity Score Matching\n\n- `Curse of dimensionality` as caveat of covariate matching:\n  - Directly controlling for and matching observations based on $\\mathbf{X}$ in a flexible, non-parametric way increasingly difficult with many covariates.\n  - Probability to find matches with similar values in all covariates decays rapidly.\n\n\n::: {.fragment}\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n- Instead controlling for the `Propensity Score`:\n  - Conditional treatment probability given the covariates, denoted by \n  - $PS(\\mathbf{X_i}) = P(T_i = 1 | \\mathbf{X_i})$\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_ob_conf_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n:::\n\n:::\n\n\n\n## Propensity Score\n\n- We need the following theorem to hold: \n\n::: {.callout-note icon=false}\n\n## Theorem 4.1: \"Propensity Score\".\n\nUnconfoundedness given $\\mathbf{X}$ implies unconfoundedness given the propensity score $PS(\\mathbf{X})$.\n\nFormally, $(Y(1), Y(0)) \\perp\\!\\!\\!\\perp T \\, | \\, \\mathbf{X} \\implies (Y(1), Y(0)) \\perp\\!\\!\\!\\perp PS(\\mathbf{X})$.\n\n:::\n\n- $PS(\\mathbf{X_i})$ can be viewed as dimension reduction tool\n  - controlling for a one-dimensional scalar instead of $\\mathbf{X}$.\n- If theorem holds, then we can state for the `treatments assignment mechanism`:\n  $P(T_i = 1 | Y_i(1), Y_i(0), PS(\\mathbf{X_i})) = P(T_i = 1 | PS(\\mathbf{X_i}))$\n\n\n## Propensity Score {.smaller}\n\n- `Proof:`\n\n$$\n\\begin{align*}\nP(T_i = 1 | Y_i(1), Y_i(0), PS(\\mathbf{X_i})) &= \\mathbb{E}[T_i | Y_i(1), Y_i(0), PS(\\mathbf{X_i})] \\\\\n&\\text{(T is binary: turn probability into expectation)} \\\\\n&= \\mathbb{E}[\\mathbb{E}[T_i | Y_i(1), Y_i(0), PS(\\mathbf{X_i}), \\mathbf{X_i}] | Y_i(1), Y_i(0), PS(\\mathbf{X_i})] \\\\\n&\\text{(law of iterated expectations: introduce X)} \\\\\n&= \\mathbb{E}[\\mathbb{E}[T_i | Y_i(1), Y_i(0), \\mathbf{X_i}] | Y_i(1), Y_i(0), PS(\\mathbf{X_i})] \\\\\n&\\text{(remove PS(X) from inner expectation, because it is redundant if we have X)} \\\\\n&= \\mathbb{E}[\\mathbb{E}[T_i | \\mathbf{X_i}] | Y_i(1), Y_i(0), PS(\\mathbf{X_i})] \\\\\n&\\text{(apply original unconfoundedness and eliminate Y_i(t))} \\\\\n&= \\mathbb{E}[P(T_i = 1 | \\mathbf{X_i}) | Y_i(1), Y_i(0), PS(\\mathbf{X_i})] \\\\\n&\\text{(T is binary: turn expectation into probability)} \\\\\n&= \\mathbb{E}[PS(\\mathbf{X_i}) | Y_i(1), Y_i(0), PS(\\mathbf{X_i})] \\\\\n&\\text{(conditioning on itself makes addional info from Y_i(t) superfluous)} \\\\\n&= PS(\\mathbf{X_i}) = P(T_i = 1 | \\mathbf{X_i})\n\\end{align*}\n$$\n\n\n## Propensity Score Estimation and Use {.smaller}\n\n\n- True $PS(\\mathbf{X_i})$ for each observation $i$ is unknown and needs to be estimated.\n- Typically a parametric binary choice model is used of the form:\n  - $PS(\\mathbf{X_i}) = P(T_i = 1 | \\mathbf{X_i}) = \\Lambda(\\beta_0 + \\beta_1 X_{i1} + \\ldots + \\beta_k X_{ik})$\n  - Non-linear link function $\\Lambda$:\n    - Normal distribution function: Probit regression model\n    - Logistic distribution function: Logit regression model\n\n::: {.fragment}\n\n- Parameters $\\mathbf{\\hat{\\beta}}$ are estimated by maximum likelihood estimation (MLE):\n   - $\\mathbf{\\hat{\\beta}} = \\arg\\max_{\\beta} \\sum_{i=1}^n \\left[ T_i \\ln \\Lambda(\\beta_0 + \\beta_1 X_{i1} + \\ldots + \\beta_k X_{ik}) + (1 - T_i) \\ln (1 - \\Lambda(\\beta_0 + \\beta_1 X_{i1} + \\ldots + \\beta_k X_{ik})) \\right]$\n- $\\hat{PS}(\\mathbf{X_i})$ is then computed based on the following prediction:\n  - $\\hat{PS}(\\mathbf{X_i}) = \\Lambda(\\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\ldots + \\hat{\\beta}_k X_{ik})$\n\n:::\n\n\n\n## Propensity Score Use \n\n- Propensity scores $\\hat{PS}(\\mathbf{X_i})$ can then be used as a single scalar covariate in:\n  - (1) Conditional outcome regression\n  - (2) Covariate matching\n  - (3) Inverse probability weighting (IPW)\n- Inference needs to take into account the estimation uncertainty of $\\hat{PS}(\\mathbf{X_i})$ via:\n  - (1) Bias correction in variance estimation\n  - (2) Bootstrapping\n\n\n## Propensity Score Matching: Example {.smaller}\n\n- Assess the impact of participating in the U.S. National Supported Work (NSW) training program targeted to 445 individuals with social and economic problems on their real earnings. \n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(Matching)                       # load Matching package for the data\ndata(lalonde)                           # load lalonde data\nattach(lalonde)                         # store all variables in own objects\nT = treat                               # define treatment (training)\nY = re78                                # define outcome \nX = cbind(age,educ,nodegr,married,black,hisp,re74,re75,u74,u75) # covariates\nps = glm(T ~ X, family=binomial)$fitted # estimate the propensity score by logit\npsmatching=Match(Y=Y, Tr=T, X=ps, BiasAdjust = TRUE, estimand = \"ATE\") # propensity score matching\nsummary(psmatching)              # matching output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEstimate...  1590.7 \nAI SE......  700.77 \nT-stat.....  2.2699 \np.val......  0.023211 \n\nOriginal number of observations..............  445 \nOriginal number of treated obs...............  185 \nMatched number of observations...............  445 \nMatched number of observations  (unweighted).  709 \n```\n\n\n:::\n:::\n\n\n::: \n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(Matching)                       # load Matching package\nlibrary(boot)                           # load boot package\ndata(lalonde)                           # load lalonde data\nattach(lalonde)                         # store all variables in own objects\nT=treat                                 # define treatment (training)\nY=re78                                  # define outcome \nX=cbind(age,educ,nodegr,married,black,hisp,re74,re75,u74,u75) # covariates\nbs=function(data, indices) {            # defines function bs for bootstrapping\n  dat=data[indices,]                    # bootstrap sample according to indices \n  ps=glm(dat[,2:ncol(dat)],data=dat,family=binomial)$fitted # propensity score\n  effect=Match(Y=dat[,1], Tr=dat[,2], X=ps, BiasAdjust = TRUE, estimand = \"ATE\")$est # ATET \n  return(effect)                        # returns the estimated ATET\n}                                       # closes the function bs   \nbootdata=data.frame(Y,T,X)              # data frame for bootstrap procedure\nset.seed(1)                             # set seed\nresults = boot(data=bootdata, statistic=bs, R=999) # 999 bootstrap estimations \nresults                                 # displays the results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = bootdata, statistic = bs, R = 999)\n\n\nBootstrap Statistics :\n    original  bias    std. error\nt1*  1590.71 41.8785    771.4947\n```\n\n\n:::\n\n```{.r .cell-code}\ntstat=results$t0/sd(results$t)          # compute the t-statistic\n2*pnorm(-abs(tstat))                    # compute the p-value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]\n[1,] 0.03922158\n```\n\n\n:::\n:::\n\n\n\n::: \n\n::: \n\n  \n# Inverse Probability Weighting (IPW) {data-stack-name=\"IPW\"}\n\n## Inverse Probability Weighting: Idea\n\n- Creating a `pseudo-population` where the treatment assignment is independent of the observed covariates:\n  - Weighting each observation by the inverse of the estimated propensity score.\n  \n- This idea can be expressed in the following theorem:\n\n::: {.callout-note icon=false}\n\n## Theorem 4.2: \"Inverse Propensity Score Weighting\".\n\nGiven $(Y(1), Y(0)) \\perp\\!\\!\\!\\perp T \\, | \\, \\mathbf{X}$ (conditional unconfoundedness) and given $0 < PS(\\mathbf{X_i}) < 1$ for all $i$ (positivity), then:\n\n$$\\tau_{\\text{ATE}} = \\mathbb{E}[Y_i(1) - Y_i(0)] = \\mathbb{E}[\\frac{T_iY_i}{PS(\\mathbf{X_i})}] - \\mathbb{E}[\\frac{(1-T_i)Y_i}{1-PS(\\mathbf{X_i})}]$$\n\n:::\n\n\n\n\n\n\n## Inverse Propensity Score Weighting {.smaller}\n\n- `Proof` for $\\mathbb{E}[Y_i(1)]$:\n\n$$\n\\begin{align*}\n\\mathbb{E}\\left[\\frac{T_iY_i}{PS(\\mathbf{X_i})}\\right] = \\mathbb{E}\\left[\\frac{T_iY_i(1)}{PS(\\mathbf{X_i})}\\right] &= \\mathbb{E}\\left[\\mathbb{E}\\left[\\frac{T_iY_i(1)}{PS(\\mathbf{X_i})} \\bigg| \\mathbf{X_i}\\right]\\right] \\\\\n&\\text{(law of iterated expectations: introduce X)} \\\\\n&= \\mathbb{E}\\left[\\frac{1}{PS(\\mathbf{X_i})} \\mathbb{E}[T_iY_i(1) | \\mathbf{X_i}]\\right] \\\\\n&\\text{(1/PS(X) is non-random given X, so it can be moved outside)} \\\\\n&= \\mathbb{E}\\left[\\frac{1}{e(X)} \\mathbb{E}(Z | X) \\mathbb{E}(Y_i(1) | \\mathbf{X_i})\\right] \\\\\n&\\text{(condional ignorability between T and Y(1) allows to separate expectations)} \\\\\n&= \\mathbb{E}\\left[\\frac{1}{PS(\\mathbf{X_i})} PS(\\mathbf{X_i}) \\mathbb{E}[Y_i(1) | \\mathbf{X_i}]\\right] \\\\\n&\\text{(E(Z∣X)=PS(X) by definition, terms cancel out)} \\\\\n&= \\mathbb{E}\\left[\\mathbb{E}[Y_i(1) | \\mathbf{X_i}]\\right] = \\mathbb{E}[Y_i(1)] \\\\\n&\\text{(law of iterated expectations)} \\\\\n\\end{align*}\n$$\n\n\n## IPW Estimators\n\n- Theorem 4.2 motivates the following estimator for ATE:\n  - $\\hat{\\tau}_{ATE} = \\frac{1}{n} \\sum_{i=1}^n \\frac{T_i Y_i}{\\hat{PS}(\\mathbf{X_i})} - \\frac{1}{n} \\sum_{i=1}^n \\frac{(1 - T_i) Y_i}{1 - \\hat{PS}(\\mathbf{X_i})}$\n  \n- Normalize the weights so that they sum up to one within the treatment groups:\n  - $\\hat{\\tau}_{ATE} = \\frac{\\sum_{i=1}^n \\frac{T_i Y_i}{\\hat{PS}(\\mathbf{X_i})}}{\\sum_{i=1}^n \\frac{T_i}{\\hat{PS}(\\mathbf{X_i})}} - \\frac{\\sum_{i=1}^n \\frac{(1 - T_i) Y_i}{1 - \\hat{PS}(\\mathbf{X_i})}}{\\sum_{i=1}^n \\frac{1 - T_i}{1 - \\hat{PS}(\\mathbf{X_i})}}$\n\n::: {.fragment}\n\n- IPW variant `Empirical Likelihood (EL)` estimator:\n  - Estimating the propensity score parameters while enforcing a moment condition (e.g., equal covariate means, variances etc. across treatment groups):\n  - $\\sum_{i=1}^n \\frac{1}{n} \\left[ \\tilde{X}_i \\cdot T_i - \\frac{ \\tilde{X}_i \\cdot (1 - T_i) \\cdot \\tilde{PS}(\\mathbf{X_i})}{1 - \\tilde{PS}(\\mathbf{X_i})} \\right] = 0$\n\n:::\n\n\n## Inverse Probability Weighting: Example {.smaller}\n\n- Assess the impact of participating in the U.S. National Supported Work (NSW) training program targeted to 445 individuals with social and economic problems on their real earnings. \n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n- Normalized IPW estimator:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(causalweight)                   # load causalweight package\nlibrary(Matching)                       # load Matching package for the data\ndata(lalonde)                           # load lalonde data\nattach(lalonde)                         # store all variables in own objects\nT = treat                               # define treatment (training)\nY = re78                                # define outcome \nX = cbind(age,educ,nodegr,married,black,hisp,re74,re75,u74,u75) # covariates\nset.seed(1)                             # set seed\nipw=treatweight(y=Y,d=T,x=X, boot=999)  # run IPW with 999 bootstraps \nipw$effect                              # show ATE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1640.468\n```\n\n\n:::\n\n```{.r .cell-code}\nipw$se                                  # show standard error\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 678.4239\n```\n\n\n:::\n\n```{.r .cell-code}\nipw$pval                                # show p-value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01560364\n```\n\n\n:::\n:::\n\n\n::: \n\n::: {.column width=\"50%\"}\n\n- Empirical Likelihood (EL) estimator to ensure mean covariate balance:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(Matching)                       # load Matching package\ndata(lalonde)                           # load lalonde data\nattach(lalonde)                         # store all variables in own objects\nlibrary(CBPS)                           # load CBPS package\nlibrary(lmtest)                         # load lmtest package\nlibrary(sandwich)                       # load sandwich package\nT = treat                              # define treatment (training)\nY = re78                                # define outcome \nX = cbind(age,educ,nodegr,married,black,hisp,re74,re75,u74,u75) # covariates\ncbps = CBPS(T ~ X, ATT = 0)                 # covariate balancing for ATE estimation\nresults = lm(Y ~ T, weights = cbps$weights)   # weighted regression\ncoeftest(results, vcov = vcovHC)        # show results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4582.71     353.65 12.9582  < 2e-16 ***\nT            1699.74     705.11  2.4106  0.01633 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n::: \n\n::: \n\n\n# Doubly Robust Methods {data-stack-name=\"Doubly Robust Methods\"}\n\n## Doubly Robust Methods: Idea\n\n- Under conditional unconfoundedness ($Y(1), Y(0)) \\perp\\!\\!\\!\\perp T \\, | \\, \\mathbf{X}$) and positivity <br>($0 < PS(\\mathbf{X_i}) < 1$) we have seen two ways to identify the ATE:\n  - `Conditional outcome regression:` \n    - $\\tau_{\\text{ATE}} = \\mathbb{E}[\\mathbb{E}[Y_i | T_i = 1, \\mathbf{X_i}] - \\mathbb{E}[Y_i | T_i = 0, \\mathbf{X_i}]] = \\mathbb{E}[\\mu_1(\\mathbf{X_i}) - \\mu_0(\\mathbf{X_i})]$\n  - `Inverse probability weighting:` \n    - $\\tau_{\\text{ATE}} = \\mathbb{E}\\left[\\frac{T_i Y_i}{PS(\\mathbf{X_i})} - \\frac{(1 - T_i) Y_i}{1 - PS(\\mathbf{X_i})}\\right]$\n  \n::: {.fragment}\n- Idea of `doubly robust` methods: \n  - Combine both approaches, such that the ATE estimator is consistent, even if only one of the two models is correctly specified. \n::: \n\n\n## Doubly Robust Estimator: Definition\n\n- Doubly robust estimator is also called the `Augmented Inverse Propensity Score Weighting (AIPW)` estimator.\n- Augmenting the outcome regression model with IPW weights:\n  - $\\tilde{\\mu}_{1}^{\\text{dr}} = \\mathbb{E}\\left[\\frac{T_i(Y_i - \\mu_1(X_i, \\beta_1))}{PS(\\mathbf{X_i, \\beta_{PS}})} + \\mu_1(\\mathbf{X_i,\\beta_1})\\right]$\n  - $\\tilde{\\mu}_{0}^{\\text{dr}} = \\mathbb{E}\\left[\\frac{(1-T_i)(Y_i - \\mu_0(X_i, \\beta_0))}{1 - PS(\\mathbf{X_i, \\beta_{PS}})} + \\mu_0(\\mathbf{X_i,\\beta_0})\\right]$\n- Or rewrite, to augment the IPW estimator with outcome regression model:\n  - $\\tilde{\\mu}_{1}^{\\text{dr}} = \\mathbb{E}\\left[\\frac{T_iY_i}{PS(\\mathbf{X_i, \\beta_{PS}})} - \\frac{T_i - PS(\\mathbf{X_i, \\beta_{PS}})}{PS(\\mathbf{X_i, \\beta_{PS}})}\\mu_1(\\mathbf{X_i,\\beta_1})\\right]$\n  - $\\tilde{\\mu}_{0}^{\\text{dr}} = \\mathbb{E}\\left[\\frac{(1-T_i)Y_i}{1-PS(\\mathbf{X_i, \\beta_{PS}})} - \\frac{PS(\\mathbf{X_i, \\beta_{PS}}) - T_i}{1-PS(\\mathbf{X_i, \\beta_{PS}})}\\mu_0(\\mathbf{X_i,\\beta_0})\\right]$\n\n  \n## Doubly Robust Estimator: Theorem\n\n- Augmentation leads to the following theoretical properties:\n\n::: {.callout-note icon=false}\n\n## Theorem 4.3: \"Doubly Robust Estimator\".\n\nGiven $(Y(1), Y(0)) \\perp\\!\\!\\!\\perp T \\, | \\, \\mathbf{X}$ (conditional unconfoundedness) and given $0 < PS(\\mathbf{X_i}) < 1$ for all $i$ (positivity), then:\n\n(1) If either $PS(\\mathbf{X_i, \\beta_{PS}}) = PS(\\mathbf{X_i})$ or $\\mu_1(\\mathbf{X_i,\\beta_1}) = \\mu_1(\\mathbf{X_i})$, then $\\tilde{\\mu}_{1}^{\\text{dr}} = \\mathbb{E}[Y_i(1)]$\n\n(2) If either $PS(\\mathbf{X_i, \\beta_{PS}}) = PS(\\mathbf{X_i})$ or $\\mu_0(\\mathbf{X_i,\\beta_0}) = \\mu_0(\\mathbf{X_i})$, then $\\tilde{\\mu}_{0}^{\\text{dr}} = \\mathbb{E}[Y_i(0)]$\n\n(3) If either $PS(\\mathbf{X_i, \\beta_{PS}}) = PS(\\mathbf{X_i})$ or $\\{\\mu_1(\\mathbf{X_i,\\beta_1}) = \\mu_1(\\mathbf{X_i}), \\mu_0(\\mathbf{X_i,\\beta_0}) = \\mu_0(\\mathbf{X_i})\\}$, then $\\tilde{\\mu}_{1}^{\\text{dr}} - \\tilde{\\mu}_{0}^{\\text{dr}} = \\tau_{\\text{ATE}}^{\\text{dr}}$\n\n:::\n\n\n## Doubly Robust Estimator: Theorem {.smaller}\n\n\n- `Proof` for $\\tilde{\\mu}_{1}^{\\text{dr}} = \\mathbb{E}[Y_i(1)]$:\n\n$$\n\\begin{align*}\n\\tilde{\\mu}_{1}^{\\text{dr}} - \\mathbb{E}[Y_i(1)] &= \\mathbb{E}\\left[\\frac{T_i(Y_i(1) - \\mu_1(X_i, \\beta_1))}{PS(\\mathbf{X_i, \\beta_{PS}})} - (Y_i(1) - \\mu_1(\\mathbf{X_i,\\beta_1}))\\right] \\\\\n&\\text{(by definition)} \\\\\n&= \\mathbb{E}\\left[\\frac{T_i - PS(\\mathbf{X_i, \\beta_{PS}})}{PS(\\mathbf{X_i, \\beta_{PS}})}(Y_i(1) - \\mu_1(\\mathbf{X_i,\\beta_1}))\\right] \\\\\n&\\text{(combining terms)} \\\\\n&= \\mathbb{E}\\left(\\mathbb{E}\\left[\\frac{T_i - PS(\\mathbf{X_i, \\beta_{PS}})}{PS(\\mathbf{X_i, \\beta_{PS}})}(Y_i(1) - \\mu_1(\\mathbf{X_i,\\beta_1})) \\bigg| \\mathbf{X_i}\\right]\\right) \\\\\n&\\text{(law of iterated expectations)} \\\\\n&= \\mathbb{E}\\left(\\mathbb{E}\\left[\\frac{T_i - PS(\\mathbf{X_i, \\beta_{PS}})}{PS(\\mathbf{X_i, \\beta_{PS}})} \\bigg| \\mathbf{X_i}\\right] \\cdot \\mathbb{E}\\left[Y_i(1) - \\mu_1(\\mathbf{X_i,\\beta_1})) \\bigg| \\mathbf{X_i}\\right]\\right) \\\\\n&\\text{(ignorability allows to separate expectations)} \\\\\n&= \\mathbb{E}\\left(\\frac{PS(\\mathbf{X_i}) - PS(\\mathbf{X_i, \\beta_{PS}})}{PS(\\mathbf{X_i, \\beta_{PS}})}  \\cdot \\left(\\mu_1(\\mathbf{X_i}) - \\mu_1(\\mathbf{X_i,\\beta_1})) \\right)\\right) \\\\\n\\end{align*}\n$$\n\n\n\n## Doubly Robust Estimator: Sample Version\n\n- `Step 1`: obtain the fitted values of the propensity scores: \n  - $PS(\\mathbf{X_i, \\hat{\\beta}_{PS}})$.\n- `Step 2`: obtain the fitted values of the outcome regressions: \n  - $\\mu_1(\\mathbf{X_i,\\hat{\\beta_1}})$ and $\\mu_0(\\mathbf{X_i,\\hat{\\beta_0}})$.\n- `Step 3`: construct the doubly robust estimator: \n  - $\\hat{\\tau}_{\\text{ATE}}^{\\text{dr}} = \\hat{\\mu}_{1}^{\\text{dr}} - \\hat{\\mu}_{0}^{\\text{dr}}$ with (definition of augmented outcome model)\n\n  - $\\hat{\\mu}_{1}^{\\text{dr}} = \\frac{1}{n} \\sum_{i=1}^n \\left[\\frac{T_i(Y_i - \\mu_1(X_i, \\hat{\\beta_1)})}{PS(\\mathbf{X_i, \\hat{\\beta}_{PS}})} + \\mu_1(\\mathbf{X_i,\\hat{\\beta_1}})\\right]$\n  \n  - $\\hat{\\mu}_{0}^{\\text{dr}} = \\frac{1}{n} \\sum_{i=1}^n \\left[\\frac{(1-T_i)(Y_i - \\mu_0(X_i, \\hat{\\beta_0)})}{1- PS(\\mathbf{X_i, \\hat{\\beta}_{PS}})} + \\mu_0(\\mathbf{X_i,\\hat{\\beta_0}})\\right]$\n\n\n## Doubly Robust Estimator: Sample Version\n\n- By definition of the augmented IPW estimator, we can also write:\n  - $\\hat{\\mu}_{1}^{\\text{dr}} = \\mathbb{E}\\left[\\frac{T_iY_i}{PS(\\mathbf{X_i, \\hat{\\beta}_{PS}})} - \\frac{T_i - PS(\\mathbf{X_i, \\hat{\\beta_{PS}}})}{PS(\\mathbf{X_i, \\hat{\\beta_{PS}}})}\\mu_1(\\mathbf{X_i,\\hat{\\beta_1}})\\right]$\n  - $\\hat{\\mu}_{0}^{\\text{dr}} = \\mathbb{E}\\left[\\frac{(1-T_i)Y_i}{1-PS(\\mathbf{X_i, \\hat{\\beta_{PS}}})} - \\frac{PS(\\mathbf{X_i, \\hat{\\beta_{PS}}}) - T_i}{1-PS(\\mathbf{X_i, \\hat{\\beta_{PS}}})}\\mu_0(\\mathbf{X_i,\\hat{\\beta_0}})\\right]$\n\n\n::: {.fragment}\n\n- Finally, in augmentation perspective:\n  - $\\hat{\\tau}_{\\text{ATE}}^{\\text{dr}} = \\hat{\\tau}_{\\text{ATE}}^{\\text{reg}} + \\frac{1}{n} \\sum_{i=1}^n \\frac{T_i(Y_i - \\mu_1(X_i, \\hat{\\beta_1)})}{PS(\\mathbf{X_i, \\hat{\\beta}_{PS}})} - \\frac{1}{n} \\sum_{i=1}^n \\frac{(1-T_i)(Y_i - \\mu_0(X_i, \\hat{\\beta_0)})}{1 - PS(\\mathbf{X_i, \\hat{\\beta}_{PS}})}$\n  - $\\hat{\\tau}_{\\text{ATE}}^{\\text{dr}} = \\hat{\\tau}_{\\text{ATE}}^{\\text{ipw}} - \\frac{1}{n} \\sum_{i=1}^n \\frac{T_i - PS(\\mathbf{X_i, \\hat{\\beta_{PS}}})}{PS(\\mathbf{X_i, \\hat{\\beta_{PS}}})}\\mu_1(\\mathbf{X_i,\\hat{\\beta_1}}) + \\frac{1}{n} \\sum_{i=1}^n \\frac{PS(\\mathbf{X_i, \\hat{\\beta_{PS}}}) - T_i}{1-PS(\\mathbf{X_i, \\hat{\\beta_{PS}}})}\\mu_0(\\mathbf{X_i,\\hat{\\beta_0}})$\n\n:::\n\n\n## Doubly Robust Estimator: Example {.smaller}\n\n- Assess the impact of participating in the U.S. National Supported Work (NSW) training program targeted to 445 individuals with social and economic problems on their real earnings. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(Matching)                       # load Matching package\nlibrary(drgee)                          # load drgee package\nT = treat                              # define treatment (training)\nY = re78                                # define outcome \nX = cbind(age,educ,nodegr,married,black,hisp,re74,re75,u74,u75) # covariates\ndr = drgee(oformula = formula(Y ~ X), eformula = formula(T ~ X), elink=\"logit\") # DR reg\nsummary(dr)                             # show results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  drgee(oformula = formula(Y ~ X), eformula = formula(T ~ X), elink = \"logit\")\n\nOutcome:  Y \n\nExposure:  T \n\nCovariates:  Xage,Xeduc,Xnodegr,Xmarried,Xblack,Xhisp,Xre74,Xre75,Xu74,Xu75 \n\nMain model:  Y ~ T \n\nOutcome nuisance model:  Y ~ Xage + Xeduc + Xnodegr + Xmarried + Xblack + Xhisp + Xre74 + Xre75 + Xu74 + Xu75 \n\nOutcome link function:  identity \n\nExposure nuisance model:  T ~ Xage + Xeduc + Xnodegr + Xmarried + Xblack + Xhisp + Xre74 + Xre75 + Xu74 + Xu75 \n\nExposure link function:  logit \n\n  Estimate Std. Error z value Pr(>|z|)  \nT   1674.1      672.4    2.49   0.0128 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Note: The estimated parameters quantify the conditional\nexposure-outcome association, given the covariates\nincluded in the nuisance models)\n\n 445  complete observations used\n```\n\n\n:::\n:::\n# {.center .center-x data-state=\"hide-menubar\"}\n<style>\n.table-text {\n  text-align: center !important;\n  font-size: 2em;\n  padding-bottom: 1em !important;\n}\n.row-bottom {\n  text-align: center !important;\n  padding-top: 2em !important;\n}\n</style>\n<table width = \"100%\">\n<tbody>\n  <tr>\n    <td colspan=\"2\" class=\"table-text\">Thank you for your attention!</td>\n  </tr>\n  <tr >\n    <td class=\"row-bottom\" style=\"vertical-align: baseline; width: 50%;\">\n          <img src=\"https://raw.githubusercontent.com/christophihl/TIE_website/main/assets/images/logo.svg\" style=\"vertical-align: middle; width: 60%; height: auto;\" />\n          <img src=\"author_pic.jpg\" class=\"picture\" style=\"vertical-align: middle; horizontal-align: right; width: 30%; height: auto;\" />\n    </td>\n    <td class = \"row-bottom\" style = \"vertical-align: middle; width = 50%\">\n      <ul style = \"list-style:none;\">\n        <li><a href = \"https://www.startupengineer.io/authors/ihl/\" target = \"_blank\"><i class = \"fas fa-home\"></i> startupengineer.io/authors/ihl</a></li>\n        <li><a href = \"https://www.linkedin.com/in/christoph-ihl/\" target = \"_blank\"><i class = \"fab fa-linkedin\"></i> christoph-ihl</a></li>\n        <li><a href = \"https://github.com/christophihl\" target = \"_blank\"><i class = \"fab fa-github\"></i> christophihl</a></li>\n        <li><a href = \"https://twitter.com/Ihluminate/\" target = \"_blank\"><i class = \"fab fa-twitter\"></i> Ihluminate</a></li>\n      </ul>\n    </td>\n  </tr>\n</tbody>\n</table>\n\n",
    "supporting": [
      "04_ob_conf_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}