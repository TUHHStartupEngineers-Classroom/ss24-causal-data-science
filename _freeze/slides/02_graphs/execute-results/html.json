{
  "hash": "cc7051853cb5f5fee5a3f965098f0577",
  "result": {
    "engine": "knitr",
    "markdown": "---\n# TITLE & AUTHOR\ntitle: \"(2) Graphical Causal Models\"\nsubtitle: \"Causal Data Science for Business Analytics\"\nauthor: \"Christoph Ihl\"\ninstitute: \"Hamburg University of Technology\"\ndate: today\ndate-format: \"dddd, D. MMMM YYYY\"\n# FORMAT OPTIONS\nformat: \n  revealjs:\n    width: 1600\n    height: 900\n    footer: \"Causal Data Science: (2) Graphical Causal Models\"\n    slide-number: true\n---\n\n\n\n# Causal Graphs {data-stack-name=\"Causal Graphs\"}\n\n\n\n\n\n::: {.cell}\n<style type=\"text/css\">\ndiv.callout-note{border-left-color:#00C1D4 !important}div.callout-note.callout-style-default .callout-title{background-color:#005e73;color:white}.callout.callout-style-default{border-left:solid #005e73 .3rem;border-right:solid 1px #005e73;border-top:solid 1px #005e73;border-bottom:solid 1px #005e73}\n</style>\n:::\n\n\n\n## Graphs\n\n::: {.nonincremental}\n- Graph theory provides a useful mathematical language to think about causality.\n- A graph consists of *vertices* (or nodes) V and *edges* (or links) E. Vertices represent variables in the model and edges the connections between them.\n- Edges can either be *undirected* or *directed*.\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell .pic40}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n<style type=\"text/css\">\n.reveal .pic40 img {\n  height: 40vh;\n  margin-left: auto;\n  margin-right: auto;\n  width: auto;\n  display: block;\n}\n.reveal .pic20 img {\n  height: 20vh;\n  margin-left: auto;\n  margin-right: auto;\n  width: auto;\n  display: block;\n}\n.reveal .pic10 img {\n  height: 10vh;\n  margin-left: auto;\n  margin-right: auto;\n  width: auto;\n  display: block;\n}\n</style>\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell .pic40}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n::::\n\n\n## Directed Graphs {.smaller}\n\n::: {.nonincremental}\n- Causal relationships are generally seen as asymmetric:\n  - If 'A causes B' is true, then 'B causes A' must be false.\n  - Therefore we'll work with *directed graphs* most of the times.\n- We'll sometimes use terminology of kinship:\n  - A is parent of B.\n  - B is child of A.\n  - A is ancestor of D.\n  - D is descendant of A.\n- A *path* is a sequence of edges connecting two vertices:\n  - $B \\gets A \\to C \\to D$ is a path from B to D.\n  - A path can go either along or against arrowheads.\n  - A path along the arrows is called *directed*: $A \\to C \\to D$.\n\n:::\n\n\n## Directed Acyclic Graphs (DAGs) {.smaller}\n\n::: {.nonincremental}\n- A directed path from a node to itself is called `directed cycle` or `feedback loop`: $B \\to C \\to D \\to B$.\n- Graph with feedback loops is called cyclic, with no feedback loops acyclic.\n- We focus on *directed acyclic graphs* (DAGs) in this course:\n  - exclude variables that influence themselves.\n  - Econometricians speak of *recursive* models that can be given a causal interpretation.\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell .pic40}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell .pic40}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n::::\n\n\n## Bayesian Networks {.smaller}\n\n- Probabilistic graphical models (not causal):\n  - Modelling the joint data distribution by factorizing with the chain rule of probability: $P(x_1, x_2, \\ldots, x_n) = P(x_1) \\prod_{i} P(x_i \\mid x_{i-1}, \\ldots, x_1)$\n  - n = 4: $P(x_1, x_2, x_3, x_4) = P(x_1)P(x_2 \\mid x_1)P(x_3 \\mid x_2, x_1)P(x_4 \\mid x_3, x_2, x_1)$\n  - $P(x_4 \\mid x_3, x_2, x_1)$ alone requires $2^3 - 1 = 8$ parameters = > Focus on local dependencies:\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell .pic40}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n$P_{joint} = P(x_1)P(x_2 \\mid x_1)P(x_3 \\mid x_2, x_1)P(x_4 \\mid x_3)$\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell .pic40}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n$P_{joint} = P(x_1)P(x_2)P(x_3 \\mid x_1)P(x_4 \\mid x_3)$\n\n\n:::\n\n::::\n\n\n## Bayesian Networks: Assumptions {.smaller}\n\nGiven a probability distribution and a corresponding DAG, we can formalize the specification of local (in-) dependencies with:\n\n::: {.callout-note icon=false}\n\n## Assumption 2.1: \"Local Markov Assumption\"\n\nGiven its parents in the DAG, a node X is independent of all its non-descendants.\n\n:::\n\nIt follows:\n\n::: {.callout-note icon=false}\n\n## Definition 2.1: \"Bayesian Network Factorization\"\n\nGiven a probability distribution $P$ and a DAG $G$, $P$ factorizes according to $G$ if: <br>\n$P(x_1, x_2, \\ldots, x_n) = \\prod_{i} P(x_i \\mid pa_i)$ <br>\nwith $pa_i$ denoting the parents of node $i$ in $G$.\n\n:::\n\nThen $P$ and $G$ are called `Markov compatible`.\n\n\n::: {.callout-note icon=false}\n\n## Assumption 2.2: \"Minimality Assumption\"\n\n1. Given its parents in the DAG, a node - is independent of all its non-descendants.\n2. Adjacent nodes in the DAG are dependent.\n\n:::\n\n\n## Causal Graph: Assumption \n\nWe need a further assumption to go from associations to causal relationships in a DAG:\n\n::: {.callout-note icon=false}\n\n## Definition 2.2: \"What is a cause?\"\n\nA variable X is said to be a cause of a variable Y if Y can change in response to changes in X.\n\n:::\n\nAn outcome variable Y `listens` to X.\n\n\n::: {.callout-note icon=false}\n\n## Assumption 2.3: \"(Strict) Causal Edge Assumption\"\n\nIn a directed graph, every parent is a direct cause of all its children.\n\n:::\n\nThis assumption is “strict” in the sense that every edge is `active`, just like in DAGs that satisfy minimality.\n\n\n\n\n# Graph Building Blocks {data-stack-name=\"Building Blocks\"}\n\n\n\n## Graph Building Blocks\n\n- Understanding the flow of association and causation in DAGs based on minimal building blocks:\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n- `Two unconnected nodes:` $P(x_1, x_2) = P(x_1) P(x_2)$\n\n\n::: {.cell .pic10}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n- `Two connected nodes:` $P(x_1, x_2) = P(x_1) P(x_2 \\mid x_1)$\n\n\n::: {.cell .pic10}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n:::\n\n:::\n\n\n::: {.fragment .fade-in}\n\n\n\n::: {.columns}\n\n\n::: {.column width=\"33%\"}\n\n- `Chain:`\n\n\n::: {.cell .pic10}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"33%\"}\n\n- `Fork:`\n\n\n::: {.cell .pic20}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"33%\"}\n\n- `Immorality:`\n\n\n::: {.cell .pic20}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n\n::::\n\n\n:::\n\n\n\n## Chains\n\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n- $x_1$ and $x_3$ are `associated` through $x_2$\n  - flow of association is symmetric whereas the flow of causality is directed\n- `\"Local Markov Assumption\"`: we can block the associative path by conditioning on the parent $x_2$\n  - $x_1 \\perp\\!\\!\\!\\perp x_3 | x_2$\n  - => $P(x_1, x_3 | x_2) = P(x_1 | x_2) P(x_3 | x_2)$\n  - Proof?\n\n\n## Chains: Proof {.smaller}\n- `\"Bayesian network factorization\"` of chains:\n  - $P(x_1, x_2, x_3) = P(x_1) P(x_2 | x_1) P(x_3 | x_2)$\n- `\"Bayes' rule\"`:\n  - $P(x_1, x_3 | x_2) = \\frac{P(x_1, x_2, x_3)}{P(x_2)}$\n- So that:\n  - $P(x_1, x_3 | x_2) = \\frac{P(x_1) P(x_2 | x_1) P(x_3 | x_2)}{P(x_2)}$\n- `\"Bayes' rule\"` twice more:\n  - $P(x_2 | x_1) = \\frac{P(x_1, x_2)}{P(x_1)}$ and $P(x_1 | x_2) = \\frac{P(x_1, x_2)}{P(x_2)}$ \n- So that we finally obtain q.e.d.:\n  - $P(x_1, x_3 | x_2) = \\frac{P(x_1, x_2)}{P(x_2)} P(x_3 | x_2) = P(x_1 | x_2) P(x_3 | x_2)$\n\n\n\n## Forks {.smaller}\n\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n:::\n\n:::\n\n\n\n- $x_1$ and $x_3$ are `associated` through $x_2$ as common cause or `confounder`\n- `\"Local Markov Assumption\"`: we can block the associative path by conditioning on parent $x_2$\n  - $x_1 \\perp\\!\\!\\!\\perp x_3 | x_2$\n  - => $P(x_1, x_3 | x_2) = P(x_1 | x_2) P(x_3 | x_2)$\n- Proof? Do try this sh.. at home!\n\n\n\n\n## Immoralities and Colliders {.smaller}\n\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n:::\n\n:::\n\n- no association in the first place: $x_1 \\perp\\!\\!\\!\\perp x_3$\n  - no common cause (\"confounder\" like in a fork)\n  - neither is $x_3$ a descendant of $x_1$ (like in a chain)\n  - $x_1$ and $x_3$ are *unrelated* things contributing to $x_2$\n  - $x_2$ acts as a `\"collider\"` that blocks the path between $x_1$ and $x_3$\n    - but only if we do **not** condition on $x_2$\n\n\n\n## Immoralities and Colliders: Proof\n- `\"Bayesian network factorization\"` of immoralities:\n  - $P(x_1, x_2, x_3) = P(x_1)P(x_3)P(x_2 \\mid x_1, x_3)$\n- Marginalizing out $x_2$ (assuming discrete variables): \n  - $P(x_1, x_3) = \\sum_{x_2}P(x_1)P(x_3)P(x_2 \\mid x_1, x_3) = P(x_1)P(x_3) \\sum_{x_2}P(x_2 \\mid x_1, x_3)$\n- Since summing over all possible values of the conditional probability $P(x_2 \\mid x_1, x_3)$ equals 1, we obtain q.e.d.:\n  - $P(x_1, x_3) = P(x_1)P(x_3)$\n\n\n## Immoralities and Colliders: Example\n\n- `Looks` and `talent` are independent of each other in the general population\n  - but both contribute to success (e.g. being casted as an actor, getting funding as founder, being in a relationship)\n  - in a selected sample of (un-) successful actors, looks and talent become negatively associated\n  - conditioning on success (by selecting a subsample) creates a `selection bias` (or `Berkson's paradox`)\n\n\n## Immoralities and Colliders: Numerical Example {.smaller}\n\n\n- `Data generating process (dgp)`: $x_1 \\sim N(0, 1), \\quad x_3 \\sim N(0,1), \\quad x_2 = x_1 + x_3$\n\n\n\n- Covariance in the population:\n\n\\begin{align*}\n\\text{Cov}(x_1, x_3) &= \\mathbb{E}[(x_1 - \\mathbb{E}[x_1])(x_3 - \\mathbb{E}[x_3])] \\\\\n&= \\mathbb{E}[x_1x_3] \\quad (\\text{zero mean})\\\\\n&= \\mathbb{E}[x_1]\\mathbb{E}[x_3] \\quad (\\text{independent}) \\\\\n&= 0\n\\end{align*}\n\n::: {.fragment .fade-in}\n\n- Conditional covariance is the expected value of the product $x_1x_3$, conditioned on $x_2$ being equal to some value $x$:\n\n\\begin{align*}\n\\text{Cov}(x_1, x_3 | x_2 = x) \n&= \\mathbb{E}[x_1x_3 | x_2 = x] \\\\\n&= \\mathbb{E}[x_1(x - x_1)] \\quad (\\text{substituting x_3 by x - x_1 as per dgp}) \\\\\n&= x\\mathbb{E}[x_1] - \\mathbb{E}[x_1^2] \\quad (\\text{x is constant and expectations linear}) \\\\\n&= -1 \\quad (\\text{E(x_1) = 0 and E(x_1*x_1) = Var(x_1) = 1})\n\\end{align*}\n\n:::\n\n\n## Immoralities and Colliders: Numerical Example\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code\"}\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(ggpubr) \n\n# simulate data\nset.seed(123) # for reproducibility\nlooks <- rnorm(1000)\ntalent <- rnorm(1000)\nx <- talent + looks\ngroup <- 1 * (x > quantile(x, c(.75)))\n\n# create a dataframe\ndf <- data.frame(looks, talent, group) %>%\n  mutate(group = if_else(group == 1, \"With Job\", \"Without Job\")) %>%\n  add_row(looks = Inf, talent = -Inf, group = \"Overall\")\n\n# plot\nggplot(df, aes(x = looks, y = talent)) +\n  geom_point(aes(color = group)) + \n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, aes(color = \"Overall\")) + # Regression line for all data\n  geom_smooth(data = subset(df, group == \"With Job\"), method = \"lm\", se = FALSE, formula = y ~ x, aes(color = \"With Job\")) +\n  geom_smooth(data = subset(df, group == \"Without Job\"), method = \"lm\", se = FALSE, formula = y ~ x, aes(color = \"Without Job\")) +\n  stat_regline_equation(aes(label = ..eq.label.., color = as.factor(group)), formula = y ~ x) +\n  stat_regline_equation(aes(label = ..eq.label.., color = \"Overall\"), formula = y ~ x) +\n  labs(color = \"Group\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n## Descendants of Colliders\n\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n:::\n\n:::\n\n# d-Separation {data-stack-name=\"d-Separation\"}\n\n## d-Separation\n\n- So far we only looked at graphs containing three variables. Can we somehow generalize these criteria?\n\n::: {.callout-note icon=false}\n\n## Definition 2.3: \"Blocked Path\"\n\nA path $p$ between nodes $X$ and $Y$ is blocked by a (potentially empty) conditioning set $Z$ if either of the following is true:\n\n1. $p$ contains a chain of nodes $... \\rightarrow W \\rightarrow ...$ or a fork $... \\leftarrow W \\rightarrow ...$, and $W$ is conditioned, i.e. $W \\in Z$.\n\n2. $p$ contains an immorality $... \\rightarrow W \\leftarrow ...$, and the collider $W$ is **not** conditioned, i.e. $W \\notin Z$.\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## Definition 2.4: \"d-Separation\"\n\nTwo nodes $X$ and $Y$ are `d-separated` by a set of nodes $Z$ if all of the paths between $X$ and $Y$ are blocked by $Z$.\n\n:::\n\n\n## d-Separation\n\n- If two nodes are d-separated, and not d-connected, the variables they represent are independent.\n\n::: {.callout-note icon=false}\n\n## Theorem 2: \"Global Markov Assumption\"\n\nGiven that $P$ is Markov compatible with respect to $G$ (satisfies the local Markov assumption), if $X$ and $Y$ are `d-separated` in $G$ conditioned on $Z$, then $X$ and $Y$ are independent in $P$ conditioned on $Z$.\n\nFormally, $X \\perp\\!\\!\\!\\perp_{G} Y \\,|\\, Z \\implies X \\perp\\!\\!\\!\\perp_{P} Y \\,|\\, Z$.\n\n:::\n\n\n\n## d-Separation Practice 1 {.smaller}\n\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code\"}\nlibrary(ggdag)\nlibrary(ggplot2)\ndag <- dagify(\n  # relationship for each node\n  W ~ Z,\n  W ~ X,\n  Y ~ X,\n  U ~ W,\n # Location of each node\n  coords = list(\n    x = c(Z = 0, W = 1, X = 2, Y = 3, U = 1),\n    y = c(Z = 0, W = -0.5, X = 0, Y = 0, U = -1)\n  )\n)\n\ndag %>%\n  ggplot(aes(x = x, y = y,\n             xend = xend, yend = yend)) +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges() +\n  geom_dag_point(shape = 1) +\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n- $Z$ and $Y$ `d-separated` conditional on\n  1. $\\emptyset$ ? 2. $\\{W\\}$ ? 3. $\\{U\\}$ ? 4. $\\{W, X\\}$ ?\n  \n\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.fragment .fade-in}\n\n\n- 1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dagitty)\ndag <- dagify(\n  W ~ Z,\n  W ~ X,\n  Y ~ X,\n  U ~ W\n)\ndseparated(dag, X=\"Z\", Y=\"Y\", Z = c())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n- 2:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n- 3:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n- 4:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n::: \n\n:::\n\n::: \n\n\n## d-Separation Practice 2 {.smaller}\n\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code\"}\nlibrary(ggdag)\nlibrary(ggplot2)\ndag <- dagify(\n  Y ~ M2 + X3 + W3,\n  W3 ~ W2,\n  W1 ~ W2,\n  T ~ W1,\n  M1 ~ T,\n  M2 ~ M1,\n  X1 ~ T,\n  X3 ~ Y,\n  X2 ~ X1 + X3,\n  coords = list(\n    x = c(T = 0, W1 = 1, W2 = 1.5, W3 = 2, M1 = 1, M2 = 2, Y = 3, X1 = 1, X2 = 1.5, X3 = 2),\n    y = c(T = -1, W1 = 0, W2 = 0.5, W3 = 0, M1 = -1, M2 = -1, Y = -1, X1 = -2, X2 = -2.5, X3 = -2)\n  )\n)\n\n\ndag %>%\n  ggplot(aes(x = x, y = y,\n             xend = xend, yend = yend)) +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges() +\n  geom_dag_point(shape = 1) +\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n- $T$ and $Y$ `d-separated` conditional on\n  1. $\\emptyset$ ?        2. $\\{W_2\\}$ ?                3. $\\{W_2, M_1\\}$ ?\n  4. $\\{W_1, M_2\\}$ ?     5. $\\{W_1, M_2, X_2 \\}$ ?     6. $\\{W_1, M_2, X_2, X_3 \\}$ ?\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.fragment .fade-in}\n\n\n- 1:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n\n- 2:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n- 3:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n- 4:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n- 5:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n- 6:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n::: \n\n\n## Flow of Association and Causation -  Summary {.smaller}\n\n- Total association between two variables flows along all unblocked paths in a causal graph.\n  - Association that flows along directed, unblocked paths is <span style=\"color:#FF7E15\">causal association</span>.\n  - The remaining association is non-causal association, e.g. `selection bias` or `confounding association`.\n  - Causal association is asymmetric, non-causal association is symmetric.\n  - Causal association is a subcategory of total association.\n- d-separation can imply \"Association is Causation` \n  - Ignoring the causal paths, are X and Y d-separated otherwise?\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-36-1.png){width=960}\n:::\n:::\n\n\n\n# do-Operator {data-stack-name=\"do-Operator\"}\n\n## Structural Causal Models {.smaller}\n\n\n\n::: {.columns}\n\n\n::: {.column width=\"40%\"}\n\n- A DAG represents an underlying structural causal model:\n- $f_i$'s can be arbitrary, non-parametric functions\n  - as opposed to structural equation models (SEM) in econometrics\n- $\\epsilon_i$'s are unobserved error terms\n  - Markovian model: all errors are assumed to be jointly independent and hence not shown in the graph.\n  - semi-Markovian model: some errors are correlated and shown in the graph; e.g. $u$ in the example.\n:::\n\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-37-1.png){width=960}\n:::\n:::\n\n\n\n- $Y = f_1(T, \\epsilon_1)$\n- $T = f_2(X_1, X_2, \\epsilon_2)$\n- $X_1 = f_3(u, \\epsilon_3)$\n- $X_2 = f_4(u, \\epsilon_4)$\n\n:::\n\n\n::: \n\n\n\n\n## Conditioning vs Intervention \n\n\n![](_images/2/condition.png){fig-align=\"center\" height=600}\n\n::: {.source}\nNeal, Brady (2020). Introduction to causal inference from a Machine Learning Perspective. Course Lecture Notes (draft).\n:::\n\n\n## Interventions and the do-Operator {.smaller}\n\n- Interventions in causal models are defined by the `do-operator`. \n- Notation: $P(Y|do(T = t))$ stands for:\n  - \"probability distribution of $Y$ if we fix $T$ to the specific value $t$\".\n  - Interventional distributions are not the same as conditional or observational distributions.\n- We can also write the `ATE` with it: $$\\text{ATE} = \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)] = \\mathbb{E}[Y|do(T = 1)] - \\mathbb{E}[Y|do(T = 0)]$$\n- Many (if not most) questions we try to answer with data involve some form of intervention, treatment, or action:\n  - P(Performance | do(Training))\n  - P(Sales | do(Incentive))\n  - P(Click-through Rate | do(Advertising))\n  - P(Churn | do(Call Center))\n\n\n## Interventions and the do-Operator {.smaller}\n- In graphical models, intervening on a variable X is similar to a kind of surgery in which we remove all edges into that variable:\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n- Pre-Intervention\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-38-1.png){width=960}\n:::\n:::\n\n\n- $Y = f_y(T, X, \\epsilon_y)$\n- $T = f_2(X, \\epsilon_T)$\n- $X = f_3(\\epsilon_X)$\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n- Post-Intervention\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-39-1.png){width=960}\n:::\n:::\n\n\n\n- $Y = f_y(T, X, \\epsilon_y)$\n- $T = t$\n- $X = f_3(\\epsilon_X)$\n\n:::\n\n:::\n\n## Interventions and the do-Operator {.smaller}\n\n- Bayesian network factorization of the pre-intervention DAG:\n  - $P(Y, T, X) = P(X)P(T|X)P(Y|T,X)$\n- If we intervene on $T$ and set it to $t$, the factorization changes:\n  - $P(Y, X | do(T = t)) = P(X)P(Y|T = t, X)$\n- Marginalizing out $X$ gives the interventional distribution of $Y$:\n  - $P(Y | do(T = t)) = \\sum_{x} P(Y|T = t, X = x)P(X = x)$\n- To obtain the causal effect, we condition on the values of $X$ and average over the distribution.\n- We only obtain the associational counterpart $P(Y|T = t)$ if $P(X)$ would have to be replaced by $P(X|T = t)$.\n  - Then: $\\sum_{x} P(Y|T = t, X = x)P(X|T = t) = \\sum_{x} P(Y, X|T = t) = P(Y| T= t)$\n\n\n\n\n## Interventions and the do-Operator \n\n- Carrying out an intervention ourselves, in a randomized control trial, is not always feasible (too expensive, impractical, or unethical).\n- How can we then identify the effect of interventions purely from observational data?\n  - We want to know $P(Y|do(T = t))$ but all we have is data $P(Y,X,T)$.\n  - And we know that $P(Y|do(T = t) \\neq P(Y|T))$ (i.e. \"correlation is not causation\").\n  - No fancy machine learning algorithm will ever (?) solve this problem.\n- One way is to find a way to transform $P(Y|do(T = t))$ into an expression that only contains observed, \"do-free\" quantities.\n\n\n# Backdoor Adjustment {data-stack-name=\"Backdoor Adjustment\"}\n\n## Backdoor Adjustment\n\n- The backdoor criterion is a graphical condition that allows us to identify causal effects from observational data.\n\n::: {.callout-note icon=false}\n\n## Definition 2.5: \"Backdoor Criterion\"\n\nA set of variables $W$ satisfies the backdoor criterion relative to $T$ and $Y$ if the following are true:\n\n1. $W$ blocks all backdoor paths between $T$ and $Y$ that contains an arrow into $T$.\n\n2. $W$ does not contain any descendants of $T$.\n\n:::\n\n- If a set of variables $W$ satisfies the backdoor criterion for $T$ and $Y$, then the causal effect is given by: $P(Y|\\text{do}(T = t)) = \\sum_{W} P(Y|T = t, W = w)P(W = w)$.\n  - i.e. condition on the values of $W$ and average over their joint distribution\n\n\n## Backdoor Adjustment: Proof\n\n- Conditioning on the variables $W$ and marginalizing them out:\n  - $P(Y|\\text{do}(T = t)) = \\sum_{W} P(Y|\\text{do}(T = t), W = w)P(W| \\text{do}(T = t))$\n  \n- Get rid of the first \"do\" by using the definition of the do-operator - \"all backdoor paths blocked\":\n  - $\\sum_{W} P(Y|\\text{do}(T = t), W = w)P(W| \\text{do}(T = t)) = \\sum_{W} P(Y|T = t, W = w)P(W| \\text{do}(T = t))$\n\n- Get rid of the second \"do\" by using the definition of the do-operator - \"no descendants of T in W\":\n  - $\\sum_{W} P(Y|T = t, W = w)P(W| \\text{do}(T = t)) = \\sum_{W} P(Y|T = t, W = w)P(W)$\n\n\n## Backdoor Adjustment: Example\n\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code\"}\nlibrary(ggdag)\nlibrary(ggplot2)\ndag <- dagify(\n  T ~ X1 + X2,\n  X6 ~ T,\n  X2 ~ X3,\n  X1 ~ X3 + X4,\n  X5 ~ X4,\n  Y ~ X1 + X5 + X6,\n  exposure = \"T\", outcome = \"Y\",\n  coords = list(\n    x = c(T = 0, X1 = 1, X2 = 0, X3 = 0, X4 = 2, X5 = 2, X6 = 1, Y = 2),\n    y = c(T = 0, X1 = 1, X2 = 1, X3 = 2, X4 = 2, X5 = 1, X6 = 0, Y = 0)\n  )\n)\n\ndag %>%\n  ggplot(aes(x = x, y = y,\n             xend = xend, yend = yend)) +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges() +\n  geom_dag_point(shape = 1) +\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![](02_graphs_files/figure-revealjs/unnamed-chunk-40-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n- Minimum sufficient adjustment sets?\n\n::: {.fragment .fade-in}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code\"}\nlibrary(ggdag)\nlibrary(ggplot2)\ndag <- dagify(\n  T ~ X1 + X2,\n  X6 ~ T,\n  X2 ~ X3,\n  X1 ~ X3 + X4,\n  X5 ~ X4,\n  Y ~ X1 + X5 + X6,\n  exposure = \"T\", outcome = \"Y\",\n  coords = list(\n    x = c(T = 0, X1 = 1, X2 = 0, X3 = 0, X4 = 2, X5 = 2, X6 = 1, Y = 2),\n    y = c(T = 0, X1 = 1, X2 = 1, X3 = 2, X4 = 2, X5 = 1, X6 = 0, Y = 0)\n  )\n)\n\nadjustmentSets(dag)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{ X1, X5 }\n{ X1, X4 }\n{ X1, X3 }\n{ X1, X2 }\n```\n\n\n:::\n:::\n\n:::\n\n:::\n\n:::\n\n\n## Relation to the Potential Outcomes Framework {.smaller}\n\n- ATE in the PO framework:\n  - $\\tau = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] = \\mathbb{E_X}[\\mathbb{E}[Y_i|T_i=1, X_i] - \\mathbb{E}[Y_i|T_i=0, X_i]]$\n- do-notation $\\mathbb{E}(Y|do(T = t))$ just another notation for the potential outcomes $\\mathbb{E}[Y(t)]$.\n  - Expectations and discrete treatement vs. probability weighted averages and continuous/ multi-valued treatments.\n- The backdoor criterion is a graphical condition to identify valid adjustment sets for the potential outcomes framework.\n  - But we had no way of knowing how to choose $W$ such that it gives us conditional exchangeability. \n  - The backdoor criterion is a graphical condition to choose a valid $W$.\n  - It is neither necessary nor suffcient to condition on all variables in the data and model.\n  - Can even be harmful to condition on a (collider) variable.\n- Once we have found an admissible adjustment set, we can estimate the causal effect by matching, inverse probability weighting, or linear regression (if you're willing to assume linearity).\n\n\n# {.center .center-x data-state=\"hide-menubar\"}\n<style>\n.table-text {\n  text-align: center !important;\n  font-size: 2em;\n  padding-bottom: 1em !important;\n}\n.row-bottom {\n  text-align: center !important;\n  padding-top: 2em !important;\n}\n</style>\n<table width = \"100%\">\n<tbody>\n  <tr>\n    <td colspan=\"2\" class=\"table-text\">Thank you for your attention!</td>\n  </tr>\n  <tr >\n    <td class=\"row-bottom\" style=\"vertical-align: baseline; width: 50%;\">\n          <img src=\"https://raw.githubusercontent.com/christophihl/TIE_website/main/assets/images/logo.svg\" style=\"vertical-align: middle; width: 60%; height: auto;\" />\n          <img src=\"author_pic.jpg\" class=\"picture\" style=\"vertical-align: middle; horizontal-align: right; width: 30%; height: auto;\" />\n    </td>\n    <td class = \"row-bottom\" style = \"vertical-align: middle; width = 50%\">\n      <ul style = \"list-style:none;\">\n        <li><a href = \"https://www.startupengineer.io/authors/ihl/\" target = \"_blank\"><i class = \"fas fa-home\"></i> startupengineer.io/authors/ihl</a></li>\n        <li><a href = \"https://www.linkedin.com/in/christoph-ihl/\" target = \"_blank\"><i class = \"fab fa-linkedin\"></i> christoph-ihl</a></li>\n        <li><a href = \"https://github.com/christophihl\" target = \"_blank\"><i class = \"fab fa-github\"></i> christophihl</a></li>\n        <li><a href = \"https://twitter.com/Ihluminate/\" target = \"_blank\"><i class = \"fab fa-twitter\"></i> Ihluminate</a></li>\n      </ul>\n    </td>\n  </tr>\n</tbody>\n</table>\n\n",
    "supporting": [
      "02_graphs_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}