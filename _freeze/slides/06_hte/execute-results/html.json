{
  "hash": "9c4cbe2b9a75b6b818ede369b9dd19ba",
  "result": {
    "engine": "knitr",
    "markdown": "---\n# TITLE & AUTHOR\ntitle: \"(6) Heterogeneous Treatment Effects\"\nsubtitle: \"Causal Data Science for Business Analytics\"\nauthor: \"Christoph Ihl\"\ninstitute: \"Hamburg University of Technology\"\ndate: today\ndate-format: \"dddd, D. MMMM YYYY\"\n# FORMAT OPTIONS\nformat: \n  revealjs:\n    width: 1600\n    height: 900\n    footer: \"Causal Data Science: (6) Heterogeneous Treatment Effects\"\n    slide-number: true\n---\n\n\n\n\n# Introduction {data-stack-name=\"Intro\"}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n<style type=\"text/css\">\ndiv.callout-note{border-left-color:#00C1D4 !important}div.callout-note.callout-style-default .callout-title{background-color:#005e73;color:white}.callout.callout-style-default{border-left:solid #005e73 .3rem;border-right:solid 1px #005e73;border-top:solid 1px #005e73;border-bottom:solid 1px #005e73}\n</style>\n:::\n\n\n\n\n\n## Treatment Effect Heterogeneity: Motivation\n\n- More comprehensive evaluation: \n  - who wins or loses and by how much?\n- This is useful along at least two dimensions:\n  - `Informs action`: \n    - More efficient allocation of public and private resources via targeting in the future:\n      - Personalized policies, ads, medicine, ...\n  - `Understanding`: \n    - Heterogeneous effects can be suggestive for underlying mechanisms\n\n\n## Treatment Effect Heterogeneity: Definition {.smaller}\n\n- Expected treatment effect in the target subpopulation with characteristics $\\mathbf{X_i}$ given by `Conditional Average Treatment Effect (CATE)`:\n  - $\\tau(\\mathbf{x}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}]$\n\n\n\n::: {.fragment}\n- $\\mathbf{X_i} = \\mathbf{H_i} \\cup \\mathbf{C_i}$\n  - $\\mathbf{H_i}$: motivated by the research question to understand specific effect heterogeneity in a pre-defined the target subpopulation.\n  - $\\mathbf{C_i}$: confounders that are required for identification.\n\n:::\n\n::: {.fragment}\n- `Randomized experiments` - no confounders:\n  - CATE defined with respect to considered heterogeneity variables: $\\mathbf{X_i} = \\mathbf{H_i}$\n:::\n\n::: {.fragment}\n- `Measured Confounding` - distinguish two types of CATEs:\n  - `Group ATE (GATE)` for  groups G defined by H: $\\tau(\\mathbf{g}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{G_i = g}]$\n  - `Individualized ATE (IATE = CATE)`: $\\tau(\\mathbf{x}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}]$\n    - most flexible/ personalized/ individualized effect prediction\n  - Estimation step is affected by whether we are interested in GATEs or IATEs.\n:::\n\n\n\n## Treatment Effect Heterogeneity: Identification\n\n- No need to establish new identification results:\n  - All target parameters can be thought of as special cases of conditioning ITE on some function $f(\\mathbf{X_i = x})$ \n  - And by the Law of Iterated Expectations (LIE):\n  \n$$\n\\begin{align*}\n\\mathbb{E}[Y_i(1) - Y_i(0) | f(\\mathbf{X_i}) = f(\\mathbf{x})] &= \\mathbb{E}[\\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}, f(\\mathbf{X_i = x}) ] | f(\\mathbf{X_i = x})] \\\\\n&= \\mathbb{E}[\\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x} ] | f(\\mathbf{X_i = x})]\n\\end{align*}\n$$\n\n::: {.fragment}\n- As $\\mathbf{X_i} = \\mathbf{H_i} \\cup \\mathbf{C_i}$ is assumed to contain all confounders, the inner expectation $\\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x} ]$ is identified in randomized experiments or under measured confounding\n- => All aggregations with respect to a function $f(\\mathbf{X_i})$ are also identified.\n\n:::\n\n\n# Group Average Treatment Effects {data-stack-name=\"Group ATEs\"}\n\n## Group ATEs: Examples\n\n- `Group ATE (GATE)`: $\\tau(\\mathbf{g}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{G_i = g}]$\n- Examples for subgroups of interest:\n  - Mutually exclusive *subgroups*, e.g.: $G = \\{\\text{female}, \\text{male}\\}$, $G = \\{ \\text{age} < 50, \\text{age} \\geq 50 \\}$, $G = \\{ \\text{age} < 50 \\space \\& \\space \\text{female}, \\text{age} \\geq 50 \\space \\& \\space \\text{female}, \\text{age} \\geq 50 \\space \\& \\space \\text{male}, \\dots  \\}$, ...\n  - Single or low-dimensional *continuous variable*, e.g.: G = age, G = income, ...\n  - Other *functions or small subsets* of $\\mathbf{X_i}$\n- Groups should be *pre-determined* and not be the result of data snooping\n\n\n## Group ATEs: Estimation\n\n- Three strategies:\n  1) Stratify the data and rerun the analysis for each subgroup.\n      - Downside: Requires a lot of data and computation, can lead to high variance estimates for small subgroups.    \n  \n  2) Specify an interaction term in an OLS regression model:\n      - $Y_i = \\beta_{0} + \\tau T_i + \\beta_{G_i} G_{i} + \\beta_{T_iG_i} T_{i} G_{i} + \\mathbf{\\beta_{X_i}X_{i}}+ \\epsilon_i$\n      - Downside: Requires a correct model specification, can be sensitive to misspecification.\n    \n  3) Double Machine Learning with AIPW model to estimate the GATEs directly.\n\n\n\n\n## Group ATEs: Double Machine Learning\n\n- Previous lecture: ATE (AIPW) can be estimated as mean of a pseudo-outcome:\n\n  - $\\hat{\\tau}_{\\text{ATE}}^{\\text{AIPW}} = \\frac{1}{N}\\sum_{i=1}^n \\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}$\n\n- Pseudo-outcome is given by:\n\n  - $\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} = \\hat{\\mu}(1, \\mathbf{X_i}) - \\hat{\\mu}(0, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\hat{\\mu}(1, \\mathbf{X_i}))}{\\hat{e}_1(\\mathbf{X_i})} - \\frac{(1-T_i)(Y_i - \\hat{\\mu}(0, \\mathbf{X_i})}{\\hat{e}_0(\\mathbf{X_i}))}$\n\n::: {.fragment}\n\n- Equivalent to a linear regression model with pseudo-outcome and constant:\n  - $\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} = \\alpha + \\epsilon_i$ with $\\hat{\\alpha} = \\hat{\\tau}_{\\text{ATE}}^{\\text{AIPW}}$\n\n- Can be extended with heterogeneity variable(s) $G_i$:\n  - $\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} = \\alpha + \\beta G_i + \\epsilon_i$\n  - => Modelling the level of the effect, not the level of the outcome.\n\n:::\n\n\n## Group ATEs: Advantages of DML\n\n- Neyman-orthogonality of $\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}$ allows to apply standard statistical inference ([Semenova and Chernozhukov, 2021](https://academic.oup.com/ectj/article-abstract/24/2/264/5899048?redirectedFrom=fulltext)).\n- Computationally less expensive than subgroup analyses\n  - Only one additional OLS, no new nuisance parameters).\n- More flexible than specifying interaction terms in a linear model, as we flexibly adjust for confounding by ML methods.\n- As $\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}$ is an unbiased signal, i.e. $\\mathbb{E}[\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} |Â G_i = g] = \\tau(g)$, to regress the pseudo-outcome $\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}$ on low-dimensional $G_i$ we can either use\n  - OLS or series regression ([Semenova and Chernozhukov, 2021](https://academic.oup.com/ectj/article-abstract/24/2/264/5899048?redirectedFrom=fulltext)).\n  - Kernel regression ([Fan et al., 2022](https://www.tandfonline.com/doi/full/10.1080/07350015.2020.1811102); [Zimmert & Lechner, 2019](https://arxiv.org/abs/1908.08779)).\n\n\n\n## Group ATEs: Proof of DML {.smaller}\n\n- Proof that $\\mathbb{E}[\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} \\mid G_i = g] = \\tau(g)$:\n\n\n$$\n\\begin{align*}\n\\mathbb{E}[\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} \\mid G_i = g] &= \\mathbb{E} \\left[  \\mu(1,\\mathbf{X_i}) + \\frac{T_i(Y_i - \\mu(1,\\mathbf{X_i}))}{e(\\mathbf{X_i})} - \\mu(0,\\mathbf{X_i}) - \\frac{(1-T_i)(Y_i - \\mu(0,\\mathbf{X_i}))}{1 - e(\\mathbf{X_i})} \\bigg| G_i = g \\right] \\\\\n&\\overset{LIE}{=} \\mathbb{E} \\left[ \\underbrace{\\mathbb{E} \\left[ \\mu(1, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\mu(1, \\mathbf{X_i}))}{e(\\mathbf{X_i})} \\mid \\mathbf{X_i = x} \\right]}_{\\text{CAPO-AIPW => }\\mathbb{E}[Y_i(1) \\mid \\mathbf{X_i = x}]} - \\underbrace{\\mathbb{E} \\left[ \\mu(0, \\mathbf{X_i}) + \\frac{(1-T_i)(Y_i - \\mu(0, \\mathbf{X_i}))}{1 - e(\\mathbf{X_i})} \\mid \\mathbf{X_i = x} \\right]}_{CAPO-AIPW => \\mathbb{E}[Y(0) \\mid \\mathbf{X_i = x}]} \\bigg| G_i = g \\right] \\\\\n&= \\mathbb{E}\\left[\\mathbb{E}[Y_i(1) \\mid \\mathbf{X_i = x}] - \\mathbb{E}[Y_i(0) \\mid \\mathbf{X_i = x}] \\bigg| G_i = g\\right] \\\\\n&\\overset{LIE}{=} \\mathbb{E}[Y_i(1) - Y_i(0) \\mid G_i = g] \\\\\n&= \\tau(g)\n\\end{align*}\n$$\n\n- Law of Iterated Expectations uses that $G_i$ is a function of $X_i$.\n\n\n\n## Group ATEs: Example based on DML {.smaller}\n\n- Assess the effect of 401(k) program participation on net financial assets of 9,915 households in the US in 1991.\n- First step (not shown): Estimate $\\hat{\\tau}_{\\text{ATE}}^{\\text{AIPW}}$ using DoubleML.\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Get the indvidual ATEs (pseudo-outcomes)\ndata$ate_i <- dml_irm_forest[[\"psi_b\"]] # get numerator of score function, which is equal to pseudo outcome\nmean_ate = mean(data$ate_i) # mean of pseudo outcomes = ATE\n\nlibrary(estimatr) # for linear robust post estimation\nsummary(lm_robust(ate_i ~ hown, data = data))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimates and significance testing of the effect of target variables\n     Estimate. Std. Error t value Pr(>|t|)    \ne401      8206       1106   7.421 1.16e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm_robust(formula = ate_i ~ hown, data = data)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(>|t|) CI Lower CI Upper   DF\n(Intercept)     3477        711   4.890 1.025e-06     2083     4870 9913\nhown            7445       1835   4.058 4.990e-05     3849    11041 9913\n\nMultiple R-squared:  0.00106 ,\tAdjusted R-squared:  0.0009588 \nF-statistic: 16.47 on 1 and 9913 DF,  p-value: 4.99e-05\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(np) # for kernel post estimation\nage = data$age            \nate_i = data$ate_i                \nnp_model = npreg(ate_i ~ age)  # kernel regression\nplot(np_model)  # plot the kernel regression\n```\n:::\n\n\n![](_images/6/cate_age.png){.r-stretch fig-align=\"center\" width=750}\n\n:::\n\n\n:::\n\n\n\n# Metalearners {data-stack-name=\"Metalearners\"}\n\n\n## Predicting Individualized ATEs\n\n- Group-level heterogeneity variables were hand-picked.\n- Now predict individualized treatment effects based on all covariates $\\mathbf{X_i}$:\n  - `Individualized ATE (IATE = CATE)`: $\\tau(\\mathbf{x}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}]$\n  - Conditional expectation with unobserved outcome (counterfactuals) \n- Given the assumptions of observed confounding, we can write the CATE as:\n  - $\\tau(\\mathbf{x}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}] = \\mathbb{E}[Y_i | T_i, \\mathbf{X_i = x}] - \\mathbb{E}[Y_i | T_i, \\mathbf{X_i = x}]$\n  - which can be approximated with ML.\n\n\n## S-Learner and T-Learner\n\n- `S-learner`:\n  - 1. Use ML estimator of your choice to fit outcome model using $\\mathbf{X_i}$ AND $T_i$ in the `full sample`: $\\hat{\\mu}(T_i; \\mathbf{X_i})$.\n  - 2. Estimate CATE as $\\hat{\\tau}(\\mathbf{x}) = \\hat{\\mu}(1; \\mathbf{X_i}) - \\hat{\\mu}(0; \\mathbf{X_i})$.\n\n::: {.fragment}\n\n- `T-learner`:\n\n  - 1. Use ML estimator of your choice to fit model $\\hat{\\mu}(1; \\mathbf{X_i})$ in `treated subsample`.\n\n  - 2. Use ML estimator of your choice to fit model $\\hat{\\mu}(0; \\mathbf{X_i})$ in `control subsample`.\n\n  - 3. Estimate CATE as $\\hat{\\tau}(\\mathbf{x}) = \\hat{\\mu}(1; \\mathbf{X_i}) - \\hat{\\mu}(0; \\mathbf{X_i})$.\n\n:::\n\n\n## S-Learner and T-Learner: Example {.smaller}\n\n- Assess the effect of 401(k) program participation on net financial assets of 9,915 households in the US in 1991.\n- Examples without proper cross-fitting.\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(hdm) # for the data\nlibrary(grf) # generalized random forests, could also use mlr3\n\n\n# Get data\ndata(pension)\n# Outcome\nY = pension$net_tfa\n# Treatment\nT = pension$p401\n# Create main effects matrix\nX = model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension)\n\n# Implement the S-Learner\nTX = cbind(T,X)\nrf = regression_forest(TX,Y)\nT0X = cbind(rep(0,length(Y)),X)\nT1X = cbind(rep(1,length(Y)),X)\ncate_sl = predict(rf,T1X)$predictions - predict(rf,T0X)$predictions\nhist(cate_sl)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Implement the T-Learner\nrfmu1 = regression_forest(X[T==1,],Y[T==1])\nrfmu0 = regression_forest(X[T==0,],Y[T==0])\ncate_tl = predict(rfmu1, X)$predictions - predict(rfmu0, X)$predictions\nhist(cate_tl)\n```\n:::\n\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06_hte_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n:::\n\n\n\n## S-Learner and T-Learner: Disadvantage {.smaller}\n\n- The prediction problems `do not know of joint goal` to approximate a difference:\n  - $\\hat{\\mu}(1; \\mathbf{X_i})$ minimizes $MSE(\\hat{\\mu}(1; \\mathbf{x})) = \\mathbb{E}[(\\hat{\\mu}(1; \\mathbf{x}) - \\mu(1; \\mathbf{X_i}))^2]$.\n  - $\\hat{\\mu}(0; \\mathbf{X_i})$ minimizes $MSE(\\hat{\\mu}(0; \\mathbf{x})) = \\mathbb{E}[(\\hat{\\mu}(0; \\mathbf{x}) - \\mu(0; \\mathbf{X_i}))^2]$.\n  - BUT they `should aim to minimize`:\n  \n$$\n\\begin{align*}\n\\text{MSE}(\\hat{\\tau}(\\mathbf{x}))) &= \\mathbb{E}[(\\hat{\\tau}(\\mathbf{x})) - \\tau(\\mathbf{x})))^2] \\\\\n&= \\mathbb{E}[(\\hat{\\mu}(1, \\mathbf{x})) - \\hat{\\mu}(0, \\mathbf{x})) - (\\mu(1, \\mathbf{x})) - \\mu(0, \\mathbf{x}))))^2] \\\\\n&= \\mathbb{E}[(\\hat{\\mu}(1, \\mathbf{x})) - \\mu(1, \\mathbf{x})))^2] + \\mathbb{E}[(\\hat{\\mu}(0, \\mathbf{x})) - \\mu(0, \\mathbf{x})))^2] \\\\\n&\\quad - 2\\mathbb{E}[(\\hat{\\mu}(1, \\mathbf{x})) - \\mu(1, \\mathbf{x})))(\\hat{\\mu}(0, \\mathbf{x})) - \\mu(0, \\mathbf{x})))] \\\\\n&= \\text{MSE}(\\hat{\\mu}(1, \\mathbf{x}))) + \\text{MSE}(\\hat{\\mu}(0, \\mathbf{x}))) - 2\\text{MCE}(\\hat{\\mu}(1, \\mathbf{x})), \\hat{\\mu}(0, \\mathbf{x})))\n\\end{align*}\n$$\n\n::: {.fragment}\n\n- [Lechner (2018)](https://arxiv.org/abs/1812.09487) calls the additional term `Mean Correlated Error (MCE)`: correlated errors matter less\n- Example - both make same error: $\\hat{\\mu}(1; \\mathbf{X_i}) = \\mu(1; \\mathbf{X_i}) + 2$ and $\\hat{\\mu}(0; \\mathbf{X_i}) = \\mu(0; \\mathbf{X_i}) + 2$ \n  - But their CATE would still be on point: $MSE(\\hat{\\tau}(\\mathbf{x})) = 4 + 4 - 2(2 \\cdot 2) = 0$\n- Example - errors go in different direction: $\\hat{\\mu}(1; \\mathbf{X_i}) = \\mu(1; \\mathbf{X_i}) + 2$ and $\\hat{\\mu}(0; \\mathbf{X_i}) = \\mu(0; \\mathbf{X_i}) - 2$ \n  - But their CATE would be off: $MSE(\\hat{\\tau}(\\mathbf{x})) = 4 + 4 - 2(2 \\cdot (-2)) = 16$\n\n:::\n\n\n## Two Approaches to Improvements\n\n1. `Modify` supervised ML methods to target causal effect estimation\n    - Method specific, e.g.:\n      - Causal tree ([Athey and Imbens, 2016](https://www.pnas.org/doi/10.1073/pnas.1510489113))\n      - Causal forest ([Athey, Tibshirani & Wager, 2019](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full))\n    - Not covered here (does not scale very well to high-dimensional data)\n\n2. `Combine` supervised ML methods to target causal effect estimation\n    - Generic approach - `Metalearners`, e.g.:\n      - X-learner ([KÃ¼nzel et al., 2019](https://arxiv.org/abs/1706.03461))\n        - not covered here; handles sample imbalance, but not doubly robust\n      - R-learner\n      - DR-learner\n\n## What are Metalearners?\n\n- Metalearners **combine multiple supervised ML steps** in a pipeline that outputs predicted CATEs.\n- The common ones require the following steps:\n    1. `Estimate nuisance parameters` using suitable ML method.\n    2. Plug them into a clever `minimization problem` targeting CATE.\n    3. `Solve the minimization problem` using suitable ML method.\n    4. `Predict` CATE using the model learned in 3.\n- Most popular ML methods are suitable and can be applied in steps 1, 3 and 4.\n- Like for standard prediction methods, **statistical inference is usually not available**.\n\n\n\n## R-learner: Idea\n\n- `Partially linear model`, but now allowing for treatment effects that vary with $\\mathbf{X}$:\n  - $Y_i = \\tau(\\mathbf{X_i}) T_i + g(\\mathbf{X_i}) + \\epsilon_{Y_i}, \\quad \\mathbb{E}(\\epsilon_{Y_i} | T_i,\\mathbf{X_i}) = 0$\n  - => $\\underbrace{Y_i - \\overbrace{\\mathbb{E}[Y_i \\mid \\mathbf{X_i}]}^{\\mu(\\mathbf{X_i})}}_{\\text{outcome residual}} = \\tau(\\mathbf{X_i}) \\underbrace{( T_i - \\overbrace{\\mathbb{E}[T_i \\mid \\mathbf{X_i}]}^{e(\\mathbf{X_i})})}_{\\text{treatment residual}} + \\epsilon_{Y_i}$\n- This motivates the R-learner of [Nie and Wager, 2020](https://academic.oup.com/biomet/article-abstract/108/2/299/5911092?redirectedFrom=fulltext):\n  - $\\hat{\\tau}_{\\text{RL}}(\\mathbf{x}) = \\arg \\min_{\\tau} \\sum_{i=1}^n ( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tau(\\mathbf{X_i}) ( T_i - \\hat{e}(\\mathbf{X_i})))^2$\n  - with cross-fitted high-quality nuisance parameters from first step.\n  - But how to estimate it?\n  \n\n## R-learner with Linear ML-Methods {.smaller}\n\n- CATE as linear function $\\tau(\\mathbf{X_i}) = \\mathbf{\\beta' X_i}$:\n\n$$\n\\begin{align*}\n\\hat{\\beta}_{RL} &= \\underset{\\beta}{\\operatorname{arg\\,min}} \\sum_{i=1}^N( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\mathbf{\\beta'} \\underbrace{(T_i - \\hat{e}(\\mathbf{X_i})) \\mathbf{X_i}}_{=\\mathbf{\\tilde{X}_i}})^2 \\\\\n&= \\underset{\\beta}{\\operatorname{arg\\,min}} \\sum_{i=1}^N \\left( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\mathbf{\\beta'} \\mathbf{\\tilde{X}_i} \\right)^2\n\\end{align*}\n$$\n  \n  - $\\mathbf{\\tilde{X}_i} = (T_i - \\hat{e}(\\mathbf{X_i})) \\mathbf{X_i}$ are modified / pseudo-covariates.\n  - $\\hat{\\tau}_{\\text{RL}}(\\mathbf{x}) = \\mathbf{\\hat{\\beta}_{RL} x} \\neq \\mathbf{\\hat{\\beta}_{RL} \\tilde{x}}$ is the estimated CATE for a specific $\\mathbf{x}$.\n\n- All linear shrinkage estimators (Lasso and friends) can be applied, nuisance parameters can still be estimated with non-linear ML.\n\n\n## R-learner with Generic ML-Methods {.smaller}\n\n- If we are not willing to impose linearity of the CATE, we can rewrite the R-learner:\n\n$$\n\\begin{align*}\n\\hat{\\tau}_{\\text{RL}}(\\mathbf{x}) &= \\arg \\min_{\\tau} \\sum_{i=1}^n ( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tau(\\mathbf{X_i}) ( T_i - \\hat{e}(\\mathbf{X_i})))^2 \\\\\n&= \\arg \\min_{\\tau} \\sum_{i=1}^n \\frac{( T_i - \\hat{e}(\\mathbf{X_i}))^2}{(T_i - \\hat{e}(\\mathbf{X_i}))^2}(Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tau(\\mathbf{X_i}) ( T_i - \\hat{e}(\\mathbf{X_i})))^2 \\\\\n&= \\arg \\min_{\\tau} \\sum_{i=1}^n (T_i - \\hat{e}(\\mathbf{X_i}))^2 \\bigg(\\frac{Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tau(\\mathbf{X_i}) ( T_i - \\hat{e}(\\mathbf{X_i}))}{ T_i - \\hat{e}(\\mathbf{X_i})}\\bigg)^2 \\\\\n&= \\arg \\min_{\\tau} \\sum_{i=1}^n (T_i - \\hat{e}(\\mathbf{X_i}))^2 \\bigg(\\frac{Y_i - \\hat{\\mu}(\\mathbf{X_i})}{ T_i - \\hat{e}(\\mathbf{X_i})} - \\tau(\\mathbf{X_i})\\bigg)^2 \\\\\n\\end{align*}\n$$\n\n- Supervised ML methods that can deal with weighted minimization (e.g. neural nets, random forest, boosting, ...) with\n  - weights: $(T_i - \\hat{e}(\\mathbf{X_i}))^2$.\n  - pseudo-outcome: $\\frac{Y_i - \\hat{\\mu}(\\mathbf{X_i})}{ T_i - \\hat{e}(\\mathbf{X_i})}$.\n  - the unmodified covariates: $\\mathbf{X_i}$.\n\n\n## DR-learner\n\n- Recall the pseudo-outcome of the AIPW-ATE from previous lecture and condition on $\\mathbf{X_i}$ (same \"trick\" as for GATE estimation):\n\n  - $\\tau_{\\text{DR}}(\\mathbf{x}) = \\mathbb{E}\\bigg[ \\underbrace{\\hat{\\mu}(1, \\mathbf{X_i}) - \\hat{\\mu}(0, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\hat{\\mu}(1, \\mathbf{X_i}))}{ \\hat{e}_1(\\mathbf{X_i})} - \\frac{(1-T_i)(Y_i - \\hat{\\mu}(0, \\mathbf{X_i})}{\\hat{e}_0(\\mathbf{X_i}))}}_{\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}} \\bigg| \\mathbf{X_i= x} \\bigg]$\n  - $\\tau_{\\text{DR}}(\\mathbf{x}) = \\mathbb{E}\\bigg[ \\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} \\bigg| \\mathbf{X_i= x} \\bigg]$\n  \n- `DR-learner` by [Kennedy (2020)](https://arxiv.org/abs/2004.14497) uses $\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}$ in a generic ML problem:\n  - $\\hat{\\tau}_{RL}(\\mathbf{x}) = \\underset{\\tau}{\\operatorname{arg\\,min}} \\sum_{i=1}^N \\left( \\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} - \\tau(\\mathbf{X_i})\\right)^2$\n  - `Cross-fitting`: in 4 subsamples (1) train a model for $\\hat{e(\\mathbf{X_i})}$, (2) train a model for $\\hat{\\mu(\\mathbf{X_i})}$, (3) construct $\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}$ and regress on $\\mathbf{X_i}$, (4) predict $\\hat{\\tau}_{RL}(\\mathbf{x})$. Then rotate.\n\n\n## DR-learner: Example {.smaller}\n\n- Assess the effect of 401(k) program participation on net financial assets of 9,915 households in the US in 1991.\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(hdm) # for the data\nlibrary(causalDML) # generalized random forests, could also use mlr3\n\n\n# Get data\ndata(pension)\n# Outcome\nY = pension$net_tfa\n# Treatment\nT = pension$p401\n# Create main effects matrix\nX = model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension)\n\n# Implement the DR-Learner\ndr = dr_learner(Y,T,X,\n      ml_w = list(create_method(\"forest_grf\")),\n      ml_y = list(create_method(\"forest_grf\")),\n      ml_tau = list(create_method(\"forest_grf\"))\n)\n\n# DR-learner distribution of B-A\nhist(dr$cates[,1])\n```\n:::\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n![](_images/6/dr_cates.png){.r-stretch fig-align=\"center\" width=750}\n\n\n:::\n\n:::\n\n\n\n# HTE Evaluation {data-stack-name=\"HTE Evaluation\"}\n\n\n## How to evaluate estimated CATEs? {.smaller}\n\n(1) `Descriptive`: histogram, kernel density plots, box plots, etc. ...\n(2) `Inference`: test whether effect heterogeneity is systematic or just noise.\n(3) Explore what drives the heterogeneous effects.\n\n\n\n::: {.fragment}\n\n- Challenges with inference:\n  - Unique to causal ML: Due to missing counterfactual, we cannot benchmark predicted against effect => no classic out-of-sample testing.\n  - Shared with supervised ML: statistical inference for predicted CATE is not available or at least challenging.\n\n- Approach to inference:\n  - Rather than (consistent) estimation of & inference on the individual CATEs directly, derive summary statistics of their (noisy) distribution.\n  - Test joint hypothesis that there is effect heterogeneity & the applied estimation method is able to detect it at least partially.\n\n:::\n\n::: {.fragment}\n\n- We discuss the three methods proposed by [Chernozhukov et al. (2017-2023)](https://arxiv.org/abs/1712.04802):\n  - Best Linear Predictor (`BLP`).\n  - High-vs.-low Sorted Group Average Treatment Effect (`GATES`).\n  - Classification Analysis (`CLAN`) to explore what drives the heterogeneous effects.\n  \n:::\n\n\n\n\n## Best Linear Predictor (BLP) - Definition\n\n- BLP is defined as the solution of the `hypothetical regression of the true CATE on the demeaned predicted CATE`:\n\n::: {.callout-note icon=false}\n\n## Definition \"Best Linear Predictor (BLP)\"\n\nThe best linear predictor of $\\tau(\\mathbf{X_i})$ by $\\hat{\\tau}(\\mathbf{X_i})$ is the solution to:\n\n$(\\beta_1, \\beta_2) = \\underset{\\tilde{\\beta_1}, \\tilde{\\beta_2}}{\\operatorname{arg\\,min}} \\space \\mathbb{E} \\left[ \\left( \\tau(\\mathbf{X_i}) - \\tilde{\\beta_1} - \\tilde{\\beta_2} \\left( \\hat{\\tau}(\\mathbf{X_i}) - \\mathbb{E}[\\hat{\\tau}(\\mathbf{X_i})] \\right) \\right)^2 \\right]$\n\n- which, if exists, is defined as\n  - $\\mathbb{E}[\\tau(\\mathbf{X_i}) | \\hat{\\tau}(\\mathbf{X_i}) ] := \\beta_1 + \\beta_2\\underbrace{(\\hat{\\tau}(\\mathbf{X_i}) - \\mathbb{E}[\\hat{\\tau}(\\mathbf{X_i})])}_{\\text{demeaned prediction}}$\n\n- where\n  - $\\beta_1 = \\mathbb{E}[\\tau(\\mathbf{X_i})] = \\text{ATE} \\text{ (because of the demeaning)}$\n  - $\\beta_2 = \\frac{\\text{Cov}[\\tau(\\mathbf{X_i}), \\hat{\\tau}(\\mathbf{X_i})]}{\\text{Var}[\\hat{\\tau}(\\mathbf{X_i})]}$\n\n:::\n\n\n## BLP - Interpretation\n\n- $\\beta_2 = \\frac{\\text{Cov}[\\tau(\\mathbf{X_i}), \\hat{\\tau}(\\mathbf{X_i})]}{\\text{Var}[\\hat{\\tau}(\\mathbf{X_i})]} = 1$ if $\\hat{\\tau}(\\mathbf{X_i}) = \\tau(\\mathbf{X_i})$ (what we would like to see)\n- $\\beta_2 = 0$ if $\\text{Cov}[\\tau(\\mathbf{X_i}), \\hat{\\tau}(\\mathbf{X_i})] = 0$, which can have `two reasons`:\n\n  - 1. $\\tau(\\mathbf{X_i})$ is `constant` (no heterogeneity to detect).\n  - 2. $\\tau(\\mathbf{X_i})$ is `not constant` but the estimator is not capable of finding it (bad estimator and/or not enough observations).\n  \n- Therefore, testing $H_0: \\beta_2 = 0$ is a `joint test` of\n  - (i) existence of heterogeneity and\n  - (ii) the estimators capability to find it.\n\n## BLP - Identification Strategy A\n\nStrategy A: `Weighted residual BLP`\n\n- $(\\beta_1, \\beta_2) = \\underset{\\tilde{\\beta_1}, \\tilde{\\beta_2}}{\\operatorname{arg\\,min}} \\space \\mathbb{E} \\left[ \\omega(\\mathbf{X_i}) \\left( Y_i - \\tilde{\\beta_1}(T_i - e(X_i)) - \\tilde{\\beta_2} (T_i - e(X_i)) \\left( \\hat{\\tau}(\\mathbf{X_i}) - \\mathbb{E}[\\hat{\\tau}(\\mathbf{X_i})] \\right) \\color{#005e73}{- \\alpha \\mathbf{X^{C}_{i}}} \\right)^2 \\right]$\n  \n- where: \n  - $\\omega(\\mathbf{X_i}) = \\frac{1}{e(\\mathbf{X_i})(1-e(\\mathbf{X_i}))}$\n  - $\\color{#005e73}{\\mathbf{X^{C}_{i}}}$ is not required for identification, but contains optional functions of $X_i$ to reduce\nestimation noise, e.g. $[1,\\hat\\mu(0,\\mathbf{X_i}), e(\\mathbf{X_i}), e(\\mathbf{X_i})\\hat{\\tau}(\\mathbf{X_i})]$\n\n- See Appendix A in [Chernozhukov et al. (2017-2023)](https://arxiv.org/abs/1712.04802) for a detailed derivation.\n\n\n## BLP - Identification Strategy B\n\nStrategy B: `Horvitz-Thompson BLP`\n\n- $(\\beta_1, \\beta_2) = \\underset{\\tilde{\\beta_1}, \\tilde{\\beta_2}}{\\operatorname{arg\\,min}} \\space \\mathbb{E} \\left[  \\left( H_iY_i - \\tilde{\\beta_1} - \\tilde{\\beta_2} \\left( \\hat{\\tau}(\\mathbf{X_i}) - \\mathbb{E}[\\hat{\\tau}(\\mathbf{X_i})] \\right) \\color{#005e73}{- \\alpha H_i \\mathbf{X^{C}_{i}}} \\right)^2 \\right]$\n  \n- where:\n  - $H_i = \\frac{T_i-e(\\mathbf{X_i})}{e(\\mathbf{X_i})(1-e(\\mathbf{X_i}))}$ are the Horvitz-Thompson (IPW) weights.\n  - $H_iY_i$ serves as a `pseudo-outcome`.\n  - $\\color{#005e73}{\\mathbf{X^{C}_{i}}}$ is not required for identification, but contains optional functions of $X_i$ to reduce\nestimation noise, e.g. $[1,\\hat\\mu(0,\\mathbf{X_i}), e(\\mathbf{X_i}), e(\\mathbf{X_i})\\hat{\\tau}(\\mathbf{X_i})]$\n\n- See Appendix A in [Chernozhukov et al. (2017-2023)](https://arxiv.org/abs/1712.04802) for a detailed derivation.\n\n\n\n## Sorted Group Average Treatment Effect (GATES) {.smaller}\n\n::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\n- `Idea`: \n  - slice the distribution of $\\hat{\\tau}(\\mathbf{X_i})$ into $K$ parts and compare the average treatment effect of individuals within each slice.\n  - if $\\hat{\\tau}(\\mathbf{X_i})$ is a good approximation of $\\tau(\\mathbf{X_i})$, then we expect to observe the following monotonicity: $\\gamma_1 \\leq \\gamma_2 \\leq \\ldots \\leq \\gamma_K$.\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06_hte_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n:::\n\n::: {.callout-note icon=false}\n\n## Definition \"Sorted Group Average Treatment Effect (GATES)\"\n\n$\\gamma_k := \\mathbb{E}[ \\tau(\\mathbf{X_i}) | G_k]$, $k = 1, \\ldots, K$\n\n- where $G_k= \\{\\hat{\\tau}(\\mathbf{X_i}) \\in I_k \\}$ with $I_k = [l_{k-1},l_k)$ and $-\\infty = l_0 < l_1 < \\dots < l_K = \\infty$.\n\n:::\n\n\n\n\n## GATES - Identification {.smaller}\n\n\nStrategy A: `Weighted residual GATES`\n\n- $(\\gamma_1, \\dots, \\gamma_K) = \\underset{\\tilde{\\gamma}_1, \\dots, \\tilde{\\gamma}_K}{\\operatorname{arg\\,min}} \\space \\mathbb{E} \\left[ \\omega(\\mathbf{X_i}) \\left( Y_i - \\sum_k\\tilde{\\gamma_k}(T_i - e(X_i))\\mathbb{1}[G_k]  \\color{#005e73}{- \\alpha \\mathbf{X^{C}_{i}}} \\right)^2 \\right]$\n  \n  - where $\\omega(\\mathbf{X_i}) = \\frac{1}{e(\\mathbf{X_i})(1-e(\\mathbf{X_i}))}$.\n\nStrategy B: `Horvitz-Thompson GATES`\n\n- $(\\gamma_1, \\dots, \\gamma_K) = \\underset{\\tilde{\\gamma}_1, \\dots, \\tilde{\\gamma}_K}{\\operatorname{arg\\,min}} \\space \\mathbb{E} \\left[  \\left( H_iY_i -\\sum_k \\tilde{\\gamma_k}\\mathbb{1}[G_k] \\color{#005e73}{- \\alpha H_i \\mathbf{X^{C}_{i}}} \\right)^2 \\right]$\n  \n  - where $H_iY_i$ serves as a `pseudo-outcome` and $H_i = \\frac{T_i-e(\\mathbf{X_i})}{e(\\mathbf{X_i})(1-e(\\mathbf{X_i}))}$ being the Horvitz-Thompson (IPW) weights.\n\n\n- $\\color{#005e73}{\\mathbf{X^{C}_{i}}}$ is not required for identification, but contains optional functions of $X_i$ to reduce\nestimation noise, e.g. $[1,\\hat\\mu(0,\\mathbf{X_i}), e(\\mathbf{X_i}), e(\\mathbf{X_i})\\hat{\\tau}(\\mathbf{X_i})]$\n- See Appendix A in [Chernozhukov et al. (2017-2023)](https://arxiv.org/abs/1712.04802) for a detailed derivation.\n\n\n\n## Classification Analysis (CLAN)\n\nClassification Analysis (CLAN) can be implemented by simple mean comparisons of covariates in extreme GATES groups:\n\n\n::: {.callout-note icon=false}\n\n## Definition \"Classification Analysis (CLAN)\"\n\nClassification Analysis (CLAN) compares the covariate values of the `least affected group` G1 with the `most affected group` GK defined for the GATES:\n\n  - $\\delta_K - \\delta_1$\n\nwhere\n\n  - $\\delta_k = \\mathbb{E}[X_i | G_k] = \\frac{1}{n_k} \\sum_{i=1}^{n} X_i \\mathbb{1}[G_k]$.\n\n:::\n\n\n\n\n\n\n\n## BLP, GATES & CLAN - Implementation {.smaller}\n\n- R package `GenericML` by [Welz, Alfons, Demirer, and Chernozhukov (2022)](https://CRAN.R-project.org/package=GenericML).\n<br><br>\n- `Algorithm`:\n\n  - `IN`: $\\text{Data} = (Y_i, \\mathbf{X_i}, T_i )^{N}_{i=1}$, significance level $\\alpha$, a suite of ML methods, number of splits $S$.\n\n  - `OUT`: $p-\\text{values}$ and $(1-2\\alpha)$ confidence intervals of point estimates of each target parameter in GATES, BLP, and CLAN.\n<br><br>\n  1. Compute propensity scores $\\hat{e}(\\mathbf{X_i})$.\n  2. Do S splits of $\\{1, . . . ,N\\}$ into disjoint sets $A$ and $M$ of same size.\n  3. **for** each ML method and each split $s = 1, . . . , S$, **do**\n      a) Tune and train each ML method to learn $\\hat{\\mu}(0, \\mathbf{X_i})$ and $\\hat{\\tau}(\\mathbf{X_i})$ on A.\n      b) On M, use $\\hat{\\mu}(0, \\mathbf{X_i})$ and $\\hat{\\tau}(\\mathbf{X_i})$ to estimate the BLP, GATES, CLAN target parameters.\n      c) Compute some performance measures for the ML methods.\n  4. Choose the best ML method based on the medians of the performance measures.\n  5. Calculate the medians of the confidence bounds, p-values, and point estimates of each target parameter.\n  6. Adjust the confidence bounds and p-values.\n\n\n\n\n\n## More References {.smaller}\n\n- `CATE Prediction Methods`:\n\n  - BART ([Hahn, Murray & Carvalho, 2020](https://projecteuclid.org/journals/bayesian-analysis/volume-15/issue-3/Bayesian-Regression-Tree-Models-for-Causal-Inference--Regularization-Confounding/10.1214/19-BA1195.full)).\n  - Causal Boosting/MARS, ... ([Powers, Qian, Jung, Schuler, Shah, Hastie & Tibshirani, 2019](https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7623)).\n  - Dragonnet ([Shi, Blei & Veitch, 20191](https://proceedings.mlr.press/v70/shalit17a.html)).\n  - Modified Causal Forest ([Lechner & Mareckova, 2022](https://arxiv.org/abs/2209.03744)).\n  - Orthogonal Random Forest ([Oprescu, Syrgkanis & Wu, 2019](https://proceedings.mlr.press/v97/oprescu19a.html?ref=https://githubhelp.com)).\n  - TARNet ([Shalit, Johansson & Sontag 2019](https://proceedings.mlr.press/v70/shalit17a.html)).\n  - X-learner ([KuÌnzel, Sekhon, Bickel & Yu, 2019](https://www.pnas.org/doi/abs/10.1073/pnas.1804597116)).\n\n<br> \n\n- `HET Evaluation`:\n\n  - Rank-Weighted Average Treatment Effect (RATE) ([Yadlowsky et al., 2021](https://arxiv.org/abs/2111.07966)).\n  - Calibration Error for Heterogeneous Treatment Effects ([Xu & Yadlowsky, 2022](https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7623)).\n  - More on GATES in experiments ([Imai & Li, 2022-2024](https://arxiv.org/abs/2203.14511)).\n\n\n# Optimal Policy Learning {data-stack-name=\"Optimal Policy Learning\"}\n\n\n## Optimal Policy Learning - Goal {.smaller}\n\n- From evaluation (What works for whom?) towards data-driven (personalized) treatment recommendations: \n  - How to optimally treat whom?\n<br> <br> \n- `Notation`:\n  - Binary treatment indicator: $T_i \\in \\{0, 1\\}$\n  - Potential outcome (PO) under treatment $t$: $Y_i(t)$\n  - Exogenous covariate(s): $\\mathbf{X_i}$\n  - Conditional Average PO: $\\mu_t(\\mathbf{x}) := \\mathbb{E}[Y(t) \\mid \\mathbf{X_i = x}]$\n  - Conditional Average Treatment Effect (CATE): $\\tau(\\mathbf{x}) := \\mu_1(\\mathbf{x}) - \\mu_0(\\mathbf{x})$\n- `Additional notation`:\n  - Policy rule for $x$ (conditional treatment choice): $\\pi(\\mathbf{X_i}) \\in \\{0,1\\}$.\n  - PO under policy  $\\pi(\\mathbf{X_i})$: $Y_i(\\pi(\\mathbf{X_i}))$.\n  - Value function (average PO under policy $\\pi(\\mathbf{X_i})$): $Q(\\pi) := \\mathbb{E}[Y_i(\\pi(\\mathbf{X_i}))]$.\n<br> <br>\n- `Goal`: Find the optimal policy $\\pi^*$ that maximizes the value function $Q(\\pi)$.\n\n\n## Optimal Policy Alternatives\n\n- 1) Assign individuals to treatment with higher PO under treatment than without?\n  - $\\pi^* = \\mathbb{1}[Y_i(1) > Y_i(0)] = \\mathbb{1}[Y_i(1) - Y_i(0) > 0] = \\mathbb{1}[\\tau_i > 0]$\n  - `Fundamental problem of causal inference`: counterfactuals unknown.\n\n::: {.fragment}\n\n- 2) Assign individuals to treatment with higher CATE than without?\n  - $\\pi^* = \\mathbb{1}[Y_i(1) > Y_i(0) | \\mathbf{X_i = x}] = \\mathbb{1}[\\tau(\\mathbf{X_i = x}) > 0]$\n  - `Problem`: minimizing $\\text{MSE}_{\\text{CATE}} = \\mathbb{E}[(\\hat{\\tau}(\\mathbf{x}) - \\tau(\\mathbf{x})^2]$ does not necessarily improve downstream policy rule learning ([Qian & Murphy, 2011](https://projecteuclid.org/journals/annals-of-statistics/volume-39/issue-2/Performance-guarantees-for-individualized-treatment-rules/10.1214/10-AOS864.full)).\n  - Similar to the case where MSE minimization in treated and control groups separately is not the best strategy to minimize CATE MSE.\n\n:::\n\n::: {.fragment}\n\n- 3. Instead: $\\pi^* = \\underset{\\pi}{\\operatorname{arg\\,min}} \\space \\mathbb{E}[Y_i(\\pi(\\mathbf{X_i}))] = \\underset{\\pi}{\\operatorname{arg\\,min}} \\space Q(\\pi(\\mathbf{X_i})))$\n  \n:::\n\n## Optimal Policy Objective Function {.smaller}\n\n- Objective function can have many different forms but one has proven very useful in the context of ML policy learning:\n  - Comparing the value function against a `benchmark policy` that assigns treatments via `fair coin flip`:\n    - 50-50 chance of being treated: $\\pi^{\\text{coin}} \\sim \\text{Bernoulli}(0,5)$.\n\n$$\n\\begin{align*}\n\\pi^* &= \\arg \\max_{\\pi} Q(\\pi) = \\arg \\max_{\\pi} \\mathbb{E}[Y(\\pi)] = \\arg \\max_{\\pi} \\mathbb{E}[Y(\\pi) \\color{#005e73}{- 0.5 \\mathbb{E}[Y(1)] + 0.5 \\mathbb{E}[Y(0)]}] \\\\\n&= \\arg \\max_{\\pi} \\mathbb{E}[\\pi Y(1) + (1 - \\pi) Y(0)] - 0.5 \\mathbb{E}[Y(1)] - 0.5 \\mathbb{E}[Y(0)] \\\\\n&= \\arg \\max_{\\pi} \\mathbb{E}[(\\pi - 0.5) Y(1)] + \\mathbb{E}[(0.5 - \\pi) Y(0)] = \\arg \\max_{\\pi} \\mathbb{E}[(\\pi - 0.5) (Y(1) - Y(0))] \\\\\n&= \\arg \\max_{\\pi}  \\color{#005e73}{2} \\mathbb{E}[(\\pi - 0.5) (Y(1) - Y(0))] \\\\\n&= \\arg \\max_{\\pi} \\mathbb{E}[(2\\pi - 1)(Y(1) - Y(0))] \\\\\n&\\overset{LIE}{=} \\arg \\max_{\\pi} \\mathbb{E}[(2\\pi - 1) \\tau(\\mathbf{X_i})] \\\\\n&= \\arg \\max_{\\pi}  \\underbrace{\\mathbb{E}[|\\tau(\\mathbf{X_i})| \\text{sign}(\\tau(\\mathbf{X_i})) (2\\pi(\\mathbf{X_i}) - 1)]}_{A(\\pi)} \\\\\n\\end{align*}\n$$\n\n- where $(2\\pi(\\mathbf{X_i}) - 1) \\in \\{-1,1\\}$ is one if policy assigns treatment and minus one if not.\n\n\n## Optimal Policy Objective Function - Intuition \n\n-  $A(\\pi) := \\mathbb{E}[|\\tau(\\mathbf{X_i})| \\text{sign}(\\tau(\\mathbf{X_i})) (2\\pi(\\mathbf{X_i}) - 1)]$ measures the  `advantage ` of a policy\ncompared to random allocation:\n    - If $\\text{sign}(\\tau(\\mathbf{X_i})) (2\\pi(\\mathbf{X_i}) - 1) = 1$, i.e. if the policy picks the better treatment for $\\mathbf{X_i}$, we  `earn the absolute value of the CATE `.\n    - If $\\text{sign}(\\tau(\\mathbf{X_i})) (2\\pi(\\mathbf{X_i}) - 1) = -1$, i.e. if the policy picks the worse treatment for $\\mathbf{X_i}$, we  `lose the absolute value of the CATE `.\n- We need to  `get it right for those with biggest CATEs `, those with CATEs close to zero are negligible.\n- This shows the  `difference to CATE MSE minimization `, where we need to find good approximations everywhere.\n\n\n## Optimal Policy Identification & Estimation \n\n- Potential outcomes or CATE functions unknown, need to be identified before optimization.\n- [Athey and Wager (2021)](https://www.econometricsociety.org/publications/econometrica/2021/01/01/policy-learning-observational-data) recommend the `pseudo-outcome` (again) because of all the nice properties:\n  - $\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} = \\hat{\\mu}(1, \\mathbf{X_i}) - \\hat{\\mu}(0, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\hat{\\mu}(1, \\mathbf{X_i}))}{\\hat{e}_1(\\mathbf{X_i})} - \\frac{(1-T_i)(Y_i - \\hat{\\mu}(0, \\mathbf{X_i})}{\\hat{e}_0(\\mathbf{X_i}))}$\n  \n::: {.fragment}\n\n- `Binary weighted classification problem`: classify the sign of the CATE while favoring correct classifications with larger absolute CATEs.\n  - $\\hat{\\pi} = \\underset{\\pi \\in \\Pi}{\\arg \\max} \\left\\{ \\frac{1}{N} \\sum_{i=1}^N \\overbrace{|\\hat{Y}_{i,\\text{ATE}}|}^{\\text{weight}} \\underbrace{\\operatorname{sign}(\\hat{Y}_{i,\\text{ATE}}}_{\\text{to be classified}} \\overbrace{(2\\pi(X_i) - 1)}^{\\text{function to be learned}} \\right\\}$\n- Possible methods: e.g. decision trees/forests, logistic lasso, SVM, etc.\n\n:::\n\n\n\n\n# {.center .center-x data-state=\"hide-menubar\"}\n<style>\n.table-text {\n  text-align: center !important;\n  font-size: 2em;\n  padding-bottom: 1em !important;\n}\n.row-bottom {\n  text-align: center !important;\n  padding-top: 2em !important;\n}\n</style>\n<table width = \"100%\">\n<tbody>\n  <tr>\n    <td colspan=\"2\" class=\"table-text\">Thank you for your attention!</td>\n  </tr>\n  <tr >\n    <td class=\"row-bottom\" style=\"vertical-align: baseline; width: 50%;\">\n          <img src=\"https://raw.githubusercontent.com/christophihl/TIE_website/main/assets/images/logo.svg\" style=\"vertical-align: middle; width: 60%; height: auto;\" />\n          <img src=\"author_pic.jpg\" class=\"picture\" style=\"vertical-align: middle; horizontal-align: right; width: 30%; height: auto;\" />\n    </td>\n    <td class = \"row-bottom\" style = \"vertical-align: middle; width = 50%\">\n      <ul style = \"list-style:none;\">\n        <li><a href = \"https://www.startupengineer.io/authors/ihl/\" target = \"_blank\"><i class = \"fas fa-home\"></i> startupengineer.io/authors/ihl</a></li>\n        <li><a href = \"https://www.linkedin.com/in/christoph-ihl/\" target = \"_blank\"><i class = \"fab fa-linkedin\"></i> christoph-ihl</a></li>\n        <li><a href = \"https://github.com/christophihl\" target = \"_blank\"><i class = \"fab fa-github\"></i> christophihl</a></li>\n        <li><a href = \"https://twitter.com/Ihluminate/\" target = \"_blank\"><i class = \"fab fa-twitter\"></i> Ihluminate</a></li>\n      </ul>\n    </td>\n  </tr>\n</tbody>\n</table>\n\n",
    "supporting": [
      "06_hte_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}